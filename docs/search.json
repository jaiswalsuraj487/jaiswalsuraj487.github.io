[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "title": "Baysian Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating\n\\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/Untitled-1.html",
    "href": "blogs/blogsData/Untitled-1.html",
    "title": "Demo",
    "section": "",
    "text": "print('hello')\n\nhello"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 18, 2023\n\n\nBaysian Linear Regression blog\n\n\nSuraj Jaiswal\n\n\n\n\nJun 12, 2023\n\n\nPlots for bayesian LR\n\n\nSuraj Jaiswal\n\n\n\n\nJun 5, 2023\n\n\nBaysian Linear Regression\n\n\nSuraj Jaiswal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Welcome to my blog! I’m Suraj Jaiswal, an M.Tech student at IIT Gandhinagar, pursuing Computer Science and Engineering under the mentorship of Dr. Nipun Batra.  Here, I’ll share my experiences, research projects, and insights as we delve into the exciting world of technology. IIT Gandhinagar, known for its excellence in education and research, provides the ideal environment for me to explore the limitless possibilities of computer science."
  },
  {
    "objectID": "blogs/blogsData/demo.html",
    "href": "blogs/blogsData/demo.html",
    "title": "Plots for bayesian LR",
    "section": "",
    "text": "import random\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\n\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n9.3\n$$                                    \\\n                       \\\nx_{n}       y_{n}   \\\nn = 1,……,N \\\n\\\n$$\nfigure 9.5\n\nN = 10\nmu = 0\nsigma = 0.2**2\n\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.2123287 ],\n       [-0.18826531]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\nml_predictions.shape\n\n(10, 1)\n\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nFigure 9.6\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n%config InlineBackend.figure_format = \"retina\"\n\n\ndef f(x, a): return a[0] + a[1] * x\n\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + beta * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5,30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()"
  },
  {
    "objectID": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "href": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Table of Symbols from the book Mathematics for Machine Learning: https://mml-book.github.io/. Latex was provided by the co-author Cheng Soon Ong (Many Thanks) and edited by Harry Wang: https://github.com/mml-book/mml-book.github.io/issues/634\nSee latex version on overleaf.com: https://www.overleaf.com/read/mnzgdyrsjfsk\n$\n% vector bf: boldface\n% matrix % transpose % inverse\n% set cal: calligraphic letters % dimension, rm: roman typestyle % rank\n% determinant % identity mapping % kernel/nullspace % image\n% generating set\n% tensor % trace\n% lagrangian % likelihood % variance % expectation % covariance\n% given % Gaussian distribution\n% other distributions $\n\n\n\n\n\n\n\nSymbol                             \nTypical Meaning\n\n\n\n\n\\(a,b,c, \\alpha,\\beta,\\gamma\\)\nScalars are lowercase\n\n\n\\(\\mathbf{x},\\mathbf{y},\\mathbf{z}\\)\nVectors are bold lowercase\n\n\n\\(\\mathbf{A},\\mathbf{B},\\mathbf{C}\\)\nMatrices are bold uppercase\n\n\n\\(\\mathbf{x} ^\\top, \\mathbf{A} ^\\top\\)\nTranspose of a vector or matrix\n\n\n\\(\\mathbf{A}^{-1}\\)\nInverse of a matrix\n\n\n\\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\)\nInner product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(\\mathbf{x} ^\\top\\mathbf{y}\\)\nDot product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(B = (\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3)\\)\n(Ordered) tuple\n\n\n\\(\\mathbf{B} = [\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3]\\)\nMatrix of column vectors stacked horizontally\n\n\n\\(\\mathcal{B} = \\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}\\)\nSet of vectors (unordered)\n\n\n\\(\\mathbb Z,\\mathbb N\\)\nIntegers and natural numbers, respectively\n\n\n\\(\\mathbb R,\\mathbb C\\)\nReal and complex numbers, respectively\n\n\n\\(\\mathbb R^n\\)\n\\(n\\)-dimensional vector space of real numbers\n\n\n\\(\\forall x\\)\nUniversal quantifier: for all \\(x\\)\n\n\n\\(\\exists x\\)\nExistential quantifier: there exists \\(x\\)\n\n\n\\(a := b\\)\n\\(a\\) is defined as \\(b\\)\n\n\n\\(a =:b\\)\n\\(b\\) is defined as \\(a\\)\n\n\n\\(a\\propto b\\)\n\\(a\\) is proportional to \\(b\\), i.e., \\(a =\\text\\{constant\\}\\cdot b\\)\n\n\n\\(g\\circ f\\)\nFunction composition: \\(g\\) after \\(f\\)\n\n\n\\(\\iff\\)\nIf and only if\n\n\n\\(\\implies\\)\nImplies\n\n\n\\(\\mathcal{A}, \\mathcal{C}\\)\nSets\n\n\n\\(a \\in \\mathcal{A}\\)\n\\(a\\) is an element of set \\(\\mathcal{A}\\)\n\n\n\\(\\emptyset\\)\nEmpty set\n\n\n\\(\\mathcal{A}\\setminus \\mathcal{B}\\)\n\\(\\mathcal{A}\\) without \\(\\mathcal{B}\\): the set of elements in \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\n\n\\(D\\)\nNumber of dimensions; indexed by \\(d=1,\\dots,D\\)\n\n\n\\(N\\)\nNumber of data points; indexed by \\(n=1,\\dots,N\\)\n\n\n\\(\\mathbf{I}_m\\)\nIdentity matrix of size \\(m\\times m\\)\n\n\n\\(\\mathbf{0}_{m,n}\\)\nMatrix of zeros of size \\(m\\times n\\)\n\n\n\\(\\mathbf{1}_{m,n}\\)\nMatrix of ones of size \\(m\\times n\\)\n\n\n\\(\\mathbf{e}_i\\)\nStandardcanonical vector (where \\(i\\) is the component that is \\(1\\))\n\n\n\\(\\mathrm{dim}\\)\nDimensionality of vector space\n\n\n\\(\\mathrm{rk}(\\mathbf{A})\\)\nRank of matrix \\(\\mathbf{A}\\)\n\n\n\\(\\mathrm{Im}(\\Phi)\\)\nImage of linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{ker}(\\Phi)\\)\nKernel (null space) of a linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{span}[\\mathbf{b}_1]\\)\nSpan (generating set) of \\(\\mathbf{b}_1\\)\n\n\n\\(\\text{tr}(\\mathbf{A})\\)\nTrace of \\(\\mathbf{A}\\)\n\n\n\\(\\det(\\mathbf{A})\\)\nDeterminant of \\(\\mathbf{A}\\)\n\n\n\\(| \\cdot |\\)\nAbsolute value or determinant (depending on context)\n\n\n\\(\\| {\\cdot} \\|\\)\nNorm; Euclidean, unless specified\n\n\n\\(\\lambda\\)\nEigenvalue or Lagrange multiplier\n\n\n\\(E_\\lambda\\)\nEigenspace corresponding to eigenvalue \\(\\lambda\\)\n\n\n\\(\\mathbf{x} \\perp \\mathbf{y}\\)\nVectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal\n\n\n\\(V\\)\nVector space\n\n\n\\(V^\\perp\\)\nOrthogonal complement of vector space \\(V\\)\n\n\n\\(\\sum_{n=1}^N x_n\\)\nSum of the \\(x_n\\): \\(x_1 + \\dotsc + x_N\\)\n\n\n\\(\\prod_{n=1}^N x_n\\)\nProduct of the \\(x_n\\): \\(x_1 \\cdot\\dotsc \\cdot x_N\\)\n\n\n\\(\\mathbf{\\theta}\\)\nParameter vector\n\n\n\\(\\frac{\\partial f}{\\partial x}\\)\nPartial derivative of \\(f\\) with respect to \\(x\\)\n\n\n\\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\)\nTotal derivative of \\(f\\) with respect to \\(x\\)\n\n\n$$\nGradient\n\n\n\\(f_* = \\min_x f(x)\\)\nThe smallest function value of \\(f\\)\n\n\n\\(x_* \\in \\arg\\min_x f(x)\\)\nThe value \\(x_*\\) that minimizes \\(f\\) (note: \\(\\arg\\min\\) returns a set of values)\n\n\n\\(\\mathfrak{L}\\)\nLagrangian\n\n\n\\(\\mathcal{L}\\)\nNegative log-likelihood\n\n\n\\(\\binom{n}{k}\\)\nBinomial coefficient, \\(n\\) choose \\(k\\)\n\n\n\\(\\mathbb{V}_X[\\mathbf{x}]\\)\nVariance of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathbb{E}_X[\\mathbf{x}]\\)\nExpectation of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathop{\\mathrm{Cov}}_{X,Y}[\\mathbf{x}, \\mathbf{y}]\\)\nCovariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).\n\n\n\\(X \\perp\\kern-5pt \\perp Y\\vert Z\\)\n\\(X\\) is conditionally independent of \\(Y\\) given \\(Z\\)\n\n\n\\(X\\sim p\\)\nRandom variable \\(X\\) is distributed according to \\(p\\)\n\n\n\\(\\mathcal{N}\\big(\\mathbf{\\mu},\\mathbf{\\Sigma}\\big)\\)\nGaussian distribution with mean \\(\\mathbf{\\mu}\\) and covariance \\(\\mathbf{\\Sigma}\\)\n\n\n\\(\\text{Ber}(\\mu)\\)\nBernoulli distribution with parameter \\(\\mu\\)\n\n\n\\(\\text{Bin}(N, \\mu)\\)\nBinomial distribution with parameters \\(N, \\mu\\)\n\n\n\\(\\text{Beta}(\\alpha, \\beta)\\)\nBeta distribution with parameters \\(\\alpha, \\beta\\)\n\n\n\n\nθ\nyn\nσ\nxn\nn = 1, . . . , N\n\n\n$ L(\\theta, σ | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, σ) $ \n\nSyntaxError: invalid syntax (2183852464.py, line 1)\n\n\n\nfrom re import L\n\n\nL(\\theta, \\sigma | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, \\sigma)"
  },
  {
    "objectID": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "href": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "title": "Bayesian Learning in a Linear Basis Function Model",
    "section": "",
    "text": "In this notebook we ilustrate the bayesian learning in a linear basis function model, as well as the sequential update of a posterior distribution.\nTaken from Christopher Bishop’s Pattern Recognition and Machine Learning book (p.155)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n\n%config InlineBackend.figure_format = \"retina\"\n\nWe consider an input \\(x\\), a target variable \\(t\\) and a linear model of the form \\[\n    y(x, {\\bf w}) = w_0 + w_1x\n\\]\n\ndef f(x, a): return a[0] + a[1] * x\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# plot_sample_w(0, )\n\n\nplt.scatter(X, T, c=\"crimson\")\nplt.grid(alpha=0.5)\nplt.plot(X, f(X,a))\n\n\n\n\nOur goal is to recover the values \\(w_0\\) and \\(w_1\\) from the data.\nRecall: \\[\n    p({\\textbf w}|t) \\propto p(t| {\\textbf w}, \\beta) p(\\bf{w})\n\\]\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\n\\(t \\sim \\mathcal{N}\\left({\\bf w}^Tx, \\beta^{-1}\\right)\\) (The assigned probability for target variables)\n\nThe posterior distribution of \\(\\bf w\\) after \\(N\\) observations is given by\n\\[\n\\begin{align}\n    m_N &= S_N(S_0^{-1}m_0 + \\beta\\Phi^T{\\bf t}) \\\\\n    S_N^{-1} &= S_0^{-1} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\nWith * \\(\\Phi\\in\\mathbb{R}^{N\\times M}\\) * \\({\\bf t}\\in\\mathbb{R}^N\\)\nIf no data has been yet seen, we consider \\[\nw \\sim \\mathcal{N}\\left(0, \\alpha^{-1}\\text{I}\\right)\n\\] Which results in a posterior distribution of the form\n\\[\n\\begin{align}\n    m_N &= \\beta S_N\\Phi^T{\\bf t} \\\\\n    S_N^{-1} &= \\alpha \\text{I} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * Phi.T @ Phi)\n    mN = SN @ (inv(S0) @ m0 + beta * Phi.T @ t)\n    return SN, mN\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in] #  concatenate arrays along the second axis\n    t = T[[ix]]\n    return Phi, t\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n        \ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\n\nseed(1643)\nN = 20\nnobs = [1, 2, 20]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N+1):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nIn the limit of an infinite number of datapoints, the posterior distribution would become a delta function centered on the true parameter values, shown by the white cross.\nOther forms of prior over parameters can be considered. For example, the generalized Gaussian:\n\\[\n    p({\\bf w}|\\alpha) = \\left[\\frac{q}{2}\\left(\\frac{q}{2}\\right)^{1/q}\\frac{1}{\\Gamma(1/q)}\\right]^M \\exp\\left(-\\frac{\\alpha}{2}\\sum_{j=1}^M|w_j|^q\\right)\n\\]\nFinding the maximum posterior distribution over \\({\\bf w}\\) corresponds to minimization of the regularized error function given by:\n\\[\n    \\frac{1}{2}\\sum_{n=1}^N\\left(t_n - {\\bf w}^T {\\bf\\phi}({\\bf x}_n)\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_j|^q\n\\]\nThe maximum posterior weight vector \\({\\bf w}_{\\text{MAP}}\\) considering a generalized Gaussian will not be (in every case) the mean, since the mean will not coincide with the mode."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html",
    "href": "blogs/blogsData/blr_blog.html",
    "title": "Baysian Linear Regression blog",
    "section": "",
    "text": "Welcome to my blog on Bayesian linear regression, where we explore the power of this technique. While traditional linear regression provides point estimates, Bayesian linear regression incorporates prior knowledge and quantifies uncertainty. By combining observed data with prior beliefs, we make more informed decisions. Throughout this blog, we’ll delve into key components like probalistic approch to linear regression, basics of types of uncertainity, prior distributions, likelihood functions, and posterior inference. Let’s embark on this enlightening journey together."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "href": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "title": "Baysian Linear Regression blog",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X])\n\n\ndef max_lik_estimate(X, y):\n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_ml = max_lik_estimate(X_aug,y)\nprint(theta_ml)\n\n[[2.116]\n [0.499]]\n\n\nNow we will make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), \\[\n\\ \\boldsymbol y_{\\text{pred}} = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\nml_prediction = Xtest_aug @ theta_ml\n\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\nText(0, 0.5, '$y$')\n\n\n\n\n\nThis gives fairly good results but what if the data is bit complex\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\nN = 10\nmu = 0\nsigma = 0.2**2\nseed(10)\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nLets first apply linear regressoin without non linear transformation\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.47109666],\n       [-0.1808517 ]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#known-entities",
    "href": "blogs/blogsData/blr_blog.html#known-entities",
    "title": "Baysian Linear Regression blog",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\\[\n\\begin{array}{l}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ m_{0} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ S_{0}\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\searrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow \\ \\ \\ \\swarrow \\\\\nx_{n} \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ y_{n} \\ \\ \\\\\nn\\ =\\ 1,......,N\\ \\\\\n\\end{array}\n\\] \\[ Graphical \\ model \\ for \\ Bayeisan \\ linear \\ regression \\]\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, m0, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#posterior",
    "href": "blogs/blogsData/blr_blog.html#posterior",
    "title": "Baysian Linear Regression blog",
    "section": "Posterior",
    "text": "Posterior\n\nParameter posteriori in closed form\nCalculating Parameter posterior: \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\n\n\nPosterior Predictive distribution\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\n\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}}) \\, |\\ X, Y, \\boldsymbol X_{\\text{test}}) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol M_{\\text{n}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol S_{\\text{N}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top + \\sigma ^ 2)\n\\end{align} \\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\n# conf_bound1 = np.sqrt(var_blr).flatten()\n# plt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\n# 95 % parameter uncertainity\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"r\")\n\n# 95 % total uncertainity ie. \nconf_bound3 = 2.0*np.sqrt(var_blr + sigma**2).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\", '95% para uncertainity', '95% total uncertainity'])\nplt.xlabel('$x$');\nplt.ylabel('$y$');\n\n\n\n\nYou can refer 9.3 section of Mathematics for Machine Learning to understand in depth about bayesian linear regression.\n\n\nVisulizing the parameter Posterior\nIn this section we will visualize the posterior and will see how it changes as it sees more data.\n\ndef f(x, a): return a[0] + a[1] * x\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(sigma **2)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + ((1 / sigma) ** 2) * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + ((1 / sigma) ** 2) * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5]) # true parameter values\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# beta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5, 15, 30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nWe can see above as the model see more data, the posterior converges close the the true values at end. Refer to Bishop - Pattern Recognition and Machine Learning fig 3.7 to understand above fig in detail."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html",
    "href": "demo_notebooks/bayesian_linear_regression.html",
    "title": "Bayesian linear regression",
    "section": "",
    "text": "1. What is Bayesian linear regression?\n2. Recap linear regression\n3. Fundamental concepts\n4. Linear regression from a probabilistic perspective\n5. Linear regression with basis functions\n\n5.1 Example basis functions\n5.2 The design matrix\n\n6. Bayesian Linear Regression\n\n6.1 Step 1: Probabilistic Model\n6.2 Generating a dataset\n6.3 Step 2: Posterior over the parameters\n6.4 Visualizing the parameter posterior\n6.5 Step 3: Posterior predictive distribution\n6.6 Visualizing the predictive posterior\n\nSources and further reading"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "href": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "title": "Bayesian linear regression",
    "section": "Link to interactive demo",
    "text": "Link to interactive demo\nClick here to run the notebook online (using Binder) without installing jupyter or downloading the code.\nSometimes, the GitHub version of the Jupyter notebook does not display the math formulas correctly. Please refer to the Binder version in case you think something might be off or missing.\nI also wrote a blog post containing the contents of the notebook."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "href": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "title": "Bayesian linear regression",
    "section": "1. What is Bayesian linear regression (BLR)? ",
    "text": "1. What is Bayesian linear regression (BLR)? \nBayesian linear regression is the Bayesian interpretation of linear regression. What does that mean? To answer this question we first have to understand the Bayesian approach. In most of the algorithms we have looked at so far we computed point estimates of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function. In the Bayesian approach we don’t work with exact values but with probabilities. This allows us to model the uncertainty in our parameter estimates. Why is this important?\nIn nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty. Bayesian methods have been around for a long time and are widely-used in many areas of science (e.g. astronomy). Although Bayesian methods have been applied to machine learning problems too, they are usually less well known to beginners. The major reason is that they require a good understanding of probability theory.\nIn the following notebook we will work our way from linear regression to Bayesian linear regression, including the most important theoretical knowledge and code examples."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "title": "Bayesian linear regression",
    "section": "2. Recap linear regression ",
    "text": "2. Recap linear regression \n\nIn linear regression, we want to find a function \\(f\\) that maps inputs \\(x \\in \\mathbb{R}^D\\) to corresponding function values \\(f(x) \\in \\mathbb{R}\\).\nWe are given an input dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\), where \\(y_n\\) is a noisy observation value: \\(y_n = f(x_n) + \\epsilon\\), with \\(\\epsilon\\) being an i.i.d. random variable that describes measurement/observation noise\nOur goal is to infer the underlying function \\(f\\) that generated the data such that we can predict function values at new input locations\nIn linear regression, we model the underlying function \\(f\\) using a linear combination of the input features:\n\n\\[\n\\begin{split}\ny &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d \\\\\n&= \\boldsymbol{x}^T \\boldsymbol{\\theta}\n\\end{split}\n\\]\n\nFor more details take a look at the notebook on linear regression"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "href": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "title": "Bayesian linear regression",
    "section": "3. Fundamental concepts ",
    "text": "3. Fundamental concepts \n\nOne fundamental tool in Bayesian learning is Bayes’ theorem\nBayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\]\n\\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters.\n\\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely.\n\\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets.\n\n\\(p(\\boldsymbol{\\theta} | \\boldsymbol{x}, y)\\) is the posterior. It describes the probability of the parameters given the observed data and targets. \nAnother important tool you need to know about is the Gaussian distribution. If you are not familiar with it I suggest you pause for a minute and understand its main properties before reading on.\n\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nFor more information take a look at the Wikipedia article on Bayesian inference."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "title": "Bayesian linear regression",
    "section": "4. Linear regression from a probabilistic perspective ",
    "text": "4. Linear regression from a probabilistic perspective \nIn order to pave the way for Bayesian linear regression we will take a probabilistic spin on linear regression. Let’s start by explicitly modelling the observation noise \\(\\epsilon\\). For simplicity, we assume that \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nAs mentioned in the beginning, a simple linear regression model assumes that the target function \\(f(x)\\) is given by a linear combination of the input features: \\[\n\\begin{split}\ny = f(\\boldsymbol{x}) + \\epsilon \\\\\n  = \\boldsymbol{x}^T \\boldsymbol{\\theta} + \\epsilon\n\\end{split}\n\\]\nThis corresponds to the following likelihood function: \\[p(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{x}^T \\boldsymbol{\\theta}, \\sigma^2)\\]\nOur goal is to find the parameters \\(\\boldsymbol{\\theta} = \\{\\theta_1, ..., \\theta_D\\}\\) that model the given data best. In standard linear regression we can find the best parameters using a least-squares, maximum likelihood (ML) or maximum a posteriori (MAP) approach. If you want to know more about these solutions take a look at the notebook on linear regression or at chapter 9.2 of the book Mathematics for Machine Learning."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "title": "Bayesian linear regression",
    "section": "5. Linear regression with basis functions ",
    "text": "5. Linear regression with basis functions \nThe simple linear regression model above is linear not only with respect to the parameters \\(\\boldsymbol{\\theta}\\) but also with respect to the inputs \\(\\boldsymbol{x}\\). When \\(\\boldsymbol{x}\\) is not a vector but a single value (that is, the dataset is one-dimensional) the model \\(y_i = x_i \\cdot \\theta\\) describes straight lines with \\(\\theta\\) being the slope of the line.\nThe plot below shows example lines produced with the model \\(y = x \\cdot \\theta\\), using different values for the slope \\(\\theta\\) and intercept 0.\n\nHaving a model which is linear both with respect to the parameters and inputs limits the functions it can learn significantly. We can make our model more powerful by making it nonlinear with respect to the inputs. After all, linear regression refers to models which are linear in the parameters, not necessarily in the inputs (linear in the parameters means that the model describes a function by a linear combination of input features).\nMaking the model nonlinear with respect to the inputs is easy. We can adapt it by using a nonlinear transformation of the input features \\(\\phi(\\boldsymbol{x})\\). With this adaptation our model looks as follows: \\[\n\\begin{split}\ny &= \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon \\\\\n&= \\sum_{k=0}^{K-1} \\theta_k \\phi_k(\\boldsymbol{x}) + \\epsilon\n\\end{split}\n\\]\nWhere \\(\\boldsymbol{\\phi}: \\mathbf{R}^D \\rightarrow \\mathbf{R}^K\\) is a (non)linear transformation of the inputs \\(\\boldsymbol{x}\\) and \\(\\phi_k: \\mathbf{R}^D \\rightarrow \\mathbf{R}\\) is the \\(k-\\)th component of the feature vector \\(\\boldsymbol{\\phi}\\):\n\\[\n\\boldsymbol{\\phi}(\\boldsymbol{x})=\\left[\\begin{array}{c}\n\\phi_{0}(\\boldsymbol{x}) \\\\\n\\phi_{1}(\\boldsymbol{x}) \\\\\n\\vdots \\\\\n\\phi_{K-1}(\\boldsymbol{x})\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith our new nonlinear transformation the likelihood function is given by\n\\[\np(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta},\\, \\sigma^2)\n\\]\n\n5.1 Example basis functions \n\nLinear regression\nThe easiest example for a basis function (for one-dimensional data) would be simple linear regression, that is, no non-linear transformation at all. In this case we would choose \\(\\phi_0(x) = 1\\) and \\(\\phi_i(x) = x\\). This would result in the following vector \\(\\boldsymbol{\\phi}(x)\\):\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\n\\vdots \\\\\nx\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\n\n\nPolynomial regression\nAnother common choice of basis function for the one-dimensional case is polynomial regression. For this we would set \\(\\phi_i(x) = x^i\\) for \\(i=0, ..., K-1\\). The corresponding feature vector \\(\\boldsymbol{\\phi}(x)\\) would look as follows:\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\nx^2 \\\\\nx^3 \\\\\n\\vdots \\\\\nx^{K-1}\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith this transformation we can lift our original one-dimensional input into a \\(K\\)-dimensional feature space. Our function \\(f\\) can be any polynomial with degree \\(\\le K-1\\): \\(f(x) = \\sum_{k=0}^{K-1} \\theta_k x^k\\)\n\n\n\n5.2 The design matrix \nTo make it easier to work with the transformations \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\) for the different input vectors \\(\\boldsymbol{x}\\) we typically create a so called design matrix (also called feature matrix). Given our dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\) we define the design matrix as follows:\n\\[\n\\boldsymbol{\\Phi}:=\\left[\\begin{array}{c}\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\vdots \\\\\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n\\phi_{0}\\left(\\boldsymbol{x}_{1}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{2}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{2}\\right) \\\\\n\\vdots & & \\vdots \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{N}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right] \\in \\mathbb{R}^{N \\times K}\n\\]\nNote that the design matrix is of shape \\(N \\times K\\). \\(N\\) is the number of input examples and \\(K\\) is the output dimension of the non-linear transformation \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\)."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "title": "Bayesian linear regression",
    "section": "6. Bayesian linear regression ",
    "text": "6. Bayesian linear regression \nWhat changes when we consider a Bayesian interpretation of linear regression? Our data stays the same as before: \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\). Given the data \\(D\\) we can define the set of all inputs as \\(\\mathcal{X} := \\{\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n\\}\\) and the set of all targets as \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\).\nIn simple linear regression we compute point estimates of our parameters (e.g. using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates distributions over the parameters and predictions. This allows us to model the uncertainty in our predictions.\nTo perform Bayesian linear regression we follow three steps: 1. We set up a probabilistic model that describes our assumptions how the data and parameters are generated 2. We perform inference for the parameters \\(\\boldsymbol{\\theta}\\), that is, we compute the posterior probability distribution over the parameters 3. With this posterior we can perform inference for new, unseen inputs \\(y_*\\). In this step we don’t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs.\n\n6.1 Step 1: Probabilistic model \nWe start by setting up a probabilistic model that describes our assumptions how the data and parameters are generated. For this, we place a prior \\(p(\\boldsymbol{\\theta})\\) over our parameters which encodes what parameter values are plausible (before we have seen any data). Example: With a single parameter \\(\\theta\\), a Gaussian prior \\(p(\\theta) = \\mathcal{N}(0, 1)\\) says that parameter values are normally distributed with mean 0 and standard deviation 1. In other words: the parameter values are most likely to fall into the interval [−2,2] which is two standard deviations around the mean value.\nTo keep things simple we will assume a Gaussian prior over the parameters: \\(p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{m}_0, \\boldsymbol{S}_0)\\). Let’s further assume that the likelihood function is Gaussian, too: \\(p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(y \\mid \\boldsymbol{\\phi}^{\\top}(\\boldsymbol{x}) \\boldsymbol{\\theta}, \\sigma^{2}\\right)\\).\nNote: When considering the set of all targets \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\), the likelihood function becomes a multivariate Gaussian distribution: \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\nThe nice thing about choosing a Gaussian distribution for our prior is that the posterior distributions will be Gaussian, too (keyword conjugate prior)!\nWe will start our BayesianLinearRegression class with the knowledge we have so far - our probabilistic model. As mentioned in the beginning we assume that the variance \\(\\sigma^2\\) of the noise \\(\\epsilon\\) is known. Furthermore, to allow plotting the data later on we will assume that it’s two dimensional (d=2).\n\nfrom scipy.stats import multivariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n        \n# Let's make sure that we can initialize our model\nprior_mean = np.array([0, 0])\nprior_cov = np.array([[0.5, 0], [0, 0.5]])\nnoise_var = 0.2\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\n\n\n6.2 Generating a dataset \nBefore going any further we need a dataset to test our implementation. Remember that we assume that our targets were generated by a function of the form \\(y = \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon\\) where \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nTo keep things simple we will work with one-dimensional data and simple linear regression (that is, no non-linear transformation of the inputs). Consequently, our data generating function will be of the form \\[ y = \\theta_0 + \\theta_1 \\, x + \\epsilon \\]\nNote that we added a parameter \\(\\theta_0\\) which corresponds to the intercept of the linear function. Until know we assumed \\(\\theta_0 = 0\\). As mentioned earlier, \\(\\theta_1\\) represents the slope of the linear function.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef compute_function_labels(slope: float, intercept: float, noise_std_dev: float, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute target values given function parameters and data.\n    \n    Args:\n        slope: slope of the function (theta_1)\n        intercept: intercept of the function (theta_0)\n        data: input feature values (x)\n        noise_std_dev: standard deviation of noise distribution (sigma)\n        \n    Returns:\n        target values, either true or corrupted with noise\n    \"\"\"\n    n_samples = len(data)\n    if noise_std_dev == 0: # Real function\n        return slope * data + intercept\n    else: # Noise corrupted\n        return slope * data + intercept + np.random.normal(0, noise_std_dev, n_samples)\n\n\n# Set random seed to ensure reproducibility\nseed = 42\nnp.random.seed(seed)\n\n# Generate true values and noise corrupted targets\nn_datapoints = 1000\nintercept = -0.7\nslope = 0.9\nnoise_std_dev = 0.5\nnoise_var = noise_std_dev**2\nlower_bound = -1.5\nupper_bound = 1.5\n\n# Generate dataset\nfeatures = np.random.uniform(lower_bound, upper_bound, n_datapoints)\nlabels = compute_function_labels(slope, intercept, 0., features)\nnoise_corrupted_labels = compute_function_labels(slope, intercept, noise_std_dev, features)\n\n\n# Plot the dataset\nplt.figure(figsize=(10,7))\nplt.plot(features, labels, color='r', label=\"True values\")\nplt.scatter(features, noise_corrupted_labels, label=\"Noise corrupted values\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Labels\")\nplt.title(\"Real function along with noisy targets\")\nplt.legend();\n\n\n\n\n\n\n6.3 Step 2: Posterior over the parameters \nWe finished setting up our probabilistic model. Next, we want to use this model and our dataset \\(\\mathcal{X, Y}\\) to estimate the parameter posterior \\(p(\\boldsymbol{\\theta} | \\mathcal{X, Y})\\). Keep in mind that we don’t compute point estimates of the parameters. Instead, we determine the mean and variance of the (Gaussian) posterior distribution and use this entire distribution when making predictions.\nWe can estimate the parameter posterior using Bayes theorem: \\[\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y})=\\frac{p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\mathcal{Y} \\mid \\mathcal{X})}\n\\]\n\n\\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})\\) is the likelihood function, \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\n\\(p(\\boldsymbol{\\theta})\\) is the prior distribution, \\(p(\\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{0}, \\boldsymbol{S}_{0}\\right)\\)\n\\(p(\\mathcal{Y} \\mid \\mathcal{X})=\\int p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\) is the evidence which ensures that the posterior is normalized (that is, that it integrates to 1).\n\nThe parameter posterior can be estimated in closed form (for proof see theorem 9.1 in the book Mathematics for Machine Learning): \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\nComing back to our BayesLinearRegression class we need to add a method which allows us to update the posterior distribution given a dataset.\n\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import norm as univariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n    def update_posterior(self, features: np.ndarray, targets: np.ndarray):\n        \"\"\"\n        Update the posterior distribution given new features and targets\n        \n        Args:\n            features: numpy array of features\n            targets: numpy array of targets\n        \"\"\"\n        # Reshape targets to allow correct matrix multiplication\n        # Input shape is (N,) but we need (N, 1)\n        targets = targets[:, np.newaxis]\n        \n        # Compute the design matrix, shape (N, 2)\n        design_matrix = self.compute_design_matrix(features)\n\n        # Update the covariance matrix, shape (2, 2)\n        design_matrix_dot_product = design_matrix.T.dot(design_matrix)\n        inv_prior_cov = np.linalg.inv(self.prior_cov)\n        self.post_cov = np.linalg.inv(inv_prior_cov +  self.noise_precision * design_matrix_dot_product)\n        \n        # Update the mean, shape (2, 1)\n        self.post_mean = self.post_cov.dot( \n                         inv_prior_cov.dot(self.prior_mean) + \n                         self.noise_precision * design_matrix.T.dot(targets))\n\n        \n        # Update the posterior distribution\n        self.param_posterior = multivariate_normal(self.post_mean.flatten(), self.post_cov)\n                \n    def compute_design_matrix(self, features: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute the design matrix. To keep things simple we use simple linear\n        regression and add the value phi_0 = 1 to our input data.\n        \n        Args:\n            features: numpy array of features\n        Returns:\n            design_matrix: numpy array of transformed features\n            \n        &gt;&gt;&gt; compute_design_matrix(np.array([2, 3]))\n        np.array([[1., 2.], [1., 3.])\n        \"\"\"\n        n_samples = len(features)\n        phi_0 = np.ones(n_samples)\n        design_matrix = np.stack((phi_0, features), axis=1)\n        return design_matrix\n    \n \n    def predict(self, features: np.ndarray):\n        \"\"\"\n        Compute predictive posterior given new datapoint\n        \n        Args:\n            features: 1d numpy array of features\n        Returns:\n            pred_posterior: predictive posterior distribution\n        \"\"\"\n        design_matrix = self.compute_design_matrix(features)\n        \n        pred_mean = design_matrix.dot(self.post_mean)\n        pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n        \n        pred_posterior = univariate_normal(loc=pred_mean.flatten(), scale=pred_cov**0.5)\n        return pred_posterior\n\n\n\n6.4 Visualizing the parameter posterior \nTo ensure that our implementation is correct we can visualize how the posterior over the parameters changes as the model sees more data. We will visualize the distribution using a contour plot - a method for visualizing three-dimensional functions. In our case we want to visualize the density of our bi-variate Gaussian for each point (that is, each slope/intercept combination). The plot below shows an example which illustrates how the lines and colours of a contour plot correspond to a Gaussian distribution:\n\nAs we can see, the density is highest in the yellow regions decreasing when moving further out into the green and blue parts. This should give you a better understanding of contour plots.\nTo analyze our Bayesian linear regression class we will start by initializing a new model. We can visualize its prior distribution over the parameters before the model has seen any real data.\n\n# Initialize BLR model\nprior_mean = np.array([0, 0])\nprior_cov = 1/2 * np.identity(2)\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\ndef plot_param_posterior(lower_bound, upper_bound, blr, title):\n    fig = plt.figure()\n    mesh_features, mesh_labels = np.mgrid[lower_bound:upper_bound:.01, lower_bound:upper_bound:.01]\n    pos = np.dstack((mesh_features, mesh_labels))\n    plt.contourf(mesh_features, mesh_labels, blr.param_posterior.pdf(pos), levels=15)\n    plt.scatter(intercept, slope, color='red', label=\"True parameter values\")\n    plt.title(title)\n    plt.xlabel(\"Intercept\")\n    plt.ylabel(\"Slope\")\n    plt.legend();\n    \n# Visualize parameter prior distribution\nplot_param_posterior(lower_bound, upper_bound, blr, title=\"Prior parameter distribution\")\n\n\n\n\nThe plot above illustrates both the prior parameter distribution and the true parameter values that we want to find. If our model works correctly, the posterior distribution should become more narrow and move closer to the true parameter values as the model sees more datapoints. This can be visualized with contour plots, too! Below we update the posterior distribution iteratively as the model sees more and more data. The contour plots for each step show how the parameter posterior develops and converges close to the true values in the end.\n\nn_points_lst = [1, 5, 10, 50, 100, 200, 500, 1000]\nprevious_n_points = 0\nfor n_points in n_points_lst:\n    train_features = features[previous_n_points:n_points]\n    train_labels = noise_corrupted_labels[previous_n_points:n_points]\n    blr.update_posterior(train_features, train_labels)\n    \n    # Visualize updated parameter posterior distribution\n    plot_param_posterior(lower_bound, \n                         upper_bound, \n                         blr, \n                         title=f\"Updated parameter distribution using {n_points} datapoints\")\n    \n    previous_n_points = n_points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5 Step 3: Posterior predictive distribution \nGiven the posterior distribution over the parameters we can determine the predictive distribution (= posterior over the outputs) for a new input \\((\\boldsymbol{x}_*, y_*)\\). This is the distribution we are really interested in. A trained model is not particularly useful when we can’t use it to make predictions, right?\nThe posterior predictive distribution looks as follows:\n\\[\n\\begin{aligned}\np\\left(y_{*} \\mid \\mathcal{X}, \\mathcal{Y}, \\boldsymbol{x}_{*}\\right) &=\\int p\\left(y_{*} \\mid \\boldsymbol{x}_{*}, \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\int \\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{\\theta}, \\sigma^{2}\\right) \\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{m}_{N}, \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\right)\n\\end{aligned}\n\\]\nFirst of all: note that the predictive posterior for a new input \\(\\boldsymbol{x}_{*}\\) is a univariate Gaussian distribution. We can see that the mean of the distribution is given by the product of the design matrix for the new example (\\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right)\\)) and the mean of the parameter posterior (\\(\\boldsymbol{m}_{N}\\)). The variance \\((\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\)) of the predictive posterior has two parts: 1. \\(\\sigma^{2}\\): The variance of the noise 2. \\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)\\): The posterior uncertainty associated with the parameters \\(\\boldsymbol{\\theta}\\)\nLet’s add a predict method to our BayesianLinearRegression class which computes the predictive posterior for a new input (you will find the method in the class definition above):\n\ndef predict(self, features: np.ndarray):\n    \"\"\"\n    Compute predictive posterior given new datapoint\n\n    Args:\n        features: 1d numpy array of features\n    Returns:\n        pred_posterior: predictive posterior distribution\n    \"\"\"\n    design_matrix = self.compute_design_matrix(features)\n\n    pred_mean = design_matrix.dot(self.post_mean)\n    pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n\n    pred_posterior = univariate_normal(pred_mean.flatten(), pred_cov)\n    return pred_posterior\n\n\n\n6.6 Visualizing the predictive posterior \nOur original dataset follows a simple linear function. After training the model it should be able to predict labels for new datapoints, even if they lie beyond the range from [-1.5, 1.5]. But how can we get from the predictive distribution that our model computes to actual labels? That’s easy: we sample from the predictive posterior.\nTo make sure that we are all on the same page: given a new input example our Bayesian linear regression model predicts not a single label but a distribution over possible labels. This distribution is Gaussian. We can get actual labels by sampling from this distribution.\nThe code below implements and visualizes this: - We create some test features for which we want predictions - Each feature is given to the trained BLR model which returns a univariate Gaussian distribution over possible labels (pred_posterior = blr.predict(np.array([feat]))) - We sample from this distribution (sample_predicted_labels = pred_posterior.rvs(size=sample_size)) - The predicted labels are saved in a format that makes it easy to plot them - Finally, we plot each input feature, its true label and the sampled predictions. Remember: the samples are generated from the predictive posterior returned by the predict method. Think of a Gaussian distribution plotted along the y-axis for each feature. We visualize this with a histogram: more likely values close to the mean will be sampled more often than less likely values.\n\nimport pandas as pd\nimport seaborn as sns\n\nall_rows = []\nsample_size = 1000\ntest_features = [-2, -1, 0, 1, 2]\nall_labels = []\n\nfor feat in test_features:\n    true_label = compute_function_labels(slope, intercept, 0, np.array([feat]))\n    all_labels.append(true_label)\n    pred_posterior = blr.predict(np.array([feat]))\n    sample_predicted_labels = pred_posterior.rvs(size=sample_size)\n    for label in sample_predicted_labels:\n        all_rows.append([feat, label])\n        \nall_data = pd.DataFrame(all_rows, columns=[\"feature\", \"label\"]) \nsns.displot(data=all_data, x=\"feature\", y=\"label\")\nplt.scatter(x=test_features, y=all_labels, color=\"red\", label=\"True values\")\nplt.title(\"Predictive posterior distributions\")\nplt.legend()\nplt.plot();"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "href": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "title": "Bayesian linear regression",
    "section": "Sources and further reading ",
    "text": "Sources and further reading \nThe basis for this notebook is chapter 9.2 of the book Mathematics for Machine Learning. I can highly recommend to read through chapter 9 to get a deeper understanding of (Bayesian) linear regression.\nYou will find explanations and an implementation of simple linear regression in the notebook on linear regression"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "href": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "title": "Baysian Linear Regression blog",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nNonlinear Features\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nPolynomial Regression class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nNow lets try different polynomial fits.\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\nYou can refer 9.1 and 9.2 section of Mathematics for Machine Learning to understand in depth about probalistic approch to linear regression."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "href": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "title": "Baysian Linear Regression blog",
    "section": "Bayes theroem",
    "text": "Bayes theroem\nThe basis of bayesian linear regression is bayes theroem. - Bayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\] - \\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters. - \\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely. - \\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "href": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "title": "Baysian Linear Regression blog",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nHave a look at Wiki"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "href": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "title": "Baysian Linear Regression blog",
    "section": "Bayeisan approch",
    "text": "Bayeisan approch\nUnlike linear regression where we computed point estimates of our parameters using maximum likelihood approach and make predictions, here in Bayesian linear regression we estimate\nFollowing are the steps: 1. We assume a that we know standard deviation of the noise, mean and covariance of the prior. 2. We than calculate parameter posteriori 3. Based on that we make posteriori predictions on unseen data ie. test data.\nNow lets see along with code\nHere we have same assumptions that we took in linear regression \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] Where epsilon is the noise from normal distribution with variance \\(\\sigma^2\\). Training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nFunction\n\ndef g(x, mu, sigma):   \n    epsilon = np.random.normal(mu, sigma, size=(x.shape))\n    return np.cos(x) + epsilon\n    \n\nWe apply non linear feature transformation on feature matrix with polynomial of degree \\(K\\) \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nSample to see nonlinear transformation\n\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n\n\npoly_features(X, 3) # defined in linear regression section\n\narray([[  1.,  -3.,   9., -27.],\n       [  1.,  -1.,   1.,  -1.],\n       [  1.,   0.,   0.,   0.],\n       [  1.,   1.,   1.,   1.],\n       [  1.,   3.,   9.,  27.]])"
  }
]