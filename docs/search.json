[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "title": "Baysian Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating\n\\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/Untitled-1.html",
    "href": "blogs/blogsData/Untitled-1.html",
    "title": "Demo",
    "section": "",
    "text": "print('hello')\n\nhello"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJul 12, 2023\n\n\nImage Super Resolution\n\n\nSuraj Jaiswal\n\n\n\n\nJun 18, 2023\n\n\nBaysian Linear Regression blog\n\n\nSuraj Jaiswal\n\n\n\n\nJun 12, 2023\n\n\nPlots for bayesian LR\n\n\nSuraj Jaiswal\n\n\n\n\nJun 5, 2023\n\n\nBaysian Linear Regression\n\n\nSuraj Jaiswal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Welcome to my blog! I’m Suraj Jaiswal, an M.Tech student at IIT Gandhinagar, pursuing Computer Science and Engineering under the mentorship of Dr. Nipun Batra.  Here, I’ll share my experiences, research projects, and insights as we delve into the exciting world of technology. IIT Gandhinagar, known for its excellence in education and research, provides the ideal environment for me to explore the limitless possibilities of computer science."
  },
  {
    "objectID": "blogs/blogsData/demo.html",
    "href": "blogs/blogsData/demo.html",
    "title": "Plots for bayesian LR",
    "section": "",
    "text": "import random\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\n\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n9.3\n$$                                    \\\n                       \\\nx_{n}       y_{n}   \\\nn = 1,……,N \\\n\\\n$$\nfigure 9.5\n\nN = 10\nmu = 0\nsigma = 0.2**2\n\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.2123287 ],\n       [-0.18826531]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\nml_predictions.shape\n\n(10, 1)\n\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nFigure 9.6\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n%config InlineBackend.figure_format = \"retina\"\n\n\ndef f(x, a): return a[0] + a[1] * x\n\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + beta * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5,30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()"
  },
  {
    "objectID": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "href": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Table of Symbols from the book Mathematics for Machine Learning: https://mml-book.github.io/. Latex was provided by the co-author Cheng Soon Ong (Many Thanks) and edited by Harry Wang: https://github.com/mml-book/mml-book.github.io/issues/634\nSee latex version on overleaf.com: https://www.overleaf.com/read/mnzgdyrsjfsk\n$\n% vector bf: boldface\n% matrix % transpose % inverse\n% set cal: calligraphic letters % dimension, rm: roman typestyle % rank\n% determinant % identity mapping % kernel/nullspace % image\n% generating set\n% tensor % trace\n% lagrangian % likelihood % variance % expectation % covariance\n% given % Gaussian distribution\n% other distributions $\n\n\n\n\n\n\n\nSymbol                             \nTypical Meaning\n\n\n\n\n\\(a,b,c, \\alpha,\\beta,\\gamma\\)\nScalars are lowercase\n\n\n\\(\\mathbf{x},\\mathbf{y},\\mathbf{z}\\)\nVectors are bold lowercase\n\n\n\\(\\mathbf{A},\\mathbf{B},\\mathbf{C}\\)\nMatrices are bold uppercase\n\n\n\\(\\mathbf{x} ^\\top, \\mathbf{A} ^\\top\\)\nTranspose of a vector or matrix\n\n\n\\(\\mathbf{A}^{-1}\\)\nInverse of a matrix\n\n\n\\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\)\nInner product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(\\mathbf{x} ^\\top\\mathbf{y}\\)\nDot product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(B = (\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3)\\)\n(Ordered) tuple\n\n\n\\(\\mathbf{B} = [\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3]\\)\nMatrix of column vectors stacked horizontally\n\n\n\\(\\mathcal{B} = \\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}\\)\nSet of vectors (unordered)\n\n\n\\(\\mathbb Z,\\mathbb N\\)\nIntegers and natural numbers, respectively\n\n\n\\(\\mathbb R,\\mathbb C\\)\nReal and complex numbers, respectively\n\n\n\\(\\mathbb R^n\\)\n\\(n\\)-dimensional vector space of real numbers\n\n\n\\(\\forall x\\)\nUniversal quantifier: for all \\(x\\)\n\n\n\\(\\exists x\\)\nExistential quantifier: there exists \\(x\\)\n\n\n\\(a := b\\)\n\\(a\\) is defined as \\(b\\)\n\n\n\\(a =:b\\)\n\\(b\\) is defined as \\(a\\)\n\n\n\\(a\\propto b\\)\n\\(a\\) is proportional to \\(b\\), i.e., \\(a =\\text\\{constant\\}\\cdot b\\)\n\n\n\\(g\\circ f\\)\nFunction composition: \\(g\\) after \\(f\\)\n\n\n\\(\\iff\\)\nIf and only if\n\n\n\\(\\implies\\)\nImplies\n\n\n\\(\\mathcal{A}, \\mathcal{C}\\)\nSets\n\n\n\\(a \\in \\mathcal{A}\\)\n\\(a\\) is an element of set \\(\\mathcal{A}\\)\n\n\n\\(\\emptyset\\)\nEmpty set\n\n\n\\(\\mathcal{A}\\setminus \\mathcal{B}\\)\n\\(\\mathcal{A}\\) without \\(\\mathcal{B}\\): the set of elements in \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\n\n\\(D\\)\nNumber of dimensions; indexed by \\(d=1,\\dots,D\\)\n\n\n\\(N\\)\nNumber of data points; indexed by \\(n=1,\\dots,N\\)\n\n\n\\(\\mathbf{I}_m\\)\nIdentity matrix of size \\(m\\times m\\)\n\n\n\\(\\mathbf{0}_{m,n}\\)\nMatrix of zeros of size \\(m\\times n\\)\n\n\n\\(\\mathbf{1}_{m,n}\\)\nMatrix of ones of size \\(m\\times n\\)\n\n\n\\(\\mathbf{e}_i\\)\nStandardcanonical vector (where \\(i\\) is the component that is \\(1\\))\n\n\n\\(\\mathrm{dim}\\)\nDimensionality of vector space\n\n\n\\(\\mathrm{rk}(\\mathbf{A})\\)\nRank of matrix \\(\\mathbf{A}\\)\n\n\n\\(\\mathrm{Im}(\\Phi)\\)\nImage of linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{ker}(\\Phi)\\)\nKernel (null space) of a linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{span}[\\mathbf{b}_1]\\)\nSpan (generating set) of \\(\\mathbf{b}_1\\)\n\n\n\\(\\text{tr}(\\mathbf{A})\\)\nTrace of \\(\\mathbf{A}\\)\n\n\n\\(\\det(\\mathbf{A})\\)\nDeterminant of \\(\\mathbf{A}\\)\n\n\n\\(| \\cdot |\\)\nAbsolute value or determinant (depending on context)\n\n\n\\(\\| {\\cdot} \\|\\)\nNorm; Euclidean, unless specified\n\n\n\\(\\lambda\\)\nEigenvalue or Lagrange multiplier\n\n\n\\(E_\\lambda\\)\nEigenspace corresponding to eigenvalue \\(\\lambda\\)\n\n\n\\(\\mathbf{x} \\perp \\mathbf{y}\\)\nVectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal\n\n\n\\(V\\)\nVector space\n\n\n\\(V^\\perp\\)\nOrthogonal complement of vector space \\(V\\)\n\n\n\\(\\sum_{n=1}^N x_n\\)\nSum of the \\(x_n\\): \\(x_1 + \\dotsc + x_N\\)\n\n\n\\(\\prod_{n=1}^N x_n\\)\nProduct of the \\(x_n\\): \\(x_1 \\cdot\\dotsc \\cdot x_N\\)\n\n\n\\(\\mathbf{\\theta}\\)\nParameter vector\n\n\n\\(\\frac{\\partial f}{\\partial x}\\)\nPartial derivative of \\(f\\) with respect to \\(x\\)\n\n\n\\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\)\nTotal derivative of \\(f\\) with respect to \\(x\\)\n\n\n$$\nGradient\n\n\n\\(f_* = \\min_x f(x)\\)\nThe smallest function value of \\(f\\)\n\n\n\\(x_* \\in \\arg\\min_x f(x)\\)\nThe value \\(x_*\\) that minimizes \\(f\\) (note: \\(\\arg\\min\\) returns a set of values)\n\n\n\\(\\mathfrak{L}\\)\nLagrangian\n\n\n\\(\\mathcal{L}\\)\nNegative log-likelihood\n\n\n\\(\\binom{n}{k}\\)\nBinomial coefficient, \\(n\\) choose \\(k\\)\n\n\n\\(\\mathbb{V}_X[\\mathbf{x}]\\)\nVariance of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathbb{E}_X[\\mathbf{x}]\\)\nExpectation of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathop{\\mathrm{Cov}}_{X,Y}[\\mathbf{x}, \\mathbf{y}]\\)\nCovariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).\n\n\n\\(X \\perp\\kern-5pt \\perp Y\\vert Z\\)\n\\(X\\) is conditionally independent of \\(Y\\) given \\(Z\\)\n\n\n\\(X\\sim p\\)\nRandom variable \\(X\\) is distributed according to \\(p\\)\n\n\n\\(\\mathcal{N}\\big(\\mathbf{\\mu},\\mathbf{\\Sigma}\\big)\\)\nGaussian distribution with mean \\(\\mathbf{\\mu}\\) and covariance \\(\\mathbf{\\Sigma}\\)\n\n\n\\(\\text{Ber}(\\mu)\\)\nBernoulli distribution with parameter \\(\\mu\\)\n\n\n\\(\\text{Bin}(N, \\mu)\\)\nBinomial distribution with parameters \\(N, \\mu\\)\n\n\n\\(\\text{Beta}(\\alpha, \\beta)\\)\nBeta distribution with parameters \\(\\alpha, \\beta\\)\n\n\n\n\nθ\nyn\nσ\nxn\nn = 1, . . . , N\n\n\n$ L(\\theta, σ | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, σ) $ \n\nSyntaxError: invalid syntax (2183852464.py, line 1)\n\n\n\nfrom re import L\n\n\nL(\\theta, \\sigma | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, \\sigma)"
  },
  {
    "objectID": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "href": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "title": "Bayesian Learning in a Linear Basis Function Model",
    "section": "",
    "text": "In this notebook we ilustrate the bayesian learning in a linear basis function model, as well as the sequential update of a posterior distribution.\nTaken from Christopher Bishop’s Pattern Recognition and Machine Learning book (p.155)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n\n%config InlineBackend.figure_format = \"retina\"\n\nWe consider an input \\(x\\), a target variable \\(t\\) and a linear model of the form \\[\n    y(x, {\\bf w}) = w_0 + w_1x\n\\]\n\ndef f(x, a): return a[0] + a[1] * x\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# plot_sample_w(0, )\n\n\nplt.scatter(X, T, c=\"crimson\")\nplt.grid(alpha=0.5)\nplt.plot(X, f(X,a))\n\n\n\n\nOur goal is to recover the values \\(w_0\\) and \\(w_1\\) from the data.\nRecall: \\[\n    p({\\textbf w}|t) \\propto p(t| {\\textbf w}, \\beta) p(\\bf{w})\n\\]\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\n\\(t \\sim \\mathcal{N}\\left({\\bf w}^Tx, \\beta^{-1}\\right)\\) (The assigned probability for target variables)\n\nThe posterior distribution of \\(\\bf w\\) after \\(N\\) observations is given by\n\\[\n\\begin{align}\n    m_N &= S_N(S_0^{-1}m_0 + \\beta\\Phi^T{\\bf t}) \\\\\n    S_N^{-1} &= S_0^{-1} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\nWith * \\(\\Phi\\in\\mathbb{R}^{N\\times M}\\) * \\({\\bf t}\\in\\mathbb{R}^N\\)\nIf no data has been yet seen, we consider \\[\nw \\sim \\mathcal{N}\\left(0, \\alpha^{-1}\\text{I}\\right)\n\\] Which results in a posterior distribution of the form\n\\[\n\\begin{align}\n    m_N &= \\beta S_N\\Phi^T{\\bf t} \\\\\n    S_N^{-1} &= \\alpha \\text{I} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * Phi.T @ Phi)\n    mN = SN @ (inv(S0) @ m0 + beta * Phi.T @ t)\n    return SN, mN\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in] #  concatenate arrays along the second axis\n    t = T[[ix]]\n    return Phi, t\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n        \ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\n\nseed(1643)\nN = 20\nnobs = [1, 2, 20]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N+1):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nIn the limit of an infinite number of datapoints, the posterior distribution would become a delta function centered on the true parameter values, shown by the white cross.\nOther forms of prior over parameters can be considered. For example, the generalized Gaussian:\n\\[\n    p({\\bf w}|\\alpha) = \\left[\\frac{q}{2}\\left(\\frac{q}{2}\\right)^{1/q}\\frac{1}{\\Gamma(1/q)}\\right]^M \\exp\\left(-\\frac{\\alpha}{2}\\sum_{j=1}^M|w_j|^q\\right)\n\\]\nFinding the maximum posterior distribution over \\({\\bf w}\\) corresponds to minimization of the regularized error function given by:\n\\[\n    \\frac{1}{2}\\sum_{n=1}^N\\left(t_n - {\\bf w}^T {\\bf\\phi}({\\bf x}_n)\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_j|^q\n\\]\nThe maximum posterior weight vector \\({\\bf w}_{\\text{MAP}}\\) considering a generalized Gaussian will not be (in every case) the mean, since the mean will not coincide with the mode."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html",
    "href": "blogs/blogsData/blr_blog.html",
    "title": "Baysian Linear Regression blog",
    "section": "",
    "text": "Welcome to my blog on Bayesian linear regression, where we explore the power of this technique. While traditional linear regression provides point estimates, Bayesian linear regression incorporates prior knowledge and quantifies uncertainty. By combining observed data with prior beliefs, we make more informed decisions. Throughout this blog, we’ll delve into key components like probalistic approch to linear regression, basics of types of uncertainity, prior distributions, likelihood functions, and posterior inference. Let’s embark on this enlightening journey together."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "href": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "title": "Baysian Linear Regression blog",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta_{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta_{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X])\n\n\ndef max_lik_estimate(X, y):\n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_ml = max_lik_estimate(X_aug,y)\nprint(theta_ml)\n\n[[2.116]\n [0.499]]\n\n\nNow we will make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), \\[\n\\ \\boldsymbol y_{\\text{pred}} = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta_{\\text{ML}}\n\\]\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\nml_prediction = Xtest_aug @ theta_ml\n\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\nText(0, 0.5, '$y$')\n\n\n\n\n\nThis gives fairly good results but what if the data is bit complex\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\nN = 10\nmu = 0\nsigma = 0.2**2\nseed(10)\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nLets first apply linear regressoin without non linear transformation\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.47109666],\n       [-0.1808517 ]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#known-entities",
    "href": "blogs/blogsData/blr_blog.html#known-entities",
    "title": "Baysian Linear Regression blog",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\\[\n\\boxed{\\begin{array}{l}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ m_{0} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ S_{0}\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\searrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ x_{n} \\ \\ \\rightarrow \\ \\ y_{n} \\ \\ \\\\\n\\ \\ \\ \\ n\\ =\\ 1,......,N\\ \\\\\n\\end{array}}\n\\] \\[ Graphical \\ model \\ for \\ Bayeisan \\ linear \\ regression \\]\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, m0, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#posterior",
    "href": "blogs/blogsData/blr_blog.html#posterior",
    "title": "Baysian Linear Regression blog",
    "section": "Posterior",
    "text": "Posterior\n\nParameter posteriori in closed form\nCalculating Parameter posterior: \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\n\n\nPosterior Predictive distribution\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\n\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}}) \\, |\\ X, Y, \\boldsymbol X_{\\text{test}}) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol M_{\\text{n}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol S_{\\text{N}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top + \\sigma ^ 2)\n\\end{align} \\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\n# conf_bound1 = np.sqrt(var_blr).flatten()\n# plt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\n# 95 % parameter uncertainity\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"r\")\n\n# 95 % total uncertainity ie. \nconf_bound3 = 2.0*np.sqrt(var_blr + sigma**2).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\", '95% para uncertainity', '95% total uncertainity'])\nplt.xlabel('$x$');\nplt.ylabel('$y$');\n\n\n\n\nYou can refer 9.3 section of Mathematics for Machine Learning to understand in depth about bayesian linear regression.\n\n\nVisulizing the parameter Posterior\nIn this section we will visualize the posterior and will see how it changes as it sees more data.\n\ndef f(x, a): return a[0] + a[1] * x\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(sigma **2)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + ((1 / sigma) ** 2) * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + ((1 / sigma) ** 2) * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5]) # true parameter values\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# beta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5, 15, 30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$$\\theta_0$$\")\n            ax[ix_fig, l].set_ylabel(\"$$\\theta_1$$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nWe can see above as the model see more data, the posterior converges close the the true values at end. Refer to Bishop - Pattern Recognition and Machine Learning fig 3.7 to understand above fig in detail."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html",
    "href": "demo_notebooks/bayesian_linear_regression.html",
    "title": "Bayesian linear regression",
    "section": "",
    "text": "1. What is Bayesian linear regression?\n2. Recap linear regression\n3. Fundamental concepts\n4. Linear regression from a probabilistic perspective\n5. Linear regression with basis functions\n\n5.1 Example basis functions\n5.2 The design matrix\n\n6. Bayesian Linear Regression\n\n6.1 Step 1: Probabilistic Model\n6.2 Generating a dataset\n6.3 Step 2: Posterior over the parameters\n6.4 Visualizing the parameter posterior\n6.5 Step 3: Posterior predictive distribution\n6.6 Visualizing the predictive posterior\n\nSources and further reading"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "href": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "title": "Bayesian linear regression",
    "section": "Link to interactive demo",
    "text": "Link to interactive demo\nClick here to run the notebook online (using Binder) without installing jupyter or downloading the code.\nSometimes, the GitHub version of the Jupyter notebook does not display the math formulas correctly. Please refer to the Binder version in case you think something might be off or missing.\nI also wrote a blog post containing the contents of the notebook."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "href": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "title": "Bayesian linear regression",
    "section": "1. What is Bayesian linear regression (BLR)? ",
    "text": "1. What is Bayesian linear regression (BLR)? \nBayesian linear regression is the Bayesian interpretation of linear regression. What does that mean? To answer this question we first have to understand the Bayesian approach. In most of the algorithms we have looked at so far we computed point estimates of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function. In the Bayesian approach we don’t work with exact values but with probabilities. This allows us to model the uncertainty in our parameter estimates. Why is this important?\nIn nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty. Bayesian methods have been around for a long time and are widely-used in many areas of science (e.g. astronomy). Although Bayesian methods have been applied to machine learning problems too, they are usually less well known to beginners. The major reason is that they require a good understanding of probability theory.\nIn the following notebook we will work our way from linear regression to Bayesian linear regression, including the most important theoretical knowledge and code examples."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "title": "Bayesian linear regression",
    "section": "2. Recap linear regression ",
    "text": "2. Recap linear regression \n\nIn linear regression, we want to find a function \\(f\\) that maps inputs \\(x \\in \\mathbb{R}^D\\) to corresponding function values \\(f(x) \\in \\mathbb{R}\\).\nWe are given an input dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\), where \\(y_n\\) is a noisy observation value: \\(y_n = f(x_n) + \\epsilon\\), with \\(\\epsilon\\) being an i.i.d. random variable that describes measurement/observation noise\nOur goal is to infer the underlying function \\(f\\) that generated the data such that we can predict function values at new input locations\nIn linear regression, we model the underlying function \\(f\\) using a linear combination of the input features:\n\n\\[\n\\begin{split}\ny &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d \\\\\n&= \\boldsymbol{x}^T \\boldsymbol{\\theta}\n\\end{split}\n\\]\n\nFor more details take a look at the notebook on linear regression"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "href": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "title": "Bayesian linear regression",
    "section": "3. Fundamental concepts ",
    "text": "3. Fundamental concepts \n\nOne fundamental tool in Bayesian learning is Bayes’ theorem\nBayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\]\n\\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters.\n\\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely.\n\\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets.\n\n\\(p(\\boldsymbol{\\theta} | \\boldsymbol{x}, y)\\) is the posterior. It describes the probability of the parameters given the observed data and targets. \nAnother important tool you need to know about is the Gaussian distribution. If you are not familiar with it I suggest you pause for a minute and understand its main properties before reading on.\n\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nFor more information take a look at the Wikipedia article on Bayesian inference."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "title": "Bayesian linear regression",
    "section": "4. Linear regression from a probabilistic perspective ",
    "text": "4. Linear regression from a probabilistic perspective \nIn order to pave the way for Bayesian linear regression we will take a probabilistic spin on linear regression. Let’s start by explicitly modelling the observation noise \\(\\epsilon\\). For simplicity, we assume that \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nAs mentioned in the beginning, a simple linear regression model assumes that the target function \\(f(x)\\) is given by a linear combination of the input features: \\[\n\\begin{split}\ny = f(\\boldsymbol{x}) + \\epsilon \\\\\n  = \\boldsymbol{x}^T \\boldsymbol{\\theta} + \\epsilon\n\\end{split}\n\\]\nThis corresponds to the following likelihood function: \\[p(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{x}^T \\boldsymbol{\\theta}, \\sigma^2)\\]\nOur goal is to find the parameters \\(\\boldsymbol{\\theta} = \\{\\theta_1, ..., \\theta_D\\}\\) that model the given data best. In standard linear regression we can find the best parameters using a least-squares, maximum likelihood (ML) or maximum a posteriori (MAP) approach. If you want to know more about these solutions take a look at the notebook on linear regression or at chapter 9.2 of the book Mathematics for Machine Learning."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "title": "Bayesian linear regression",
    "section": "5. Linear regression with basis functions ",
    "text": "5. Linear regression with basis functions \nThe simple linear regression model above is linear not only with respect to the parameters \\(\\boldsymbol{\\theta}\\) but also with respect to the inputs \\(\\boldsymbol{x}\\). When \\(\\boldsymbol{x}\\) is not a vector but a single value (that is, the dataset is one-dimensional) the model \\(y_i = x_i \\cdot \\theta\\) describes straight lines with \\(\\theta\\) being the slope of the line.\nThe plot below shows example lines produced with the model \\(y = x \\cdot \\theta\\), using different values for the slope \\(\\theta\\) and intercept 0.\n\nHaving a model which is linear both with respect to the parameters and inputs limits the functions it can learn significantly. We can make our model more powerful by making it nonlinear with respect to the inputs. After all, linear regression refers to models which are linear in the parameters, not necessarily in the inputs (linear in the parameters means that the model describes a function by a linear combination of input features).\nMaking the model nonlinear with respect to the inputs is easy. We can adapt it by using a nonlinear transformation of the input features \\(\\phi(\\boldsymbol{x})\\). With this adaptation our model looks as follows: \\[\n\\begin{split}\ny &= \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon \\\\\n&= \\sum_{k=0}^{K-1} \\theta_k \\phi_k(\\boldsymbol{x}) + \\epsilon\n\\end{split}\n\\]\nWhere \\(\\boldsymbol{\\phi}: \\mathbf{R}^D \\rightarrow \\mathbf{R}^K\\) is a (non)linear transformation of the inputs \\(\\boldsymbol{x}\\) and \\(\\phi_k: \\mathbf{R}^D \\rightarrow \\mathbf{R}\\) is the \\(k-\\)th component of the feature vector \\(\\boldsymbol{\\phi}\\):\n\\[\n\\boldsymbol{\\phi}(\\boldsymbol{x})=\\left[\\begin{array}{c}\n\\phi_{0}(\\boldsymbol{x}) \\\\\n\\phi_{1}(\\boldsymbol{x}) \\\\\n\\vdots \\\\\n\\phi_{K-1}(\\boldsymbol{x})\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith our new nonlinear transformation the likelihood function is given by\n\\[\np(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta},\\, \\sigma^2)\n\\]\n\n5.1 Example basis functions \n\nLinear regression\nThe easiest example for a basis function (for one-dimensional data) would be simple linear regression, that is, no non-linear transformation at all. In this case we would choose \\(\\phi_0(x) = 1\\) and \\(\\phi_i(x) = x\\). This would result in the following vector \\(\\boldsymbol{\\phi}(x)\\):\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\n\\vdots \\\\\nx\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\n\n\nPolynomial regression\nAnother common choice of basis function for the one-dimensional case is polynomial regression. For this we would set \\(\\phi_i(x) = x^i\\) for \\(i=0, ..., K-1\\). The corresponding feature vector \\(\\boldsymbol{\\phi}(x)\\) would look as follows:\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\nx^2 \\\\\nx^3 \\\\\n\\vdots \\\\\nx^{K-1}\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith this transformation we can lift our original one-dimensional input into a \\(K\\)-dimensional feature space. Our function \\(f\\) can be any polynomial with degree \\(\\le K-1\\): \\(f(x) = \\sum_{k=0}^{K-1} \\theta_k x^k\\)\n\n\n\n5.2 The design matrix \nTo make it easier to work with the transformations \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\) for the different input vectors \\(\\boldsymbol{x}\\) we typically create a so called design matrix (also called feature matrix). Given our dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\) we define the design matrix as follows:\n\\[\n\\boldsymbol{\\Phi}:=\\left[\\begin{array}{c}\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\vdots \\\\\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n\\phi_{0}\\left(\\boldsymbol{x}_{1}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{2}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{2}\\right) \\\\\n\\vdots & & \\vdots \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{N}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right] \\in \\mathbb{R}^{N \\times K}\n\\]\nNote that the design matrix is of shape \\(N \\times K\\). \\(N\\) is the number of input examples and \\(K\\) is the output dimension of the non-linear transformation \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\)."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "title": "Bayesian linear regression",
    "section": "6. Bayesian linear regression ",
    "text": "6. Bayesian linear regression \nWhat changes when we consider a Bayesian interpretation of linear regression? Our data stays the same as before: \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\). Given the data \\(D\\) we can define the set of all inputs as \\(\\mathcal{X} := \\{\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n\\}\\) and the set of all targets as \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\).\nIn simple linear regression we compute point estimates of our parameters (e.g. using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates distributions over the parameters and predictions. This allows us to model the uncertainty in our predictions.\nTo perform Bayesian linear regression we follow three steps: 1. We set up a probabilistic model that describes our assumptions how the data and parameters are generated 2. We perform inference for the parameters \\(\\boldsymbol{\\theta}\\), that is, we compute the posterior probability distribution over the parameters 3. With this posterior we can perform inference for new, unseen inputs \\(y_*\\). In this step we don’t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs.\n\n6.1 Step 1: Probabilistic model \nWe start by setting up a probabilistic model that describes our assumptions how the data and parameters are generated. For this, we place a prior \\(p(\\boldsymbol{\\theta})\\) over our parameters which encodes what parameter values are plausible (before we have seen any data). Example: With a single parameter \\(\\theta\\), a Gaussian prior \\(p(\\theta) = \\mathcal{N}(0, 1)\\) says that parameter values are normally distributed with mean 0 and standard deviation 1. In other words: the parameter values are most likely to fall into the interval [−2,2] which is two standard deviations around the mean value.\nTo keep things simple we will assume a Gaussian prior over the parameters: \\(p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{m}_0, \\boldsymbol{S}_0)\\). Let’s further assume that the likelihood function is Gaussian, too: \\(p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(y \\mid \\boldsymbol{\\phi}^{\\top}(\\boldsymbol{x}) \\boldsymbol{\\theta}, \\sigma^{2}\\right)\\).\nNote: When considering the set of all targets \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\), the likelihood function becomes a multivariate Gaussian distribution: \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\nThe nice thing about choosing a Gaussian distribution for our prior is that the posterior distributions will be Gaussian, too (keyword conjugate prior)!\nWe will start our BayesianLinearRegression class with the knowledge we have so far - our probabilistic model. As mentioned in the beginning we assume that the variance \\(\\sigma^2\\) of the noise \\(\\epsilon\\) is known. Furthermore, to allow plotting the data later on we will assume that it’s two dimensional (d=2).\n\nfrom scipy.stats import multivariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n        \n# Let's make sure that we can initialize our model\nprior_mean = np.array([0, 0])\nprior_cov = np.array([[0.5, 0], [0, 0.5]])\nnoise_var = 0.2\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\n\n\n6.2 Generating a dataset \nBefore going any further we need a dataset to test our implementation. Remember that we assume that our targets were generated by a function of the form \\(y = \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon\\) where \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nTo keep things simple we will work with one-dimensional data and simple linear regression (that is, no non-linear transformation of the inputs). Consequently, our data generating function will be of the form \\[ y = \\theta_0 + \\theta_1 \\, x + \\epsilon \\]\nNote that we added a parameter \\(\\theta_0\\) which corresponds to the intercept of the linear function. Until know we assumed \\(\\theta_0 = 0\\). As mentioned earlier, \\(\\theta_1\\) represents the slope of the linear function.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef compute_function_labels(slope: float, intercept: float, noise_std_dev: float, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute target values given function parameters and data.\n    \n    Args:\n        slope: slope of the function (theta_1)\n        intercept: intercept of the function (theta_0)\n        data: input feature values (x)\n        noise_std_dev: standard deviation of noise distribution (sigma)\n        \n    Returns:\n        target values, either true or corrupted with noise\n    \"\"\"\n    n_samples = len(data)\n    if noise_std_dev == 0: # Real function\n        return slope * data + intercept\n    else: # Noise corrupted\n        return slope * data + intercept + np.random.normal(0, noise_std_dev, n_samples)\n\n\n# Set random seed to ensure reproducibility\nseed = 42\nnp.random.seed(seed)\n\n# Generate true values and noise corrupted targets\nn_datapoints = 1000\nintercept = -0.7\nslope = 0.9\nnoise_std_dev = 0.5\nnoise_var = noise_std_dev**2\nlower_bound = -1.5\nupper_bound = 1.5\n\n# Generate dataset\nfeatures = np.random.uniform(lower_bound, upper_bound, n_datapoints)\nlabels = compute_function_labels(slope, intercept, 0., features)\nnoise_corrupted_labels = compute_function_labels(slope, intercept, noise_std_dev, features)\n\n\n# Plot the dataset\nplt.figure(figsize=(10,7))\nplt.plot(features, labels, color='r', label=\"True values\")\nplt.scatter(features, noise_corrupted_labels, label=\"Noise corrupted values\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Labels\")\nplt.title(\"Real function along with noisy targets\")\nplt.legend();\n\n\n\n\n\n\n6.3 Step 2: Posterior over the parameters \nWe finished setting up our probabilistic model. Next, we want to use this model and our dataset \\(\\mathcal{X, Y}\\) to estimate the parameter posterior \\(p(\\boldsymbol{\\theta} | \\mathcal{X, Y})\\). Keep in mind that we don’t compute point estimates of the parameters. Instead, we determine the mean and variance of the (Gaussian) posterior distribution and use this entire distribution when making predictions.\nWe can estimate the parameter posterior using Bayes theorem: \\[\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y})=\\frac{p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\mathcal{Y} \\mid \\mathcal{X})}\n\\]\n\n\\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})\\) is the likelihood function, \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\n\\(p(\\boldsymbol{\\theta})\\) is the prior distribution, \\(p(\\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{0}, \\boldsymbol{S}_{0}\\right)\\)\n\\(p(\\mathcal{Y} \\mid \\mathcal{X})=\\int p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\) is the evidence which ensures that the posterior is normalized (that is, that it integrates to 1).\n\nThe parameter posterior can be estimated in closed form (for proof see theorem 9.1 in the book Mathematics for Machine Learning): \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\nComing back to our BayesLinearRegression class we need to add a method which allows us to update the posterior distribution given a dataset.\n\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import norm as univariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n    def update_posterior(self, features: np.ndarray, targets: np.ndarray):\n        \"\"\"\n        Update the posterior distribution given new features and targets\n        \n        Args:\n            features: numpy array of features\n            targets: numpy array of targets\n        \"\"\"\n        # Reshape targets to allow correct matrix multiplication\n        # Input shape is (N,) but we need (N, 1)\n        targets = targets[:, np.newaxis]\n        \n        # Compute the design matrix, shape (N, 2)\n        design_matrix = self.compute_design_matrix(features)\n\n        # Update the covariance matrix, shape (2, 2)\n        design_matrix_dot_product = design_matrix.T.dot(design_matrix)\n        inv_prior_cov = np.linalg.inv(self.prior_cov)\n        self.post_cov = np.linalg.inv(inv_prior_cov +  self.noise_precision * design_matrix_dot_product)\n        \n        # Update the mean, shape (2, 1)\n        self.post_mean = self.post_cov.dot( \n                         inv_prior_cov.dot(self.prior_mean) + \n                         self.noise_precision * design_matrix.T.dot(targets))\n\n        \n        # Update the posterior distribution\n        self.param_posterior = multivariate_normal(self.post_mean.flatten(), self.post_cov)\n                \n    def compute_design_matrix(self, features: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute the design matrix. To keep things simple we use simple linear\n        regression and add the value phi_0 = 1 to our input data.\n        \n        Args:\n            features: numpy array of features\n        Returns:\n            design_matrix: numpy array of transformed features\n            \n        &gt;&gt;&gt; compute_design_matrix(np.array([2, 3]))\n        np.array([[1., 2.], [1., 3.])\n        \"\"\"\n        n_samples = len(features)\n        phi_0 = np.ones(n_samples)\n        design_matrix = np.stack((phi_0, features), axis=1)\n        return design_matrix\n    \n \n    def predict(self, features: np.ndarray):\n        \"\"\"\n        Compute predictive posterior given new datapoint\n        \n        Args:\n            features: 1d numpy array of features\n        Returns:\n            pred_posterior: predictive posterior distribution\n        \"\"\"\n        design_matrix = self.compute_design_matrix(features)\n        \n        pred_mean = design_matrix.dot(self.post_mean)\n        pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n        \n        pred_posterior = univariate_normal(loc=pred_mean.flatten(), scale=pred_cov**0.5)\n        return pred_posterior\n\n\n\n6.4 Visualizing the parameter posterior \nTo ensure that our implementation is correct we can visualize how the posterior over the parameters changes as the model sees more data. We will visualize the distribution using a contour plot - a method for visualizing three-dimensional functions. In our case we want to visualize the density of our bi-variate Gaussian for each point (that is, each slope/intercept combination). The plot below shows an example which illustrates how the lines and colours of a contour plot correspond to a Gaussian distribution:\n\nAs we can see, the density is highest in the yellow regions decreasing when moving further out into the green and blue parts. This should give you a better understanding of contour plots.\nTo analyze our Bayesian linear regression class we will start by initializing a new model. We can visualize its prior distribution over the parameters before the model has seen any real data.\n\n# Initialize BLR model\nprior_mean = np.array([0, 0])\nprior_cov = 1/2 * np.identity(2)\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\ndef plot_param_posterior(lower_bound, upper_bound, blr, title):\n    fig = plt.figure()\n    mesh_features, mesh_labels = np.mgrid[lower_bound:upper_bound:.01, lower_bound:upper_bound:.01]\n    pos = np.dstack((mesh_features, mesh_labels))\n    plt.contourf(mesh_features, mesh_labels, blr.param_posterior.pdf(pos), levels=15)\n    plt.scatter(intercept, slope, color='red', label=\"True parameter values\")\n    plt.title(title)\n    plt.xlabel(\"Intercept\")\n    plt.ylabel(\"Slope\")\n    plt.legend();\n    \n# Visualize parameter prior distribution\nplot_param_posterior(lower_bound, upper_bound, blr, title=\"Prior parameter distribution\")\n\n\n\n\nThe plot above illustrates both the prior parameter distribution and the true parameter values that we want to find. If our model works correctly, the posterior distribution should become more narrow and move closer to the true parameter values as the model sees more datapoints. This can be visualized with contour plots, too! Below we update the posterior distribution iteratively as the model sees more and more data. The contour plots for each step show how the parameter posterior develops and converges close to the true values in the end.\n\nn_points_lst = [1, 5, 10, 50, 100, 200, 500, 1000]\nprevious_n_points = 0\nfor n_points in n_points_lst:\n    train_features = features[previous_n_points:n_points]\n    train_labels = noise_corrupted_labels[previous_n_points:n_points]\n    blr.update_posterior(train_features, train_labels)\n    \n    # Visualize updated parameter posterior distribution\n    plot_param_posterior(lower_bound, \n                         upper_bound, \n                         blr, \n                         title=f\"Updated parameter distribution using {n_points} datapoints\")\n    \n    previous_n_points = n_points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5 Step 3: Posterior predictive distribution \nGiven the posterior distribution over the parameters we can determine the predictive distribution (= posterior over the outputs) for a new input \\((\\boldsymbol{x}_*, y_*)\\). This is the distribution we are really interested in. A trained model is not particularly useful when we can’t use it to make predictions, right?\nThe posterior predictive distribution looks as follows:\n\\[\n\\begin{aligned}\np\\left(y_{*} \\mid \\mathcal{X}, \\mathcal{Y}, \\boldsymbol{x}_{*}\\right) &=\\int p\\left(y_{*} \\mid \\boldsymbol{x}_{*}, \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\int \\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{\\theta}, \\sigma^{2}\\right) \\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{m}_{N}, \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\right)\n\\end{aligned}\n\\]\nFirst of all: note that the predictive posterior for a new input \\(\\boldsymbol{x}_{*}\\) is a univariate Gaussian distribution. We can see that the mean of the distribution is given by the product of the design matrix for the new example (\\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right)\\)) and the mean of the parameter posterior (\\(\\boldsymbol{m}_{N}\\)). The variance \\((\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\)) of the predictive posterior has two parts: 1. \\(\\sigma^{2}\\): The variance of the noise 2. \\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)\\): The posterior uncertainty associated with the parameters \\(\\boldsymbol{\\theta}\\)\nLet’s add a predict method to our BayesianLinearRegression class which computes the predictive posterior for a new input (you will find the method in the class definition above):\n\ndef predict(self, features: np.ndarray):\n    \"\"\"\n    Compute predictive posterior given new datapoint\n\n    Args:\n        features: 1d numpy array of features\n    Returns:\n        pred_posterior: predictive posterior distribution\n    \"\"\"\n    design_matrix = self.compute_design_matrix(features)\n\n    pred_mean = design_matrix.dot(self.post_mean)\n    pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n\n    pred_posterior = univariate_normal(pred_mean.flatten(), pred_cov)\n    return pred_posterior\n\n\n\n6.6 Visualizing the predictive posterior \nOur original dataset follows a simple linear function. After training the model it should be able to predict labels for new datapoints, even if they lie beyond the range from [-1.5, 1.5]. But how can we get from the predictive distribution that our model computes to actual labels? That’s easy: we sample from the predictive posterior.\nTo make sure that we are all on the same page: given a new input example our Bayesian linear regression model predicts not a single label but a distribution over possible labels. This distribution is Gaussian. We can get actual labels by sampling from this distribution.\nThe code below implements and visualizes this: - We create some test features for which we want predictions - Each feature is given to the trained BLR model which returns a univariate Gaussian distribution over possible labels (pred_posterior = blr.predict(np.array([feat]))) - We sample from this distribution (sample_predicted_labels = pred_posterior.rvs(size=sample_size)) - The predicted labels are saved in a format that makes it easy to plot them - Finally, we plot each input feature, its true label and the sampled predictions. Remember: the samples are generated from the predictive posterior returned by the predict method. Think of a Gaussian distribution plotted along the y-axis for each feature. We visualize this with a histogram: more likely values close to the mean will be sampled more often than less likely values.\n\nimport pandas as pd\nimport seaborn as sns\n\nall_rows = []\nsample_size = 1000\ntest_features = [-2, -1, 0, 1, 2]\nall_labels = []\n\nfor feat in test_features:\n    true_label = compute_function_labels(slope, intercept, 0, np.array([feat]))\n    all_labels.append(true_label)\n    pred_posterior = blr.predict(np.array([feat]))\n    sample_predicted_labels = pred_posterior.rvs(size=sample_size)\n    for label in sample_predicted_labels:\n        all_rows.append([feat, label])\n        \nall_data = pd.DataFrame(all_rows, columns=[\"feature\", \"label\"]) \nsns.displot(data=all_data, x=\"feature\", y=\"label\")\nplt.scatter(x=test_features, y=all_labels, color=\"red\", label=\"True values\")\nplt.title(\"Predictive posterior distributions\")\nplt.legend()\nplt.plot();"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "href": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "title": "Bayesian linear regression",
    "section": "Sources and further reading ",
    "text": "Sources and further reading \nThe basis for this notebook is chapter 9.2 of the book Mathematics for Machine Learning. I can highly recommend to read through chapter 9 to get a deeper understanding of (Bayesian) linear regression.\nYou will find explanations and an implementation of simple linear regression in the notebook on linear regression"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "href": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "title": "Baysian Linear Regression blog",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nNonlinear Features\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nPolynomial Regression class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nNow lets try different polynomial fits.\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\nYou can refer 9.1 and 9.2 section of Mathematics for Machine Learning to understand in depth about probalistic approch to linear regression."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "href": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "title": "Baysian Linear Regression blog",
    "section": "Bayes theroem",
    "text": "Bayes theroem\nThe basis of bayesian linear regression is bayes theroem. - Bayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\] - \\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters. - \\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely. - \\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "href": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "title": "Baysian Linear Regression blog",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nHave a look at Wiki"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "href": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "title": "Baysian Linear Regression blog",
    "section": "Bayeisan approch",
    "text": "Bayeisan approch\nUnlike linear regression where we computed point estimates of our parameters using maximum likelihood approach and make predictions, here in Bayesian linear regression we estimate\nFollowing are the steps: 1. We assume a that we know standard deviation of the noise, mean and covariance of the prior. 2. We than calculate parameter posteriori 3. Based on that we make posteriori predictions on unseen data ie. test data.\nNow lets see along with code\nHere we have same assumptions that we took in linear regression \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] Where epsilon is the noise from normal distribution with variance \\(\\sigma^2\\). Training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nFunction\n\ndef g(x, mu, sigma):   \n    epsilon = np.random.normal(mu, sigma, size=(x.shape))\n    return np.cos(x) + epsilon\n    \n\nWe apply non linear feature transformation on feature matrix with polynomial of degree \\(K\\) \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nSample to see nonlinear transformation\n\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n\n\npoly_features(X, 3) # defined in linear regression section\n\narray([[  1.,  -3.,   9., -27.],\n       [  1.,  -1.,   1.,  -1.],\n       [  1.,   0.,   0.,   0.],\n       [  1.,   1.,   1.,   1.],\n       [  1.,   3.,   9.,  27.]])"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html",
    "href": "blogs/blogsData/Image_super_resolution.html",
    "title": "Image Super Resolution",
    "section": "",
    "text": "The central aim of Super-Resolution (SR) is to generate a higher resolution image from lower resolution images. It is basically the process of retrieving the underlying high quality original image given a corrupted image.\n\n\nHigh resolution image offers a high pixel density and thereby more details about the original scene. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. High resolution is of importance in medical imaging for diagnosis. Many applications require zooming of a specific area of interest in the image wherein high resolution becomes essential, e.g. surveillance, forensic and satellite imaging applications.\nHowever, high resolution images are not always available. This is since the setup for high resolution imaging proves expensive and also it may not always be feasible due to the inherent limitations of the sensor, optics manufacturing technology. These problems can be overcome through the use of image processing algorithms, which are relatively inexpensive, giving rise to concept of super-resolution. It provides an advantage as it may cost less and the existing low resolution imaging systems can still be utilized.\nCamera image:\n\n\n\nflower_blur_unblur.jpg\n\n\nSecurity camera image:\n\n\n\nnumberplate_blur_unblur.jpg\n\n\nGeological image:\n\n\n\ngeo_lr_hr.png\n\n\n\n\n\nWe always do not have low and high resolution images of the same scene. So we create our own dataset. We take high resolution images (from net) and from that we create low resolution images by downscaling the high resolution images. We have used two methods for downscaling the images: 1. Bicubic Interpolation 2. Gaussian Blur\nWe won’t discussing in detail about these methods. We will be using the Gaussian Blur method for downscaling the images."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#why-super-resolution",
    "href": "blogs/blogsData/Image_super_resolution.html#why-super-resolution",
    "title": "Image Super Resolution",
    "section": "",
    "text": "High resolution image offers a high pixel density and thereby more details about the original scene. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. High resolution is of importance in medical imaging for diagnosis. Many applications require zooming of a specific area of interest in the image wherein high resolution becomes essential, e.g. surveillance, forensic and satellite imaging applications.\nHowever, high resolution images are not always available. This is since the setup for high resolution imaging proves expensive and also it may not always be feasible due to the inherent limitations of the sensor, optics manufacturing technology. These problems can be overcome through the use of image processing algorithms, which are relatively inexpensive, giving rise to concept of super-resolution. It provides an advantage as it may cost less and the existing low resolution imaging systems can still be utilized.\nCamera image:\n\n\n\nflower_blur_unblur.jpg\n\n\nSecurity camera image:\n\n\n\nnumberplate_blur_unblur.jpg\n\n\nGeological image:\n\n\n\ngeo_lr_hr.png"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#how-we-create-our-dataset",
    "href": "blogs/blogsData/Image_super_resolution.html#how-we-create-our-dataset",
    "title": "Image Super Resolution",
    "section": "",
    "text": "We always do not have low and high resolution images of the same scene. So we create our own dataset. We take high resolution images (from net) and from that we create low resolution images by downscaling the high resolution images. We have used two methods for downscaling the images: 1. Bicubic Interpolation 2. Gaussian Blur\nWe won’t discussing in detail about these methods. We will be using the Gaussian Blur method for downscaling the images."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#but-why-cnn-why-not-mlp",
    "href": "blogs/blogsData/Image_super_resolution.html#but-why-cnn-why-not-mlp",
    "title": "Image Super Resolution",
    "section": "But why CNN? Why not MLP?",
    "text": "But why CNN? Why not MLP?\n\nMLPs (Multilayer Perceptron) use one perceptron for each input (e.g. pixel in an image) and the amount of weights rapidly becomes unmanageable for large images. It includes too many parameters because it is fully connected. Each node is connected to every other node in next and the previous layer, forming a very dense web — resulting in redundancy and inefficiency. As a result, difficulties arise whilst training and overfitting can occur which makes it lose the ability to generalize.\nAnother common problem is that MLPs react differently to an input (images) and its shifted version — they are not translation invariant.\nThe main problems is that spatial information is lost when the image is flattened(matrix to vector) into an MLP.\n\n\nBasic implementation of CNN and MLP on Mnist dataset for classification.(you can skip if you know basics about CNN and MLP)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# Reshape the data for MLP\nx_train_mlp = x_train.reshape((-1, 28*28))\nx_test_mlp = x_test.reshape((-1, 28*28))\n\n# Reshape the data for CNN\nx_train_cnn = x_train.reshape((-1, 28, 28, 1))\nx_test_cnn = x_test.reshape((-1, 28, 28, 1))\n\n\n\n\nMLP_fig_imageclassification.jpg\n\n\n\n# MLP model\nmlp_model = Sequential()\nmlp_model.add(Dense(256, activation='relu', input_shape=(28*28,)))\nmlp_model.add(Dense(128, activation='relu'))\nmlp_model.add(Dense(10, activation='softmax'))\nmlp_model.summary()\nmlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train MLP model\nmlp_model.fit(x_train_mlp, y_train, epochs=10, batch_size=32, validation_data=(x_test_mlp, y_test))\n\n# Evaluate MLP model\nmlp_loss, mlp_accuracy = mlp_model.evaluate(x_test_mlp, y_test)\nprint(\"MLP Accuracy:\", mlp_accuracy)\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 256)               200960    \n                                                                 \n dense_6 (Dense)             (None, 128)               32896     \n                                                                 \n dense_7 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 235146 (918.54 KB)\nTrainable params: 235146 (918.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2051 - accuracy: 0.9395 - val_loss: 0.1189 - val_accuracy: 0.9636\nEpoch 2/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0852 - accuracy: 0.9729 - val_loss: 0.0767 - val_accuracy: 0.9756\nEpoch 3/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 0.0714 - val_accuracy: 0.9762\nEpoch 4/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0455 - accuracy: 0.9856 - val_loss: 0.0743 - val_accuracy: 0.9761\nEpoch 5/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.0663 - val_accuracy: 0.9798\nEpoch 6/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0763 - val_accuracy: 0.9792\nEpoch 7/10\n1875/1875 [==============================] - 10s 6ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0899 - val_accuracy: 0.9775\nEpoch 8/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0878 - val_accuracy: 0.9792\nEpoch 9/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1028 - val_accuracy: 0.9746\nEpoch 10/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0165 - accuracy: 0.9941 - val_loss: 0.0898 - val_accuracy: 0.9791\n313/313 [==============================] - 1s 3ms/step - loss: 0.0898 - accuracy: 0.9791\nMLP Accuracy: 0.9790999889373779\n\n\n\n\n\nCNN_fig_imageclassification.jpg\n\n\n\n# CNN model\ncnn_model = Sequential()\ncnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\ncnn_model.add(MaxPooling2D((2, 2)))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dense(10, activation='softmax'))\ncnn_model.summary()\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train CNN model\ncnn_model.fit(x_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(x_test_cnn, y_test))\n\n# Evaluate CNN model\ncnn_loss, cnn_accuracy = cnn_model.evaluate(x_test_cnn, y_test)\nprint(\"CNN Accuracy:\", cnn_accuracy)\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 13, 13, 32)        0         \n g2D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 5408)              0         \n                                                                 \n dense_8 (Dense)             (None, 128)               692352    \n                                                                 \n dense_9 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 693962 (2.65 MB)\nTrainable params: 693962 (2.65 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 36s 19ms/step - loss: 0.1383 - accuracy: 0.9592 - val_loss: 0.0611 - val_accuracy: 0.9816\nEpoch 2/10\n1875/1875 [==============================] - 35s 19ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9851\nEpoch 3/10\n1875/1875 [==============================] - 37s 20ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.0427 - val_accuracy: 0.9856\nEpoch 4/10\n1875/1875 [==============================] - 34s 18ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0475 - val_accuracy: 0.9843\nEpoch 5/10\n1875/1875 [==============================] - 38s 20ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0455 - val_accuracy: 0.9862\nEpoch 6/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0448 - val_accuracy: 0.9864\nEpoch 7/10\n1875/1875 [==============================] - 42s 23ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0475 - val_accuracy: 0.9871\nEpoch 8/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0526 - val_accuracy: 0.9857\nEpoch 9/10\n1875/1875 [==============================] - 41s 22ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0576 - val_accuracy: 0.9856\nEpoch 10/10\n1875/1875 [==============================] - 43s 23ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0526 - val_accuracy: 0.9861\n313/313 [==============================] - 2s 6ms/step - loss: 0.0526 - accuracy: 0.9861\nCNN Accuracy: 0.9861000180244446\n\n\n\n\n# Select a random image from the test set\nindex = np.random.randint(0, x_test.shape[0])\n\n# Get the image and its label\nimage = x_test[index]\nlabel = y_test[index]\n\n# Reshape the image for MLP model prediction\nimage_mlp = image.reshape((1, 28*28))\n\n# Reshape the image for CNN model prediction\nimage_cnn = image.reshape((1, 28, 28, 1))\n\n# Predict using MLP model\nmlp_prediction = np.argmax(mlp_model.predict(image_mlp))\n\n# Predict using CNN model\ncnn_prediction = np.argmax(cnn_model.predict(image_cnn))\n\n# Plot the image and predictions\nplt.figure(figsize=(8, 4))\n\n# MLP plot\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title(f\"MLP Prediction: {mlp_prediction}\")\nplt.axis('off')\n\n# CNN plot\nplt.subplot(1, 2, 2)\nplt.imshow(image, cmap='gray')\nplt.title(f\"CNN Prediction: {cnn_prediction}\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 82ms/step\n\n\n\n\n\nCNN does perform well on image classification task than MLP but above for toy dataset MNIST the difference is not that much. But for real world dataset the difference is huge.\nShow the difference between MLP and CNN for real world dataset.(open MLP vs CNN using transfer learning VGG16 on Snake vs Antelope dataset.ipynb)\nHere we can see that CNN is better than MLP for image classification. So we will be using CNN for our task."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#how-gans-works-analogy-counterfeiters-and-police",
    "href": "blogs/blogsData/Image_super_resolution.html#how-gans-works-analogy-counterfeiters-and-police",
    "title": "Image Super Resolution",
    "section": "How GANS works? Analogy: Counterfeiters and Police",
    "text": "How GANS works? Analogy: Counterfeiters and Police\n\nCounterfeiters: Generator\nPolice: Discriminator\n\nWe have an ambitious young criminal who wants to counterfeit money. He has a printing machine and he wants to print fake money. He has no idea how real money looks like. So he prints some money and goes to a shop to buy something. The shopkeeper is the discriminator. The shopkeeper knows how real money looks like. So he can easily identify the fake money. So the criminal goes back and prints some more money. This time the money looks more real. He goes to the shopkeeper again. The shopkeeper again identifies the fake money. This process continues until the criminal is able to print the exact replica of the real money. Now the shopkeeper is not able to identify the fake money. So the criminal is able to buy anything from the shopkeeper. The criminal has successfully fooled the shopkeeper. The criminal is the generator and the shopkeeper is the discriminator. This results in very realistic fake money. This is how GANS work.\nIn this sense both of them are getting better. The generator is getting better at generating fake money and the discriminator is getting better at identifying fake money. This is how GANS work. The generator generates fake images and the discriminator tries to identify the fake images. The generator tries to fool the discriminator and the discriminator tries to identify the fake images. This process continues until the discriminator is not able to identify the fake images. At this point the generator has successfully fooled the discriminator. The generator is now able to generate fake images which are indistinguishable from the real images.\nThis results in very realistic images. This is how GANS work.\n\nThe purpose of the generator Network is take random data initializations and decode it into synthetic sample\nThe purpose of the discriminator Network is to then take this input from our Generator and predict whether or not this sample came from the real dataset or not.\n\n\n\n\ngan_architecture.png"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#training-gans",
    "href": "blogs/blogsData/Image_super_resolution.html#training-gans",
    "title": "Image Super Resolution",
    "section": "Training GANS",
    "text": "Training GANS\n\nTraining GANS is very difficult compared to Neural Networks we use gradient descent to change our weights and biases. But in GANS we have two networks generator and discriminator that works against eachother. So we have to train both of them simultaneously.\nWe are not seeking to minimize a loss function. We are seeking to find an equilibrium between the generator and discriminator.\nTraining stops when the discriminator is no longer able to distinguish between real and fake images.\n\n\nTraining process\n\nwe randomly generate a noisy vector\ninput this noisy vector into the generator to generate a fake image\nWe take some sample data from our real data and mix it with the fake data.\nWe train the discriminator to classifyf this mixed data as real or fake and update the weights of the discriminator.\nWe then train the generator. We make more random noisy vectors and create synthetic images. With the weights of the discriminator frozen, we use the feedbcak from the discriminator to update the weights of the generator.\n\nThis is how both Generator(to make better fake images) and Discriminator(to identify fake images) are getting better."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#application-of-gans",
    "href": "blogs/blogsData/Image_super_resolution.html#application-of-gans",
    "title": "Image Super Resolution",
    "section": "Application of GANS:",
    "text": "Application of GANS:\n\nRealistic photo generation\nImage super-resolution\nImaage to image translation\nText to image translation\nSemantic image to photo translation"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#gans-for-super-resolution",
    "href": "blogs/blogsData/Image_super_resolution.html#gans-for-super-resolution",
    "title": "Image Super Resolution",
    "section": "GANS for Super Resolution",
    "text": "GANS for Super Resolution\nImporting necessary libraries\n\nfrom keras.datasets import mnist\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, Dropout\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport time\n\n\n\n# a function to format display the losses\ndef hmsString(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:&gt;02}:{:&gt;05.2f}\".format(h, m, s)\n\n# downsample and introduce noise in the images\ndef downSampleAndNoisyfi(X):\n    shape = X[0].shape\n    X_down = []\n    for x_i in X:\n       x_c = cv2.resize(x_i, (shape[0]//4, shape[1]//4), interpolation = cv2.INTER_AREA)\n       x_c = np.clip(x_c+ np.random.normal(0, 5, x_c.shape) , 0, 255).astype('uint8')\n       X_down.append(x_c)\n    X_down = np.array(X_down, dtype = 'uint8')\n    return X_down\n\n\nCode for Generator Block\n\ndef Generator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])  \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X = Add()([X_shortcut, X])\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)   \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    generator_model = Model(inputs=X_input, outputs=X)\n    return generator_model\n\n\n\n\nCode for Discriminator Block\n\ndef Discriminator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.8)(X)\n    X = Activation('relu')(X)\n    \n    discriminator_model = Model(inputs=X_input, outputs=X)\n    return discriminator_model\n\n\n\n\nTraing GANS\n\n# One step of the test step\n@tf.function\ndef train_step(X, Y, generator, discriminator, generator_optimizer, discriminator_optimizer):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(X, training=True)\n\n    real_output = discriminator(Y, training=True)\n    fake_output = discriminator(generated_images, training=False)\n\n    gen_loss = tf.keras.losses.MSE(Y, generated_images)\n    disc_loss = tf.keras.losses.MSE(real_output, fake_output)\n    \n\n    gradients_of_generator = gen_tape.gradient(\\\n        gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n  return gen_loss,disc_loss\n\n# The main function to train the GAN\ndef train(X_train, Y_train, generator, discriminator, batch_size=100, epochs=50):\n    generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        prev_i = 0\n        for i in range(X_train.shape[0]):\n            if((i+1)%batch_size == 0):\n                t = train_step(X_train[prev_i:i+1], Y_train[prev_i:i+1], generator, discriminator, generator_optimizer, discriminator_optimizer)\n                gen_loss_list.append(t[0])\n                disc_loss_list.append(t[1])\n                prev_i = i+1\n        g_loss = np.sum(np.array(gen_loss_list)) / np.sum(np.array(gen_loss_list).shape)\n        d_loss = np.sum(np.array(disc_loss_list)) / np.sum(np.array(disc_loss_list).shape)\n        \n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {hmsString(epoch_elapsed)}')\n        \n    elapsed = time.time()-start\n    print (f'Training time: {hmsString(elapsed)}')\n    \n\n\n# loading the dataset(the original image are the HR 28*28 images)\n(Y_train, _), (Y_test, _) = mnist.load_data()\n# downsampling and introducing gaussian noise\n# this downsampled and noised dataset is out X or inputs\nX_train = downSampleAndNoisyfi(Y_train)\nX_test = downSampleAndNoisyfi(Y_test)\n\n# introduce a new dimension to the data (None, 28, 28, 1)\nX_test = X_test[..., np.newaxis]\nX_train = X_train[..., np.newaxis]\nY_train = Y_train[..., np.newaxis]\nY_test = Y_test[..., np.newaxis]\n\n# Creating a generator model\n# Showing the summary of generator \ngenerator = Generator((7,7,1))\ngenerator.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 7, 7, 1)]            0         []                            \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 7, 7, 32)             320       ['input_1[0][0]']             \n                                                                                                  \n batch_normalization (Batch  (None, 7, 7, 32)             128       ['conv2d_2[0][0]']            \n Normalization)                                                                                   \n                                                                                                  \n activation (Activation)     (None, 7, 7, 32)             0         ['batch_normalization[0][0]'] \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 7, 7, 32)             9248      ['activation[0][0]']          \n                                                                                                  \n batch_normalization_1 (Bat  (None, 7, 7, 32)             128       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_1 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n add (Add)                   (None, 7, 7, 32)             0         ['activation[0][0]',          \n                                                                     'activation_1[0][0]']        \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 7, 7, 32)             9248      ['add[0][0]']                 \n                                                                                                  \n batch_normalization_2 (Bat  (None, 7, 7, 32)             128       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_2 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_1 (Add)                 (None, 7, 7, 32)             0         ['add[0][0]',                 \n                                                                     'activation_2[0][0]']        \n                                                                                                  \n activation_3 (Activation)   (None, 7, 7, 32)             0         ['add_1[0][0]']               \n                                                                                                  \n up_sampling2d (UpSampling2  (None, 14, 14, 32)           0         ['activation_3[0][0]']        \n D)                                                                                               \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 14, 14, 32)           9248      ['up_sampling2d[0][0]']       \n                                                                                                  \n batch_normalization_3 (Bat  (None, 14, 14, 32)           128       ['conv2d_5[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_4 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_6 (Conv2D)           (None, 14, 14, 32)           9248      ['activation_4[0][0]']        \n                                                                                                  \n batch_normalization_4 (Bat  (None, 14, 14, 32)           128       ['conv2d_6[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_5 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_2 (Add)                 (None, 14, 14, 32)           0         ['activation_4[0][0]',        \n                                                                     'activation_5[0][0]']        \n                                                                                                  \n conv2d_7 (Conv2D)           (None, 14, 14, 32)           9248      ['add_2[0][0]']               \n                                                                                                  \n batch_normalization_5 (Bat  (None, 14, 14, 32)           128       ['conv2d_7[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_6 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_3 (Add)                 (None, 14, 14, 32)           0         ['add_2[0][0]',               \n                                                                     'activation_6[0][0]']        \n                                                                                                  \n activation_7 (Activation)   (None, 14, 14, 32)           0         ['add_3[0][0]']               \n                                                                                                  \n up_sampling2d_1 (UpSamplin  (None, 28, 28, 32)           0         ['activation_7[0][0]']        \n g2D)                                                                                             \n                                                                                                  \n conv2d_8 (Conv2D)           (None, 28, 28, 32)           9248      ['up_sampling2d_1[0][0]']     \n                                                                                                  \n batch_normalization_6 (Bat  (None, 28, 28, 32)           128       ['conv2d_8[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_8 (Activation)   (None, 28, 28, 32)           0         ['batch_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_9 (Conv2D)           (None, 28, 28, 1)            289       ['activation_8[0][0]']        \n                                                                                                  \n batch_normalization_7 (Bat  (None, 28, 28, 1)            4         ['conv2d_9[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_9 (Activation)   (None, 28, 28, 1)            0         ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n==================================================================================================\nTotal params: 56997 (222.64 KB)\nTrainable params: 56547 (220.89 KB)\nNon-trainable params: 450 (1.76 KB)\n__________________________________________________________________________________________________\n\n\n\n# Creating a discriminator model\n# Showing the summary of discriminator\ndiscriminator = Discriminator((28,28,1))\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 28, 28, 32)        320       \n                                                                 \n activation_10 (Activation)  (None, 28, 28, 32)        0         \n                                                                 \n conv2d_11 (Conv2D)          (None, 28, 28, 64)        18496     \n                                                                 \n batch_normalization_8 (Bat  (None, 28, 28, 64)        256       \n chNormalization)                                                \n                                                                 \n activation_11 (Activation)  (None, 28, 28, 64)        0         \n                                                                 \n=================================================================\nTotal params: 19072 (74.50 KB)\nTrainable params: 18944 (74.00 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\n\n\n\n# training with batch size of 100 and for 50 epochs\ntrain(X_train, Y_train, generator, discriminator, 100, 5) #50)\n\n# save the generator model for future use\ngenerator.save(\"mnist_generator_model\")\ngenerator.save(\"mnist_generator_model.h5\")\n\nHere we have run only for 5 epochs but you can run for more epochs to get better results.\n\n# testing the model\nY_pred = generator.predict(X_test)\n# showing the first 5 results\nfig,a =  plt.subplots(3,5)\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    a[0][i].imshow(X_test[i])\n    a[0][i].axes.get_xaxis().set_visible(False)\n    a[0][i].axes.get_yaxis().set_visible(False)\n    a[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    a[1][i].imshow(Y_pred[i])\n    a[1][i].axes.get_xaxis().set_visible(False)\n    a[1][i].axes.get_yaxis().set_visible(False)\n    a[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    a[2][i].imshow(Y_test[i])\n    a[2][i].axes.get_xaxis().set_visible(False)\n    a[2][i].axes.get_yaxis().set_visible(False)\n    a[2][i].title.set_text(\"HR: \"+str(i+1)) \n    \n\n313/313 [==============================] - 9s 27ms/step\n\n\n\n\n\n\n# showing the first 5 random results\nimport random\nfigb,ab =  plt.subplots(3,5)\nfigb.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    ii = random.randint(0, 10000) \n    \n    ab[0][i].imshow(X_test[ii])\n    ab[0][i].axes.get_xaxis().set_visible(False)\n    ab[0][i].axes.get_yaxis().set_visible(False)\n    ab[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    ab[1][i].imshow(Y_pred[ii])\n    ab[1][i].axes.get_xaxis().set_visible(False)\n    ab[1][i].axes.get_yaxis().set_visible(False)\n    ab[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    ab[2][i].imshow(Y_test[ii])\n    ab[2][i].axes.get_xaxis().set_visible(False)\n    ab[2][i].axes.get_yaxis().set_visible(False)\n    ab[2][i].title.set_text(\"HR: \"+str(i+1)) \n\n\n\n\nWell GAN does perform good but it has some problems.\n\n\nProblem with GANS:\n\nAchieving equilibrium: between the generator and discriminator is very difficult.\nTime: Training gans is computationally expensive and necessitates tweaking of hyperparameters such as initializations, altering hidden layers, different activation, using Batch Normalization or Dropout, etc.\nBad Initializations: If the generator and discriminator are not initialized properly, then the training will fail.\nMode Collapse: happens when regardless of the nosie input fed into your generator, the generated output varies very little. It occurs when a small set of images look good to the descriminator and get scored better than other images. The GAN simple learns to reproduce those images over and over again. Analgous to overfittiing.\n\nOne quick solution to the problem of high training time is to use transfer learning using VGG16 or VGG19 in Generator and discriminator architecture. This will reduce the training time."
  },
  {
    "objectID": "demo_notebooks/ISR_CNN_UNET.html",
    "href": "demo_notebooks/ISR_CNN_UNET.html",
    "title": "Drawing the model (using ONNX and Netron)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport torch\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n# Download some MNIST to demonstrate super-resolution\nfrom torchvision import datasets, transforms\nmnist = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\nmnist_test = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n\n\n\n# Displaying an image\ndef show_image(img):\n    plt.imshow(img.permute(1, 2, 0).squeeze(), cmap='gray')\n    plt.axis('off')\n\n# Displaying a batch of images in 1 row and n columns\ndef show_batch(batch):\n    fig, ax = plt.subplots(1, len(batch), figsize=(20, 20))\n    for i, img in enumerate(batch):\n        ax[i].imshow(img.permute(1, 2, 0).squeeze(), cmap='gray')\n        ax[i].axis('off')\n    \n\n\nshow_image(mnist[0][0])\n\n\n\n\n\nshow_batch(torch.stack([mnist[i][0] for i in range(10)]))\n\n\n\n\n\nmnist[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Downsample the images\ndownsample = transforms.Resize(7)\n\n# First 10000 images X\nmnist_small = [downsample(mnist[i][0]) for i in range(10000)]\nmnist_small = torch.stack(mnist_small)\n\n# First 10000 images Y\nmnist_large = torch.stack([mnist[i][0] for i in range(10000)])\n\n# Test set X\nmnist_test_small = [downsample(mnist_test[i][0]) for i in range(10000)]\nmnist_test_small = torch.stack(mnist_test_small)\n\n# Test set Y\nmnist_test_large = torch.stack([mnist_test[i][0] for i in range(10000)])\n\nC:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n\n\n\n# Show the downsampled images and the original images side-by-side\n\nshow_batch(torch.stack([mnist_small[i] for i in range(10)]))\nplt.figure()\nshow_batch(torch.stack([mnist[i][0] for i in range(10)]))\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\nmnist_small.shape, mnist.data.shape\n\n(torch.Size([10000, 1, 7, 7]), torch.Size([60000, 28, 28]))\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass SinActivation(nn.Module):\n    def forward(self, x):\n        return torch.sin(x)\n\n# Create an instance of the custom SinActivation module\nsin_activation = SinActivation()\n\nclass UNet(nn.Module):\n    def __init__(self, activation=sin_activation):\n        super(UNet, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Input: (batch_size, 1, 7, 7), Output: (batch_size, 16, 7, 7)\n            # Use the custom activation function\n            activation,\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Input: (batch_size, 16, 7, 7), Output: (batch_size, 32, 7, 7)\n            activation,\n            nn.MaxPool2d(kernel_size=2, stride=2)  # Input: (batch_size, 32, 7, 7), Output: (batch_size, 32, 3, 3)\n        )\n\n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Input: (batch_size, 32, 3, 3), Output: (batch_size, 64, 3, 3)\n            activation,\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=4, padding=0),  # Input: (batch_size, 64, 3, 3), Output: (batch_size, 32, 12, 12)\n            activation,\n            # Input (batch_size, 32, 12, 12), Output: (batch_size, 16, 12, 12)\n            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=0),\n            activation,\n            # Input (batch_size, 16, 12, 12), Output: (batch_size, 1, 28, 28)\n            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1)\n            \n            )\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.encoder(x)\n\n        # Bottleneck\n        x = self.bottleneck(x1)\n\n        # Decoder\n        x = self.decoder(x)\n\n        return x\n\n# Create an instance of the modified UNet model\nmodel = UNet(nn.GELU())\n\n# Print the model architecture with input and output shape\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\ndummy_input = torch.randn(input_size)\noutput = model(dummy_input)\nprint(model)\nprint(f\"Input shape: {input_size}\")\nprint(f\"Output shape: {output.shape}\")\n\nUNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): GELU(approximate='none')\n    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): GELU(approximate='none')\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (bottleneck): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): GELU(approximate='none')\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(4, 4))\n    (1): GELU(approximate='none')\n    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n    (3): GELU(approximate='none')\n    (4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  )\n)\nInput shape: (1, 1, 7, 7)\nOutput shape: torch.Size([1, 1, 28, 28])\n\n\n\n#Provide an example input to the model\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\ndummy_input = torch.randn(input_size)\n\n# Export the model to ONNX\nonnx_path = \"unet_model.onnx\"\ntorch.onnx.export(model, dummy_input, onnx_path, verbose=False)\n\nprint(\"Model exported to ONNX successfully.\")\n\n============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nModel exported to ONNX successfully.\n\n\n\n# Input to the model is a batch of 1-channel 7x7 images\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\n\n# Create an instance of the modified UNet model\n\n# Output of the model is a batch of 1-channel 28x28 images\noutput_size = (batch_size, 1, 28, 28)\n\n\n# Input to the model is a batch of 1-channel 7x7 images\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\n\n# Create an instance of the modified UNet model\n\n# Output of the model is a batch of 1-channel 28x28 images\noutput_size = (batch_size, 1, 28, 28)\n\n\n# Create X_train, Y_train, X_test, Y_test\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nX_train = mnist_small.float().to(device)\nY_train = mnist_large.float().to(device)\n\nX_test = mnist_test_small.float().to(device)\nY_test = mnist_test_large.float().to(device)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\nmodel = UNet(activation=sin_activation).to(device)\n\n\n# Define the loss function\nloss_fn = nn.MSELoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# Number of epochs\n# n_epochs = 5001\nn_epochs = 500\n\n# List to store losses\nlosses = []\n\n# Loop over epochs\nfor epoch in range(n_epochs):\n    # Forward pass\n    Y_pred = model(X_train)\n\n    # Compute Loss\n    loss = loss_fn(Y_pred, Y_train)\n\n    # Print loss\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n    # Store loss\n    losses.append(loss.item())\n\n    # Zero the gradients\n    optimizer.zero_grad()\n\n    # Backpropagation\n    loss.backward()\n\n    # Update the weights\n    optimizer.step()\n\nEpoch 1 loss: 0.15383368730545044\nEpoch 101 loss: 0.05964525789022446\nEpoch 201 loss: 0.04472550377249718\nEpoch 301 loss: 0.03792179003357887\nEpoch 401 loss: 0.034905895590782166\n\n\n\n# Plot the losses\nplt.plot(losses)\n\n\n\n\n\n# Extract a mini-batch of 10 images\nX_mini = X_train[:10]\nY_mini = Y_train[:10]\n\n# Forward pass\nY_hat = model(X_mini)\n\n# Move the tensors to CPU\nX_mini = X_mini.cpu()\nY_mini = Y_mini.cpu()\nY_hat = Y_hat.cpu()\n\ndef plot_images(X_mini, Y_mini, Y_hat=None):\n\n    # Plot 3 rows\n    rows = 3\n\n    # 10 images X 3 \n    # First row: 10 images from the mini-batch\n    # Second row: 10 ground truth images\n    # Third row: 10 predicted images\n\n    fig, ax = plt.subplots(rows, 10, figsize=(20, 6))\n\n    for i in range(rows):\n        for j in range(10):\n            if i == 0:\n                ax[i][j].imshow(X_mini[j].squeeze(), cmap=\"gray\")\n            elif i == 1:\n                ax[i][j].imshow(Y_mini[j].squeeze(), cmap=\"gray\")\n            else:\n                ax[i][j].imshow(Y_hat[j].detach().squeeze(), cmap=\"gray\")\n\n            ax[i][j].axis(\"off\")\n\n    # Put labels for the three rows using suptitle()\n    fig.suptitle(\"MNIST Image Generation using U-Net\", fontsize=16)\n\n    ax[0][0].set_title(\"Input Images\")\n    ax[1][0].set_title(\"Ground Truth Images\")\n    ax[2][0].set_title(\"Predicted Images\")\n\nplot_images(X_mini, Y_mini, Y_hat)\n\n\n\n\n\n# Get unseen images from the test set\nX_test = mnist_test_small.float().to(device)\nY_test = mnist_test_large.float().to(device)\n\n# Forward pass\nY_hat = model(X_test)\n\nplot_images(X_test.cpu(), Y_test.cpu(), Y_hat.cpu())"
  },
  {
    "objectID": "demo_notebooks/ISR_GAN_Mnist.html",
    "href": "demo_notebooks/ISR_GAN_Mnist.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "from keras.datasets import mnist\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, Dropout\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport time\n\n\n\n# a function to format display the losses\ndef hmsString(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:&gt;02}:{:&gt;05.2f}\".format(h, m, s)\n\n# downsample and introduce noise in the images\ndef downSampleAndNoisyfi(X):\n    shape = X[0].shape\n    X_down = []\n    for x_i in X:\n       x_c = cv2.resize(x_i, (shape[0]//4, shape[1]//4), interpolation = cv2.INTER_AREA)\n       x_c = np.clip(x_c+ np.random.normal(0, 5, x_c.shape) , 0, 255).astype('uint8')\n       X_down.append(x_c)\n    X_down = np.array(X_down, dtype = 'uint8')\n    return X_down\n\n\n\n################# CODE FOR GENERATOR BLOCK\ndef Generator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])  \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X = Add()([X_shortcut, X])\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)   \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    generator_model = Model(inputs=X_input, outputs=X)\n    return generator_model\n\n\n\n################# CODE FOR DISCRIMINATOR BLOCK\ndef Discriminator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.8)(X)\n    X = Activation('relu')(X)\n    \n    discriminator_model = Model(inputs=X_input, outputs=X)\n    return discriminator_model\n\n\n\n# One step of the test step\n@tf.function\ndef train_step(X, Y, generator, discriminator, generator_optimizer, discriminator_optimizer):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(X, training=True)\n\n    real_output = discriminator(Y, training=True)\n    fake_output = discriminator(generated_images, training=False)\n\n    gen_loss = tf.keras.losses.MSE(Y, generated_images)\n    disc_loss = tf.keras.losses.MSE(real_output, fake_output)\n    \n\n    gradients_of_generator = gen_tape.gradient(\\\n        gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n  return gen_loss,disc_loss\n\n# The main function to train the GAN\ndef train(X_train, Y_train, generator, discriminator, batch_size=100, epochs=50):\n    generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        prev_i = 0\n        for i in range(X_train.shape[0]):\n            if((i+1)%batch_size == 0):\n                t = train_step(X_train[prev_i:i+1], Y_train[prev_i:i+1], generator, discriminator, generator_optimizer, discriminator_optimizer)\n                gen_loss_list.append(t[0])\n                disc_loss_list.append(t[1])\n                prev_i = i+1\n        g_loss = np.sum(np.array(gen_loss_list)) / np.sum(np.array(gen_loss_list).shape)\n        d_loss = np.sum(np.array(disc_loss_list)) / np.sum(np.array(disc_loss_list).shape)\n        \n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {hmsString(epoch_elapsed)}')\n        \n    elapsed = time.time()-start\n    print (f'Training time: {hmsString(elapsed)}')\n    \n\n\n    \n# loading the dataset(the original image are the HR 28*28 images)\n(Y_train, _), (Y_test, _) = mnist.load_data()\n# downsampling and introducing gaussian noise\n# this downsampled and noised dataset is out X or inputs\nX_train = downSampleAndNoisyfi(Y_train)\nX_test = downSampleAndNoisyfi(Y_test)\n\n# introduce a new dimension to the data (None, 28, 28, 1)\nX_test = X_test[..., np.newaxis]\nX_train = X_train[..., np.newaxis]\nY_train = Y_train[..., np.newaxis]\nY_test = Y_test[..., np.newaxis]\n\n# Creating a generator and discriminator model\ngenerator = Generator((7,7,1))\ndiscriminator = Discriminator((28,28,1))\n\n# Showing the summary of generator and discriminator\ngenerator.summary()\ndiscriminator.summary()\n# training with batch size of 100 and for 50 epochs\ntrain(X_train, Y_train, generator, discriminator, 100, 5)#50)\n\n# save the generator model for future use\ngenerator.save(\"mnist_generator_model\")\ngenerator.save(\"mnist_generator_model.h5\")\n\n\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_5 (InputLayer)        [(None, 7, 7, 1)]            0         []                            \n                                                                                                  \n conv2d_20 (Conv2D)          (None, 7, 7, 32)             320       ['input_5[0][0]']             \n                                                                                                  \n batch_normalization_18 (Ba  (None, 7, 7, 32)             128       ['conv2d_20[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_24 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_18[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_21 (Conv2D)          (None, 7, 7, 32)             9248      ['activation_24[0][0]']       \n                                                                                                  \n batch_normalization_19 (Ba  (None, 7, 7, 32)             128       ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_25 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_19[0][0]\n                                                                    ']                            \n                                                                                                  \n add_8 (Add)                 (None, 7, 7, 32)             0         ['activation_24[0][0]',       \n                                                                     'activation_25[0][0]']       \n                                                                                                  \n conv2d_22 (Conv2D)          (None, 7, 7, 32)             9248      ['add_8[0][0]']               \n                                                                                                  \n batch_normalization_20 (Ba  (None, 7, 7, 32)             128       ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_26 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_20[0][0]\n                                                                    ']                            \n                                                                                                  \n add_9 (Add)                 (None, 7, 7, 32)             0         ['add_8[0][0]',               \n                                                                     'activation_26[0][0]']       \n                                                                                                  \n activation_27 (Activation)  (None, 7, 7, 32)             0         ['add_9[0][0]']               \n                                                                                                  \n up_sampling2d_4 (UpSamplin  (None, 14, 14, 32)           0         ['activation_27[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n conv2d_23 (Conv2D)          (None, 14, 14, 32)           9248      ['up_sampling2d_4[0][0]']     \n                                                                                                  \n batch_normalization_21 (Ba  (None, 14, 14, 32)           128       ['conv2d_23[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_28 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_21[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_24 (Conv2D)          (None, 14, 14, 32)           9248      ['activation_28[0][0]']       \n                                                                                                  \n batch_normalization_22 (Ba  (None, 14, 14, 32)           128       ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_29 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_22[0][0]\n                                                                    ']                            \n                                                                                                  \n add_10 (Add)                (None, 14, 14, 32)           0         ['activation_28[0][0]',       \n                                                                     'activation_29[0][0]']       \n                                                                                                  \n conv2d_25 (Conv2D)          (None, 14, 14, 32)           9248      ['add_10[0][0]']              \n                                                                                                  \n batch_normalization_23 (Ba  (None, 14, 14, 32)           128       ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_30 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_23[0][0]\n                                                                    ']                            \n                                                                                                  \n add_11 (Add)                (None, 14, 14, 32)           0         ['add_10[0][0]',              \n                                                                     'activation_30[0][0]']       \n                                                                                                  \n activation_31 (Activation)  (None, 14, 14, 32)           0         ['add_11[0][0]']              \n                                                                                                  \n up_sampling2d_5 (UpSamplin  (None, 28, 28, 32)           0         ['activation_31[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n conv2d_26 (Conv2D)          (None, 28, 28, 32)           9248      ['up_sampling2d_5[0][0]']     \n                                                                                                  \n batch_normalization_24 (Ba  (None, 28, 28, 32)           128       ['conv2d_26[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_32 (Activation)  (None, 28, 28, 32)           0         ['batch_normalization_24[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_27 (Conv2D)          (None, 28, 28, 1)            289       ['activation_32[0][0]']       \n                                                                                                  \n batch_normalization_25 (Ba  (None, 28, 28, 1)            4         ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_33 (Activation)  (None, 28, 28, 1)            0         ['batch_normalization_25[0][0]\n                                                                    ']                            \n                                                                                                  \n==================================================================================================\nTotal params: 56997 (222.64 KB)\nTrainable params: 56547 (220.89 KB)\nNon-trainable params: 450 (1.76 KB)\n__________________________________________________________________________________________________\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_28 (Conv2D)          (None, 28, 28, 32)        320       \n                                                                 \n activation_34 (Activation)  (None, 28, 28, 32)        0         \n                                                                 \n conv2d_29 (Conv2D)          (None, 28, 28, 64)        18496     \n                                                                 \n batch_normalization_26 (Ba  (None, 28, 28, 64)        256       \n tchNormalization)                                               \n                                                                 \n activation_35 (Activation)  (None, 28, 28, 64)        0         \n                                                                 \n=================================================================\nTotal params: 19072 (74.50 KB)\nTrainable params: 18944 (74.00 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\nEpoch 1, gen loss=444240933.9259259,disc loss=5967.341931216931, 0:06:59.43\nEpoch 2, gen loss=442459325.6296296,disc loss=2971.5945767195767, 0:06:50.58\nEpoch 3, gen loss=441174395.2592593,disc loss=1824.6559193121693, 0:05:31.16\nEpoch 4, gen loss=439941780.994709,disc loss=1088.3287037037037, 0:05:21.85\nEpoch 5, gen loss=438728021.3333333,disc loss=618.3444527116402, 0:05:26.83\nTraining time: 0:30:09.87\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: mnist_generator_model\\assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\nINFO:tensorflow:Assets written to: mnist_generator_model\\assets\nC:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\n\n# testing the model\nY_pred = generator.predict(X_test)\n# showing the first 5 results\nfig,a =  plt.subplots(3,5)\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    a[0][i].imshow(X_test[i])\n    a[0][i].axes.get_xaxis().set_visible(False)\n    a[0][i].axes.get_yaxis().set_visible(False)\n    a[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    a[1][i].imshow(Y_pred[i])\n    a[1][i].axes.get_xaxis().set_visible(False)\n    a[1][i].axes.get_yaxis().set_visible(False)\n    a[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    a[2][i].imshow(Y_test[i])\n    a[2][i].axes.get_xaxis().set_visible(False)\n    a[2][i].axes.get_yaxis().set_visible(False)\n    a[2][i].title.set_text(\"HR: \"+str(i+1)) \n    \n\n313/313 [==============================] - 9s 27ms/step\n\n\n\n\n\n\n# showing the first 5 random results\nimport random\nfigb,ab =  plt.subplots(3,5)\nfigb.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    ii = random.randint(0, 10000) \n    \n    ab[0][i].imshow(X_test[ii])\n    ab[0][i].axes.get_xaxis().set_visible(False)\n    ab[0][i].axes.get_yaxis().set_visible(False)\n    ab[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    ab[1][i].imshow(Y_pred[ii])\n    ab[1][i].axes.get_xaxis().set_visible(False)\n    ab[1][i].axes.get_yaxis().set_visible(False)\n    ab[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    ab[2][i].imshow(Y_test[ii])\n    ab[2][i].axes.get_xaxis().set_visible(False)\n    ab[2][i].axes.get_yaxis().set_visible(False)\n    ab[2][i].title.set_text(\"HR: \"+str(i+1))"
  }
]