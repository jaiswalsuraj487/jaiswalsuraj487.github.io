[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "title": "Baysian Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating\n\\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html",
    "href": "blogs/blogsData/blr_blog.html",
    "title": "Baysian Linear Regression blog",
    "section": "",
    "text": "Welcome to my blog on Bayesian linear regression, where we explore the power of this technique. While traditional linear regression provides point estimates, Bayesian linear regression incorporates prior knowledge and quantifies uncertainty. By combining observed data with prior beliefs, we make more informed decisions. Throughout this blog, we’ll delve into key components like probalistic approch to linear regression, basics of types of uncertainity, prior distributions, likelihood functions, and posterior inference. Let’s embark on this enlightening journey together."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "href": "blogs/blogsData/blr_blog.html#maximum-likelihood",
    "title": "Baysian Linear Regression blog",
    "section": "Maximum Likelihood",
    "text": "Maximum Likelihood\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta_{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta_{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X])\n\n\ndef max_lik_estimate(X, y):\n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_ml = max_lik_estimate(X_aug,y)\nprint(theta_ml)\n\n[[2.116]\n [0.499]]\n\n\nNow we will make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), \\[\n\\ \\boldsymbol y_{\\text{pred}} = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta_{\\text{ML}}\n\\]\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\nml_prediction = Xtest_aug @ theta_ml\n\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\nText(0, 0.5, '$y$')\n\n\n\n\n\nThis gives fairly good results but what if the data is bit complex\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\nN = 10\nmu = 0\nsigma = 0.2**2\nseed(10)\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nLets first apply linear regressoin without non linear transformation\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.47109666],\n       [-0.1808517 ]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "href": "blogs/blogsData/blr_blog.html#polynomial-regression",
    "title": "Baysian Linear Regression blog",
    "section": "Polynomial Regression",
    "text": "Polynomial Regression\n\nNonlinear Features\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nPolynomial Regression class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nNow lets try different polynomial fits.\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\nYou can refer 9.1 and 9.2 section of Mathematics for Machine Learning to understand in depth about probalistic approch to linear regression."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "href": "blogs/blogsData/blr_blog.html#bayes-theroem",
    "title": "Baysian Linear Regression blog",
    "section": "Bayes theroem",
    "text": "Bayes theroem\nThe basis of bayesian linear regression is bayes theroem. - Bayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\] - \\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters. - \\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely. - \\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets."
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "href": "blogs/blogsData/blr_blog.html#bayesian-inference",
    "title": "Baysian Linear Regression blog",
    "section": "Bayesian inference",
    "text": "Bayesian inference\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nHave a look at Wiki"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "href": "blogs/blogsData/blr_blog.html#bayeisan-approch",
    "title": "Baysian Linear Regression blog",
    "section": "Bayeisan approch",
    "text": "Bayeisan approch\nUnlike linear regression where we computed point estimates of our parameters using maximum likelihood approach and make predictions, here in Bayesian linear regression we estimate\nFollowing are the steps: 1. We assume a that we know standard deviation of the noise, mean and covariance of the prior. 2. We than calculate parameter posteriori 3. Based on that we make posteriori predictions on unseen data ie. test data.\nNow lets see along with code\nHere we have same assumptions that we took in linear regression \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] Where epsilon is the noise from normal distribution with variance \\(\\sigma^2\\). Training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nFunction\n\ndef g(x, mu, sigma):   \n    epsilon = np.random.normal(mu, sigma, size=(x.shape))\n    return np.cos(x) + epsilon\n    \n\nWe apply non linear feature transformation on feature matrix with polynomial of degree \\(K\\) \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nSample to see nonlinear transformation\n\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n\n\npoly_features(X, 3) # defined in linear regression section\n\narray([[  1.,  -3.,   9., -27.],\n       [  1.,  -1.,   1.,  -1.],\n       [  1.,   0.,   0.,   0.],\n       [  1.,   1.,   1.,   1.],\n       [  1.,   3.,   9.,  27.]])"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#known-entities",
    "href": "blogs/blogsData/blr_blog.html#known-entities",
    "title": "Baysian Linear Regression blog",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\\[\n\\boxed{\\begin{array}{l}\n\\ \\ \\ \\ \\ \\ \\ \\ \\ m_{0} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ S_{0}\\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\searrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma \\\\\n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\downarrow \\ \\ \\ \\swarrow \\\\\n\\ \\ \\ \\ x_{n} \\ \\ \\rightarrow \\ \\ y_{n} \\ \\ \\\\\n\\ \\ \\ \\ n\\ =\\ 1,......,N\\ \\\\\n\\end{array}}\n\\] \\[ Graphical \\ model \\ for \\ Bayeisan \\ linear \\ regression \\]\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, m0, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/blr_blog.html#posterior",
    "href": "blogs/blogsData/blr_blog.html#posterior",
    "title": "Baysian Linear Regression blog",
    "section": "Posterior",
    "text": "Posterior\n\nParameter posteriori in closed form\nCalculating Parameter posterior: \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\n\n\nPosterior Predictive distribution\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\n\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}}) \\, |\\ X, Y, \\boldsymbol X_{\\text{test}}) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol M_{\\text{n}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol S_{\\text{N}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top + \\sigma ^ 2)\n\\end{align} \\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\n# conf_bound1 = np.sqrt(var_blr).flatten()\n# plt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\n# 95 % parameter uncertainity\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"r\")\n\n# 95 % total uncertainity ie. \nconf_bound3 = 2.0*np.sqrt(var_blr + sigma**2).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\", '95% para uncertainity', '95% total uncertainity'])\nplt.xlabel('$x$');\nplt.ylabel('$y$');\n\n\n\n\nYou can refer 9.3 section of Mathematics for Machine Learning to understand in depth about bayesian linear regression.\n\n\nVisulizing the parameter Posterior\nIn this section we will visualize the posterior and will see how it changes as it sees more data.\n\ndef f(x, a): return a[0] + a[1] * x\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(sigma **2)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + ((1 / sigma) ** 2) * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + ((1 / sigma) ** 2) * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5]) # true parameter values\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# beta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5, 15, 30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$$\\theta_0$$\")\n            ax[ix_fig, l].set_ylabel(\"$$\\theta_1$$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nWe can see above as the model see more data, the posterior converges close the the true values at end. Refer to Bishop - Pattern Recognition and Machine Learning fig 3.7 to understand above fig in detail."
  },
  {
    "objectID": "blogs/blogsData/demo.html",
    "href": "blogs/blogsData/demo.html",
    "title": "Plots for bayesian LR",
    "section": "",
    "text": "import random\nimport math\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\n\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n9.3\n$$                                    \\\n                       \\\nx_{n}       y_{n}   \\\nn = 1,……,N \\\n\\\n$$\nfigure 9.5\n\nN = 10\nmu = 0\nsigma = 0.2**2\n\nxn = np.random.uniform(-5, 5, N)\nepsilon = np.random.normal(mu, sigma, N)\nyn = -np.sin(xn/5) + np.cos(xn) + epsilon\ndataset = np.column_stack((xn, yn))\nxn = xn.reshape(-1,1)\nyn = yn.reshape(-1,1)\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\nN, D = xn.shape\nX_aug = np.hstack([np.ones((N,1)), xn]) # augmented training inputs of size N x (D+1)\n# theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\n\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\ntheta_aug_ml = max_lik_estimate(X_aug, yn)\ntheta_aug_ml\n\narray([[-0.2123287 ],\n       [-0.18826531]])\n\n\n\nml_predictions = X_aug @ theta_aug_ml \n# X: K x D matrix of test inputs\n# theta: D x 1 vector of parameters\n# returns: prediction of f(X); K x 1 vector\n\n\nml_predictions.shape\n\n(10, 1)\n\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(xn, ml_predictions)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\n\ndef poly_features(X, p):\n    \"\"\"Returns a matrix with p columns containing the polynomial features of the input vector X.\"\"\"\n    X = X.flatten()\n    return np.array([1.0*X**i for i in range(p+1)]).T\n\n\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\n\np = 4\nPhi = poly_features(xn, p)\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\nX_test = np.linspace(-5,5, 100).reshape(-1,1)\nPhi_test =  poly_features(X_test, p)\ny_pred = Phi_test @ theta_ml\n\n\n# Plot the training set\nplt.figure()\nplt.plot(xn, yn, '+', markersize=10)\nplt.plot(X_test, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.ylim(-5, 5) \nplt.xlim(-5, 5) \n\n(-5.0, 5.0)\n\n\n\n\n\nFigure 9.6\n\n# Values of p to consider\np_values = [0, 1, 3, 4, 6, 9]\n\n# Create a 2x3 grid of subplots\nfig, axs = plt.subplots(2, 3, figsize=(12, 8))\n\nfor i, p in enumerate(p_values):\n\n    Phi = poly_features(xn, p)\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, yn)\n    Phi_test = poly_features(X_test, p)\n    y_pred = Phi_test @ theta_ml\n\n    ax = axs[i // 3, i % 3]  # Get the correct subplot\n    ax.plot(xn, yn, '+', markersize=10,label='Training data')\n    ax.plot(X_test, y_pred, label = 'MLE')\n    ax.set_xlabel(\"$x$\")\n    ax.set_ylabel(\"$y$\")\n    ax.set_ylim(-5, 5)\n    ax.set_xlim(-5, 5)\n    ax.set_title(f\"P = {p}\")\n    ax.legend()\n\n# Adjust the spacing between subplots\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n%config InlineBackend.figure_format = \"retina\"\n\n\ndef f(x, a): return a[0] + a[1] * x\n\n\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n\ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    return ith data\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in]\n    t = T[[ix]]\n    return Phi, t\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * phi.T @ phi)\n    mN = SN @ (inv(S0) @ m0 + beta * phi.T @ t)\n    return SN, mN\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\nseed(1643)\n\n\nnobs = [1, 5,30]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html",
    "href": "blogs/blogsData/Image_super_resolution.html",
    "title": "Image Super Resolution",
    "section": "",
    "text": "The central aim of Super-Resolution (SR) is to generate a higher resolution image from lower resolution images. It is basically the process of retrieving the underlying high quality original image given a corrupted image.\n\n\nHigh resolution image offers a high pixel density and thereby more details about the original scene. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. High resolution is of importance in medical imaging for diagnosis. Many applications require zooming of a specific area of interest in the image wherein high resolution becomes essential, e.g. surveillance, forensic and satellite imaging applications.\nHowever, high resolution images are not always available. This is since the setup for high resolution imaging proves expensive and also it may not always be feasible due to the inherent limitations of the sensor, optics manufacturing technology. These problems can be overcome through the use of image processing algorithms, which are relatively inexpensive, giving rise to concept of super-resolution. It provides an advantage as it may cost less and the existing low resolution imaging systems can still be utilized.\nCamera image:\n\n\n\nflower_blur_unblur.jpg\n\n\nSecurity camera image:\n\n\n\nnumberplate_blur_unblur.jpg\n\n\nGeological image:\n\n\n\ngeo_lr_hr.png\n\n\n\n\n\nWe always do not have low and high resolution images of the same scene. So we create our own dataset. We take high resolution images (from net) and from that we create low resolution images by downscaling the high resolution images. We have used two methods for downscaling the images: 1. Bicubic Interpolation 2. Gaussian Blur\nWe won’t discussing in detail about these methods. We will be using the Gaussian Blur method for downscaling the images.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport cv2\n# Load MNIST dataset\n(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n\n# Select a random image from the dataset\nimage = x_train[np.random.randint(0, x_train.shape[0])]\n\n# Normalize the pixel values\nimage = image.astype('float32') / 255.0\n\n# Blur the image using Gaussian filter\nblurred_image = cv2.GaussianBlur(image, (3, 3), 0)\n\n# Plot the original and blurred images\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(blurred_image, cmap='gray')\nplt.title('Blurred Image')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nOne other quick way is to size down image then resize it to original size. This way we can get low resolution image."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#why-super-resolution",
    "href": "blogs/blogsData/Image_super_resolution.html#why-super-resolution",
    "title": "Image Super Resolution",
    "section": "",
    "text": "High resolution image offers a high pixel density and thereby more details about the original scene. The need for high resolution is common in computer vision applications for better performance in pattern recognition and analysis of images. High resolution is of importance in medical imaging for diagnosis. Many applications require zooming of a specific area of interest in the image wherein high resolution becomes essential, e.g. surveillance, forensic and satellite imaging applications.\nHowever, high resolution images are not always available. This is since the setup for high resolution imaging proves expensive and also it may not always be feasible due to the inherent limitations of the sensor, optics manufacturing technology. These problems can be overcome through the use of image processing algorithms, which are relatively inexpensive, giving rise to concept of super-resolution. It provides an advantage as it may cost less and the existing low resolution imaging systems can still be utilized.\nCamera image:\n\n\n\nflower_blur_unblur.jpg\n\n\nSecurity camera image:\n\n\n\nnumberplate_blur_unblur.jpg\n\n\nGeological image:\n\n\n\ngeo_lr_hr.png"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#how-we-create-our-dataset",
    "href": "blogs/blogsData/Image_super_resolution.html#how-we-create-our-dataset",
    "title": "Image Super Resolution",
    "section": "",
    "text": "We always do not have low and high resolution images of the same scene. So we create our own dataset. We take high resolution images (from net) and from that we create low resolution images by downscaling the high resolution images. We have used two methods for downscaling the images: 1. Bicubic Interpolation 2. Gaussian Blur\nWe won’t discussing in detail about these methods. We will be using the Gaussian Blur method for downscaling the images.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nimport cv2\n# Load MNIST dataset\n(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n\n# Select a random image from the dataset\nimage = x_train[np.random.randint(0, x_train.shape[0])]\n\n# Normalize the pixel values\nimage = image.astype('float32') / 255.0\n\n# Blur the image using Gaussian filter\nblurred_image = cv2.GaussianBlur(image, (3, 3), 0)\n\n# Plot the original and blurred images\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title('Original Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(blurred_image, cmap='gray')\nplt.title('Blurred Image')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nOne other quick way is to size down image then resize it to original size. This way we can get low resolution image."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#but-why-cnn-why-not-mlp",
    "href": "blogs/blogsData/Image_super_resolution.html#but-why-cnn-why-not-mlp",
    "title": "Image Super Resolution",
    "section": "But why CNN? Why not MLP?",
    "text": "But why CNN? Why not MLP?\n\nMLPs (Multilayer Perceptron) use one perceptron for each input (e.g. pixel in an image) and the amount of weights rapidly becomes unmanageable for large images. It includes too many parameters because it is fully connected. Each node is connected to every other node in next and the previous layer, forming a very dense web — resulting in redundancy and inefficiency. As a result, difficulties arise whilst training and overfitting can occur which makes it lose the ability to generalize.\nAnother common problem is that MLPs react differently to an input (images) and its shifted version — they are not translation invariant.\nThe main problems is that spatial information is lost when the image is flattened(matrix to vector) into an MLP.\n\nBasic implementation of CNN and MLP on Mnist dataset for classification.(you can skip if you know basics about CNN and MLP)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n\n\n# Load MNIST dataset\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# Preprocess the data\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\n# Reshape the data for MLP\nx_train_mlp = x_train.reshape((-1, 28*28))\nx_test_mlp = x_test.reshape((-1, 28*28))\n\n# Reshape the data for CNN\nx_train_cnn = x_train.reshape((-1, 28, 28, 1))\nx_test_cnn = x_test.reshape((-1, 28, 28, 1))\n\n\n\n\nMLP_fig_imageclassification.jpg\n\n\n\n# MLP model\nmlp_model = Sequential()\nmlp_model.add(Dense(256, activation='relu', input_shape=(28*28,)))\nmlp_model.add(Dense(128, activation='relu'))\nmlp_model.add(Dense(10, activation='softmax'))\nmlp_model.summary()\nmlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train MLP model\nmlp_model.fit(x_train_mlp, y_train, epochs=10, batch_size=32, validation_data=(x_test_mlp, y_test))\n\n# Evaluate MLP model\nmlp_loss, mlp_accuracy = mlp_model.evaluate(x_test_mlp, y_test)\nprint(\"MLP Accuracy:\", mlp_accuracy)\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_5 (Dense)             (None, 256)               200960    \n                                                                 \n dense_6 (Dense)             (None, 128)               32896     \n                                                                 \n dense_7 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 235146 (918.54 KB)\nTrainable params: 235146 (918.54 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.2051 - accuracy: 0.9395 - val_loss: 0.1189 - val_accuracy: 0.9636\nEpoch 2/10\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0852 - accuracy: 0.9729 - val_loss: 0.0767 - val_accuracy: 0.9756\nEpoch 3/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0580 - accuracy: 0.9815 - val_loss: 0.0714 - val_accuracy: 0.9762\nEpoch 4/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0455 - accuracy: 0.9856 - val_loss: 0.0743 - val_accuracy: 0.9761\nEpoch 5/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0352 - accuracy: 0.9889 - val_loss: 0.0663 - val_accuracy: 0.9798\nEpoch 6/10\n1875/1875 [==============================] - 12s 7ms/step - loss: 0.0285 - accuracy: 0.9907 - val_loss: 0.0763 - val_accuracy: 0.9792\nEpoch 7/10\n1875/1875 [==============================] - 10s 6ms/step - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.0899 - val_accuracy: 0.9775\nEpoch 8/10\n1875/1875 [==============================] - 9s 5ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0878 - val_accuracy: 0.9792\nEpoch 9/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.1028 - val_accuracy: 0.9746\nEpoch 10/10\n1875/1875 [==============================] - 7s 4ms/step - loss: 0.0165 - accuracy: 0.9941 - val_loss: 0.0898 - val_accuracy: 0.9791\n313/313 [==============================] - 1s 3ms/step - loss: 0.0898 - accuracy: 0.9791\nMLP Accuracy: 0.9790999889373779\n\n\n\n\n\nCNN_fig_imageclassification.jpg\n\n\n\n# CNN model\ncnn_model = Sequential()\ncnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\ncnn_model.add(MaxPooling2D((2, 2)))\ncnn_model.add(Flatten())\ncnn_model.add(Dense(128, activation='relu'))\ncnn_model.add(Dense(10, activation='softmax'))\ncnn_model.summary()\ncnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train CNN model\ncnn_model.fit(x_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(x_test_cnn, y_test))\n\n# Evaluate CNN model\ncnn_loss, cnn_accuracy = cnn_model.evaluate(x_test_cnn, y_test)\nprint(\"CNN Accuracy:\", cnn_accuracy)\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_1 (Conv2D)           (None, 26, 26, 32)        320       \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 13, 13, 32)        0         \n g2D)                                                            \n                                                                 \n flatten_1 (Flatten)         (None, 5408)              0         \n                                                                 \n dense_8 (Dense)             (None, 128)               692352    \n                                                                 \n dense_9 (Dense)             (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 693962 (2.65 MB)\nTrainable params: 693962 (2.65 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n1875/1875 [==============================] - 36s 19ms/step - loss: 0.1383 - accuracy: 0.9592 - val_loss: 0.0611 - val_accuracy: 0.9816\nEpoch 2/10\n1875/1875 [==============================] - 35s 19ms/step - loss: 0.0479 - accuracy: 0.9851 - val_loss: 0.0432 - val_accuracy: 0.9851\nEpoch 3/10\n1875/1875 [==============================] - 37s 20ms/step - loss: 0.0295 - accuracy: 0.9908 - val_loss: 0.0427 - val_accuracy: 0.9856\nEpoch 4/10\n1875/1875 [==============================] - 34s 18ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.0475 - val_accuracy: 0.9843\nEpoch 5/10\n1875/1875 [==============================] - 38s 20ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0455 - val_accuracy: 0.9862\nEpoch 6/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0103 - accuracy: 0.9965 - val_loss: 0.0448 - val_accuracy: 0.9864\nEpoch 7/10\n1875/1875 [==============================] - 42s 23ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.0475 - val_accuracy: 0.9871\nEpoch 8/10\n1875/1875 [==============================] - 40s 21ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0526 - val_accuracy: 0.9857\nEpoch 9/10\n1875/1875 [==============================] - 41s 22ms/step - loss: 0.0063 - accuracy: 0.9977 - val_loss: 0.0576 - val_accuracy: 0.9856\nEpoch 10/10\n1875/1875 [==============================] - 43s 23ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0526 - val_accuracy: 0.9861\n313/313 [==============================] - 2s 6ms/step - loss: 0.0526 - accuracy: 0.9861\nCNN Accuracy: 0.9861000180244446\n\n\n\n# Select a random image from the test set\nindex = np.random.randint(0, x_test.shape[0])\n# Get the image and its label\nimage = x_test[index]\nlabel = y_test[index]\n# Reshape the image for MLP model prediction\nimage_mlp = image.reshape((1, 28*28))\n# Reshape the image for CNN model prediction\nimage_cnn = image.reshape((1, 28, 28, 1))\n# Predict using MLP model\nmlp_prediction = np.argmax(mlp_model.predict(image_mlp))\n# Predict using CNN model\ncnn_prediction = np.argmax(cnn_model.predict(image_cnn))\n# Plot the image and predictions\nplt.figure(figsize=(8, 4))\n\n# MLP plot\nplt.subplot(1, 2, 1)\nplt.imshow(image, cmap='gray')\nplt.title(f\"MLP Prediction: {mlp_prediction}\")\nplt.axis('off')\n\n# CNN plot\nplt.subplot(1, 2, 2)\nplt.imshow(image, cmap='gray')\nplt.title(f\"CNN Prediction: {cnn_prediction}\")\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\n1/1 [==============================] - 0s 156ms/step\n1/1 [==============================] - 0s 82ms/step\n\n\n\n\n\nCNN does perform well on image classification task than MLP but above for toy dataset MNIST the difference is not that much. But for real world dataset the difference is huge.\nShow the difference between MLP and CNN for real world dataset.(open MLP vs CNN using transfer learning VGG16 on Snake vs Antelope dataset.ipynb)\nHere we can see that CNN is better than MLP for image classification. So we will be using CNN for our task."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#unet-architecture-for-super-resolution",
    "href": "blogs/blogsData/Image_super_resolution.html#unet-architecture-for-super-resolution",
    "title": "Image Super Resolution",
    "section": "UNET architecture for super resolution:",
    "text": "UNET architecture for super resolution:\nHere we are just understand the high level view of unet architecture. We will be discussing in detail about unet in next blog. Unet has two parts: 1. Encoder 2. Decoder\nEncoder is same as CNN where we have convolutional layers and max-pooling layers. But in decoder we have upsampling layers instead of max-pooling layers. So the image is not shrinked and we get the high resolution image.\n\n\n\nu-net-architecture.png"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#how-gans-works-analogy-counterfeiters-and-police",
    "href": "blogs/blogsData/Image_super_resolution.html#how-gans-works-analogy-counterfeiters-and-police",
    "title": "Image Super Resolution",
    "section": "How GANS works? Analogy: Counterfeiters and Police",
    "text": "How GANS works? Analogy: Counterfeiters and Police\n\nCounterfeiters: Generator\nPolice: Discriminator\n\nWe have an ambitious young criminal who wants to counterfeit money. He has a printing machine and he wants to print fake money. He has no idea how real money looks like. So he prints some money and goes to a shop to buy something. The shopkeeper is the discriminator. The shopkeeper knows how real money looks like. So he can easily identify the fake money. So the criminal goes back and prints some more money. This time the money looks more real. He goes to the shopkeeper again. The shopkeeper again identifies the fake money. This process continues until the criminal is able to print the exact replica of the real money. Now the shopkeeper is not able to identify the fake money. So the criminal is able to buy anything from the shopkeeper. The criminal has successfully fooled the shopkeeper. The criminal is the generator and the shopkeeper is the discriminator. This results in very realistic fake money. This is how GANS work.\nIn this sense both of them are getting better. The generator is getting better at generating fake money and the discriminator is getting better at identifying fake money. This is how GANS work. The generator generates fake images and the discriminator tries to identify the fake images. The generator tries to fool the discriminator and the discriminator tries to identify the fake images. This process continues until the discriminator is not able to identify the fake images. At this point the generator has successfully fooled the discriminator. The generator is now able to generate fake images which are indistinguishable from the real images.\nThis results in very realistic images. This is how GANS work.\n\nThe purpose of the generator Network is take random data initializations and decode it into synthetic sample\nThe purpose of the discriminator Network is to then take this input from our Generator and predict whether or not this sample came from the real dataset or not.\n\n\n\n\ngan_architecture.png"
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#training-gans",
    "href": "blogs/blogsData/Image_super_resolution.html#training-gans",
    "title": "Image Super Resolution",
    "section": "Training GANS",
    "text": "Training GANS\n\nTraining GANS is very difficult compared to Neural Networks we use gradient descent to change our weights and biases. But in GANS we have two networks generator and discriminator that works against eachother. So we have to train both of them simultaneously.\nWe are not seeking to minimize a loss function. We are seeking to find an equilibrium between the generator and discriminator.\nTraining stops when the discriminator is no longer able to distinguish between real and fake images.\n\n\nTraining process\n\nwe randomly generate a noisy vector\ninput this noisy vector into the generator to generate a fake image\nWe take some sample data from our real data and mix it with the fake data.\nWe train the discriminator to classifyf this mixed data as real or fake and update the weights of the discriminator.\nWe then train the generator. We make more random noisy vectors and create synthetic images. With the weights of the discriminator frozen, we use the feedbcak from the discriminator to update the weights of the generator.\n\nThis is how both Generator(to make better fake images) and Discriminator(to identify fake images) are getting better."
  },
  {
    "objectID": "blogs/blogsData/Image_super_resolution.html#gans-for-super-resolution",
    "href": "blogs/blogsData/Image_super_resolution.html#gans-for-super-resolution",
    "title": "Image Super Resolution",
    "section": "GANS for Super Resolution",
    "text": "GANS for Super Resolution\nImporting necessary libraries\n\nfrom keras.datasets import mnist\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, Dropout\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport time\n\n\n\n# a function to format display the losses\ndef hmsString(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:&gt;02}:{:&gt;05.2f}\".format(h, m, s)\n\n# downsample and introduce noise in the images\ndef downSampleAndNoisyfi(X):\n    shape = X[0].shape\n    X_down = []\n    for x_i in X:\n       x_c = cv2.resize(x_i, (shape[0]//4, shape[1]//4), interpolation = cv2.INTER_AREA)\n       x_c = np.clip(x_c+ np.random.normal(0, 5, x_c.shape) , 0, 255).astype('uint8')\n       X_down.append(x_c)\n    X_down = np.array(X_down, dtype = 'uint8')\n    return X_down\n\n\nCode for Generator Block\n\ndef Generator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])  \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X = Add()([X_shortcut, X])\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)   \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    generator_model = Model(inputs=X_input, outputs=X)\n    return generator_model\n\n\n\n\nCode for Discriminator Block\n\ndef Discriminator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.8)(X)\n    X = Activation('relu')(X)\n    \n    discriminator_model = Model(inputs=X_input, outputs=X)\n    return discriminator_model\n\n\n\n\nTraing GANS\n\n# One step of the test step\n@tf.function\ndef train_step(X, Y, generator, discriminator, generator_optimizer, discriminator_optimizer):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(X, training=True)\n\n    real_output = discriminator(Y, training=True)\n    fake_output = discriminator(generated_images, training=False)\n\n    gen_loss = tf.keras.losses.MSE(Y, generated_images)\n    disc_loss = tf.keras.losses.MSE(real_output, fake_output)\n    \n\n    gradients_of_generator = gen_tape.gradient(\\\n        gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n  return gen_loss,disc_loss\n\n# The main function to train the GAN\ndef train(X_train, Y_train, generator, discriminator, batch_size=100, epochs=50):\n    generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        prev_i = 0\n        for i in range(X_train.shape[0]):\n            if((i+1)%batch_size == 0):\n                t = train_step(X_train[prev_i:i+1], Y_train[prev_i:i+1], generator, discriminator, generator_optimizer, discriminator_optimizer)\n                gen_loss_list.append(t[0])\n                disc_loss_list.append(t[1])\n                prev_i = i+1\n        g_loss = np.sum(np.array(gen_loss_list)) / np.sum(np.array(gen_loss_list).shape)\n        d_loss = np.sum(np.array(disc_loss_list)) / np.sum(np.array(disc_loss_list).shape)\n        \n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {hmsString(epoch_elapsed)}')\n        \n    elapsed = time.time()-start\n    print (f'Training time: {hmsString(elapsed)}')\n    \n\n\n# loading the dataset(the original image are the HR 28*28 images)\n(Y_train, _), (Y_test, _) = mnist.load_data()\n# downsampling and introducing gaussian noise\n# this downsampled and noised dataset is out X or inputs\nX_train = downSampleAndNoisyfi(Y_train)\nX_test = downSampleAndNoisyfi(Y_test)\n\n# introduce a new dimension to the data (None, 28, 28, 1)\nX_test = X_test[..., np.newaxis]\nX_train = X_train[..., np.newaxis]\nY_train = Y_train[..., np.newaxis]\nY_test = Y_test[..., np.newaxis]\n\n# Creating a generator model\n# Showing the summary of generator \ngenerator = Generator((7,7,1))\ngenerator.summary()\n\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_1 (InputLayer)        [(None, 7, 7, 1)]            0         []                            \n                                                                                                  \n conv2d_2 (Conv2D)           (None, 7, 7, 32)             320       ['input_1[0][0]']             \n                                                                                                  \n batch_normalization (Batch  (None, 7, 7, 32)             128       ['conv2d_2[0][0]']            \n Normalization)                                                                                   \n                                                                                                  \n activation (Activation)     (None, 7, 7, 32)             0         ['batch_normalization[0][0]'] \n                                                                                                  \n conv2d_3 (Conv2D)           (None, 7, 7, 32)             9248      ['activation[0][0]']          \n                                                                                                  \n batch_normalization_1 (Bat  (None, 7, 7, 32)             128       ['conv2d_3[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_1 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_1[0][0]'\n                                                                    ]                             \n                                                                                                  \n add (Add)                   (None, 7, 7, 32)             0         ['activation[0][0]',          \n                                                                     'activation_1[0][0]']        \n                                                                                                  \n conv2d_4 (Conv2D)           (None, 7, 7, 32)             9248      ['add[0][0]']                 \n                                                                                                  \n batch_normalization_2 (Bat  (None, 7, 7, 32)             128       ['conv2d_4[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_2 (Activation)   (None, 7, 7, 32)             0         ['batch_normalization_2[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_1 (Add)                 (None, 7, 7, 32)             0         ['add[0][0]',                 \n                                                                     'activation_2[0][0]']        \n                                                                                                  \n activation_3 (Activation)   (None, 7, 7, 32)             0         ['add_1[0][0]']               \n                                                                                                  \n up_sampling2d (UpSampling2  (None, 14, 14, 32)           0         ['activation_3[0][0]']        \n D)                                                                                               \n                                                                                                  \n conv2d_5 (Conv2D)           (None, 14, 14, 32)           9248      ['up_sampling2d[0][0]']       \n                                                                                                  \n batch_normalization_3 (Bat  (None, 14, 14, 32)           128       ['conv2d_5[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_4 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_3[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_6 (Conv2D)           (None, 14, 14, 32)           9248      ['activation_4[0][0]']        \n                                                                                                  \n batch_normalization_4 (Bat  (None, 14, 14, 32)           128       ['conv2d_6[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_5 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_4[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_2 (Add)                 (None, 14, 14, 32)           0         ['activation_4[0][0]',        \n                                                                     'activation_5[0][0]']        \n                                                                                                  \n conv2d_7 (Conv2D)           (None, 14, 14, 32)           9248      ['add_2[0][0]']               \n                                                                                                  \n batch_normalization_5 (Bat  (None, 14, 14, 32)           128       ['conv2d_7[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_6 (Activation)   (None, 14, 14, 32)           0         ['batch_normalization_5[0][0]'\n                                                                    ]                             \n                                                                                                  \n add_3 (Add)                 (None, 14, 14, 32)           0         ['add_2[0][0]',               \n                                                                     'activation_6[0][0]']        \n                                                                                                  \n activation_7 (Activation)   (None, 14, 14, 32)           0         ['add_3[0][0]']               \n                                                                                                  \n up_sampling2d_1 (UpSamplin  (None, 28, 28, 32)           0         ['activation_7[0][0]']        \n g2D)                                                                                             \n                                                                                                  \n conv2d_8 (Conv2D)           (None, 28, 28, 32)           9248      ['up_sampling2d_1[0][0]']     \n                                                                                                  \n batch_normalization_6 (Bat  (None, 28, 28, 32)           128       ['conv2d_8[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_8 (Activation)   (None, 28, 28, 32)           0         ['batch_normalization_6[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv2d_9 (Conv2D)           (None, 28, 28, 1)            289       ['activation_8[0][0]']        \n                                                                                                  \n batch_normalization_7 (Bat  (None, 28, 28, 1)            4         ['conv2d_9[0][0]']            \n chNormalization)                                                                                 \n                                                                                                  \n activation_9 (Activation)   (None, 28, 28, 1)            0         ['batch_normalization_7[0][0]'\n                                                                    ]                             \n                                                                                                  \n==================================================================================================\nTotal params: 56997 (222.64 KB)\nTrainable params: 56547 (220.89 KB)\nNon-trainable params: 450 (1.76 KB)\n__________________________________________________________________________________________________\n\n\n\n# Creating a discriminator model\n# Showing the summary of discriminator\ndiscriminator = Discriminator((28,28,1))\ndiscriminator.summary()\n\nModel: \"model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_10 (Conv2D)          (None, 28, 28, 32)        320       \n                                                                 \n activation_10 (Activation)  (None, 28, 28, 32)        0         \n                                                                 \n conv2d_11 (Conv2D)          (None, 28, 28, 64)        18496     \n                                                                 \n batch_normalization_8 (Bat  (None, 28, 28, 64)        256       \n chNormalization)                                                \n                                                                 \n activation_11 (Activation)  (None, 28, 28, 64)        0         \n                                                                 \n=================================================================\nTotal params: 19072 (74.50 KB)\nTrainable params: 18944 (74.00 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\n\n\n\n# training with batch size of 100 and for 50 epochs\ntrain(X_train, Y_train, generator, discriminator, 100, 5) #50)\n\n# save the generator model for future use\ngenerator.save(\"mnist_generator_model\")\ngenerator.save(\"mnist_generator_model.h5\")\n\nHere we have run only for 5 epochs but you can run for more epochs to get better results.\n\n# testing the model\nY_pred = generator.predict(X_test)\n# showing the first 5 results\nfig,a =  plt.subplots(3,5)\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    a[0][i].imshow(X_test[i])\n    a[0][i].axes.get_xaxis().set_visible(False)\n    a[0][i].axes.get_yaxis().set_visible(False)\n    a[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    a[1][i].imshow(Y_pred[i])\n    a[1][i].axes.get_xaxis().set_visible(False)\n    a[1][i].axes.get_yaxis().set_visible(False)\n    a[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    a[2][i].imshow(Y_test[i])\n    a[2][i].axes.get_xaxis().set_visible(False)\n    a[2][i].axes.get_yaxis().set_visible(False)\n    a[2][i].title.set_text(\"HR: \"+str(i+1)) \n\n313/313 [==============================] - 9s 27ms/step\n\n\n\n\n\n\n# showing the first 5 random results\nimport random\nfigb,ab =  plt.subplots(3,5)\nfigb.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    ii = random.randint(0, 10000) \n    \n    ab[0][i].imshow(X_test[ii])\n    ab[0][i].axes.get_xaxis().set_visible(False)\n    ab[0][i].axes.get_yaxis().set_visible(False)\n    ab[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    ab[1][i].imshow(Y_pred[ii])\n    ab[1][i].axes.get_xaxis().set_visible(False)\n    ab[1][i].axes.get_yaxis().set_visible(False)\n    ab[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    ab[2][i].imshow(Y_test[ii])\n    ab[2][i].axes.get_xaxis().set_visible(False)\n    ab[2][i].axes.get_yaxis().set_visible(False)\n    ab[2][i].title.set_text(\"HR: \"+str(i+1)) \n\n\n\n\nWell GAN does perform good but it has some problems.\n\n\nProblem with GANS:\n\nAchieving equilibrium: between the generator and discriminator is very difficult.\nTime: Training gans is computationally expensive and necessitates tweaking of hyperparameters such as initializations, altering hidden layers, different activation, using Batch Normalization or Dropout, etc.\nBad Initializations: If the generator and discriminator are not initialized properly, then the training will fail.\nMode Collapse: happens when regardless of the nosie input fed into your generator, the generated output varies very little. It occurs when a small set of images look good to the descriminator and get scored better than other images. The GAN simple learns to reproduce those images over and over again. Analgous to overfittiing.\n\nOne quick solution to the problem of high training time is to use transfer learning using VGG16 or VGG19 in Generator and discriminator architecture. This will reduce the training time."
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Process and Screen\n\n\n\n\n\n\n\nSystem\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nSuraj Jaiswal and Siddhesh Dosi\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression using the Pyro\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nImage Super Resolution\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nBaysian Linear Regression blog\n\n\n\n\n\n\n\nRegression\n\n\nBayesian\n\n\n\n\n\n\n\n\n\n\n\nJun 18, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nPlots for bayesian LR\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nBaysian Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html",
    "href": "demo_notebooks/bayesian_linear_regression.html",
    "title": "Bayesian linear regression",
    "section": "",
    "text": "1. What is Bayesian linear regression?\n2. Recap linear regression\n3. Fundamental concepts\n4. Linear regression from a probabilistic perspective\n5. Linear regression with basis functions\n\n5.1 Example basis functions\n5.2 The design matrix\n\n6. Bayesian Linear Regression\n\n6.1 Step 1: Probabilistic Model\n6.2 Generating a dataset\n6.3 Step 2: Posterior over the parameters\n6.4 Visualizing the parameter posterior\n6.5 Step 3: Posterior predictive distribution\n6.6 Visualizing the predictive posterior\n\nSources and further reading"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "href": "demo_notebooks/bayesian_linear_regression.html#link-to-interactive-demo",
    "title": "Bayesian linear regression",
    "section": "Link to interactive demo",
    "text": "Link to interactive demo\nClick here to run the notebook online (using Binder) without installing jupyter or downloading the code.\nSometimes, the GitHub version of the Jupyter notebook does not display the math formulas correctly. Please refer to the Binder version in case you think something might be off or missing.\nI also wrote a blog post containing the contents of the notebook."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "href": "demo_notebooks/bayesian_linear_regression.html#what-is-bayesian-linear-regression-blr",
    "title": "Bayesian linear regression",
    "section": "1. What is Bayesian linear regression (BLR)? ",
    "text": "1. What is Bayesian linear regression (BLR)? \nBayesian linear regression is the Bayesian interpretation of linear regression. What does that mean? To answer this question we first have to understand the Bayesian approach. In most of the algorithms we have looked at so far we computed point estimates of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function. In the Bayesian approach we don’t work with exact values but with probabilities. This allows us to model the uncertainty in our parameter estimates. Why is this important?\nIn nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty. Bayesian methods have been around for a long time and are widely-used in many areas of science (e.g. astronomy). Although Bayesian methods have been applied to machine learning problems too, they are usually less well known to beginners. The major reason is that they require a good understanding of probability theory.\nIn the following notebook we will work our way from linear regression to Bayesian linear regression, including the most important theoretical knowledge and code examples."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#recap-linear-regression",
    "title": "Bayesian linear regression",
    "section": "2. Recap linear regression ",
    "text": "2. Recap linear regression \n\nIn linear regression, we want to find a function \\(f\\) that maps inputs \\(x \\in \\mathbb{R}^D\\) to corresponding function values \\(f(x) \\in \\mathbb{R}\\).\nWe are given an input dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\), where \\(y_n\\) is a noisy observation value: \\(y_n = f(x_n) + \\epsilon\\), with \\(\\epsilon\\) being an i.i.d. random variable that describes measurement/observation noise\nOur goal is to infer the underlying function \\(f\\) that generated the data such that we can predict function values at new input locations\nIn linear regression, we model the underlying function \\(f\\) using a linear combination of the input features:\n\n\\[\n\\begin{split}\ny &= \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_d x_d \\\\\n&= \\boldsymbol{x}^T \\boldsymbol{\\theta}\n\\end{split}\n\\]\n\nFor more details take a look at the notebook on linear regression"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "href": "demo_notebooks/bayesian_linear_regression.html#fundamental-concepts",
    "title": "Bayesian linear regression",
    "section": "3. Fundamental concepts ",
    "text": "3. Fundamental concepts \n\nOne fundamental tool in Bayesian learning is Bayes’ theorem\nBayes’ theorem looks as follows: \\[\n\\begin{equation}\np(\\boldsymbol{\\theta} | \\mathbf{x}, y) = \\frac{p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})}{p(\\boldsymbol{x}, y)}\n\\end{equation}\n\\]\n\\(p(y | \\boldsymbol{x}, \\boldsymbol{\\theta})\\) is the likelihood. It describes the probability of the target values given the data and parameters.\n\\(p(\\boldsymbol{\\theta})\\) is the prior. It describes our initial knowledge about which parameter values are likely and unlikely.\n\\(p(\\boldsymbol{x}, y)\\) is the evidence. It describes the joint probability of the data and targets.\n\n\\(p(\\boldsymbol{\\theta} | \\boldsymbol{x}, y)\\) is the posterior. It describes the probability of the parameters given the observed data and targets. \nAnother important tool you need to know about is the Gaussian distribution. If you are not familiar with it I suggest you pause for a minute and understand its main properties before reading on.\n\nIn general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis \\(p(h)\\) 2. We observe some data, representating new evidence \\(e\\) 3. We use Bayes’ theorem to update our belief given the new evidence: \\(p(h|e) = \\frac{p(e |h)p(h)}{p(e)}\\)\nFor more information take a look at the Wikipedia article on Bayesian inference."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-from-a-probabilistic-perspective",
    "title": "Bayesian linear regression",
    "section": "4. Linear regression from a probabilistic perspective ",
    "text": "4. Linear regression from a probabilistic perspective \nIn order to pave the way for Bayesian linear regression we will take a probabilistic spin on linear regression. Let’s start by explicitly modelling the observation noise \\(\\epsilon\\). For simplicity, we assume that \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nAs mentioned in the beginning, a simple linear regression model assumes that the target function \\(f(x)\\) is given by a linear combination of the input features: \\[\n\\begin{split}\ny = f(\\boldsymbol{x}) + \\epsilon \\\\\n  = \\boldsymbol{x}^T \\boldsymbol{\\theta} + \\epsilon\n\\end{split}\n\\]\nThis corresponds to the following likelihood function: \\[p(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{x}^T \\boldsymbol{\\theta}, \\sigma^2)\\]\nOur goal is to find the parameters \\(\\boldsymbol{\\theta} = \\{\\theta_1, ..., \\theta_D\\}\\) that model the given data best. In standard linear regression we can find the best parameters using a least-squares, maximum likelihood (ML) or maximum a posteriori (MAP) approach. If you want to know more about these solutions take a look at the notebook on linear regression or at chapter 9.2 of the book Mathematics for Machine Learning."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "href": "demo_notebooks/bayesian_linear_regression.html#linear-regression-with-basis-functions",
    "title": "Bayesian linear regression",
    "section": "5. Linear regression with basis functions ",
    "text": "5. Linear regression with basis functions \nThe simple linear regression model above is linear not only with respect to the parameters \\(\\boldsymbol{\\theta}\\) but also with respect to the inputs \\(\\boldsymbol{x}\\). When \\(\\boldsymbol{x}\\) is not a vector but a single value (that is, the dataset is one-dimensional) the model \\(y_i = x_i \\cdot \\theta\\) describes straight lines with \\(\\theta\\) being the slope of the line.\nThe plot below shows example lines produced with the model \\(y = x \\cdot \\theta\\), using different values for the slope \\(\\theta\\) and intercept 0.\n\nHaving a model which is linear both with respect to the parameters and inputs limits the functions it can learn significantly. We can make our model more powerful by making it nonlinear with respect to the inputs. After all, linear regression refers to models which are linear in the parameters, not necessarily in the inputs (linear in the parameters means that the model describes a function by a linear combination of input features).\nMaking the model nonlinear with respect to the inputs is easy. We can adapt it by using a nonlinear transformation of the input features \\(\\phi(\\boldsymbol{x})\\). With this adaptation our model looks as follows: \\[\n\\begin{split}\ny &= \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon \\\\\n&= \\sum_{k=0}^{K-1} \\theta_k \\phi_k(\\boldsymbol{x}) + \\epsilon\n\\end{split}\n\\]\nWhere \\(\\boldsymbol{\\phi}: \\mathbf{R}^D \\rightarrow \\mathbf{R}^K\\) is a (non)linear transformation of the inputs \\(\\boldsymbol{x}\\) and \\(\\phi_k: \\mathbf{R}^D \\rightarrow \\mathbf{R}\\) is the \\(k-\\)th component of the feature vector \\(\\boldsymbol{\\phi}\\):\n\\[\n\\boldsymbol{\\phi}(\\boldsymbol{x})=\\left[\\begin{array}{c}\n\\phi_{0}(\\boldsymbol{x}) \\\\\n\\phi_{1}(\\boldsymbol{x}) \\\\\n\\vdots \\\\\n\\phi_{K-1}(\\boldsymbol{x})\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith our new nonlinear transformation the likelihood function is given by\n\\[\np(y | \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta},\\, \\sigma^2)\n\\]\n\n5.1 Example basis functions \n\nLinear regression\nThe easiest example for a basis function (for one-dimensional data) would be simple linear regression, that is, no non-linear transformation at all. In this case we would choose \\(\\phi_0(x) = 1\\) and \\(\\phi_i(x) = x\\). This would result in the following vector \\(\\boldsymbol{\\phi}(x)\\):\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\n\\vdots \\\\\nx\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\n\n\nPolynomial regression\nAnother common choice of basis function for the one-dimensional case is polynomial regression. For this we would set \\(\\phi_i(x) = x^i\\) for \\(i=0, ..., K-1\\). The corresponding feature vector \\(\\boldsymbol{\\phi}(x)\\) would look as follows:\n\\[\n\\boldsymbol{\\phi}(x)=\n\\left[\n\\begin{array}{c}\n\\phi_{0}(x) \\\\\n\\phi_{1}(x) \\\\\n\\vdots \\\\\n\\phi_{K-1}(x)\n\\end{array}\n\\right] =\n\\left[\n\\begin{array}{c}\n1 \\\\\nx \\\\\nx^2 \\\\\nx^3 \\\\\n\\vdots \\\\\nx^{K-1}\n\\end{array}\n\\right]\n\\in \\mathbb{R}^{K}\n\\]\nWith this transformation we can lift our original one-dimensional input into a \\(K\\)-dimensional feature space. Our function \\(f\\) can be any polynomial with degree \\(\\le K-1\\): \\(f(x) = \\sum_{k=0}^{K-1} \\theta_k x^k\\)\n\n\n\n5.2 The design matrix \nTo make it easier to work with the transformations \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\) for the different input vectors \\(\\boldsymbol{x}\\) we typically create a so called design matrix (also called feature matrix). Given our dataset \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\) we define the design matrix as follows:\n\\[\n\\boldsymbol{\\Phi}:=\\left[\\begin{array}{c}\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\vdots \\\\\n\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n\\phi_{0}\\left(\\boldsymbol{x}_{1}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{1}\\right) \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{2}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{2}\\right) \\\\\n\\vdots & & \\vdots \\\\\n\\phi_{0}\\left(\\boldsymbol{x}_{N}\\right) & \\cdots & \\phi_{K-1}\\left(\\boldsymbol{x}_{N}\\right)\n\\end{array}\\right] \\in \\mathbb{R}^{N \\times K}\n\\]\nNote that the design matrix is of shape \\(N \\times K\\). \\(N\\) is the number of input examples and \\(K\\) is the output dimension of the non-linear transformation \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\)."
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "href": "demo_notebooks/bayesian_linear_regression.html#bayesian-linear-regression",
    "title": "Bayesian linear regression",
    "section": "6. Bayesian linear regression ",
    "text": "6. Bayesian linear regression \nWhat changes when we consider a Bayesian interpretation of linear regression? Our data stays the same as before: \\(D = \\big \\{ \\mathbf{x}_n, y_n \\big \\}_{n=1}^N\\). Given the data \\(D\\) we can define the set of all inputs as \\(\\mathcal{X} := \\{\\boldsymbol{x}_1, ..., \\boldsymbol{x}_n\\}\\) and the set of all targets as \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\).\nIn simple linear regression we compute point estimates of our parameters (e.g. using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates distributions over the parameters and predictions. This allows us to model the uncertainty in our predictions.\nTo perform Bayesian linear regression we follow three steps: 1. We set up a probabilistic model that describes our assumptions how the data and parameters are generated 2. We perform inference for the parameters \\(\\boldsymbol{\\theta}\\), that is, we compute the posterior probability distribution over the parameters 3. With this posterior we can perform inference for new, unseen inputs \\(y_*\\). In this step we don’t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs.\n\n6.1 Step 1: Probabilistic model \nWe start by setting up a probabilistic model that describes our assumptions how the data and parameters are generated. For this, we place a prior \\(p(\\boldsymbol{\\theta})\\) over our parameters which encodes what parameter values are plausible (before we have seen any data). Example: With a single parameter \\(\\theta\\), a Gaussian prior \\(p(\\theta) = \\mathcal{N}(0, 1)\\) says that parameter values are normally distributed with mean 0 and standard deviation 1. In other words: the parameter values are most likely to fall into the interval [−2,2] which is two standard deviations around the mean value.\nTo keep things simple we will assume a Gaussian prior over the parameters: \\(p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{m}_0, \\boldsymbol{S}_0)\\). Let’s further assume that the likelihood function is Gaussian, too: \\(p(y \\mid \\boldsymbol{x}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(y \\mid \\boldsymbol{\\phi}^{\\top}(\\boldsymbol{x}) \\boldsymbol{\\theta}, \\sigma^{2}\\right)\\).\nNote: When considering the set of all targets \\(\\mathcal{Y} := \\{y_1, ..., y_n \\}\\), the likelihood function becomes a multivariate Gaussian distribution: \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\nThe nice thing about choosing a Gaussian distribution for our prior is that the posterior distributions will be Gaussian, too (keyword conjugate prior)!\nWe will start our BayesianLinearRegression class with the knowledge we have so far - our probabilistic model. As mentioned in the beginning we assume that the variance \\(\\sigma^2\\) of the noise \\(\\epsilon\\) is known. Furthermore, to allow plotting the data later on we will assume that it’s two dimensional (d=2).\n\nfrom scipy.stats import multivariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n        \n# Let's make sure that we can initialize our model\nprior_mean = np.array([0, 0])\nprior_cov = np.array([[0.5, 0], [0, 0.5]])\nnoise_var = 0.2\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\n\n\n6.2 Generating a dataset \nBefore going any further we need a dataset to test our implementation. Remember that we assume that our targets were generated by a function of the form \\(y = \\boldsymbol{\\phi}^T(\\boldsymbol{x}) \\boldsymbol{\\theta} + \\epsilon\\) where \\(\\epsilon\\) is normally distributed with mean \\(0\\) and some known variance \\(\\sigma^2\\): \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\).\nTo keep things simple we will work with one-dimensional data and simple linear regression (that is, no non-linear transformation of the inputs). Consequently, our data generating function will be of the form \\[ y = \\theta_0 + \\theta_1 \\, x + \\epsilon \\]\nNote that we added a parameter \\(\\theta_0\\) which corresponds to the intercept of the linear function. Until know we assumed \\(\\theta_0 = 0\\). As mentioned earlier, \\(\\theta_1\\) represents the slope of the linear function.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef compute_function_labels(slope: float, intercept: float, noise_std_dev: float, data: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute target values given function parameters and data.\n    \n    Args:\n        slope: slope of the function (theta_1)\n        intercept: intercept of the function (theta_0)\n        data: input feature values (x)\n        noise_std_dev: standard deviation of noise distribution (sigma)\n        \n    Returns:\n        target values, either true or corrupted with noise\n    \"\"\"\n    n_samples = len(data)\n    if noise_std_dev == 0: # Real function\n        return slope * data + intercept\n    else: # Noise corrupted\n        return slope * data + intercept + np.random.normal(0, noise_std_dev, n_samples)\n\n\n# Set random seed to ensure reproducibility\nseed = 42\nnp.random.seed(seed)\n\n# Generate true values and noise corrupted targets\nn_datapoints = 1000\nintercept = -0.7\nslope = 0.9\nnoise_std_dev = 0.5\nnoise_var = noise_std_dev**2\nlower_bound = -1.5\nupper_bound = 1.5\n\n# Generate dataset\nfeatures = np.random.uniform(lower_bound, upper_bound, n_datapoints)\nlabels = compute_function_labels(slope, intercept, 0., features)\nnoise_corrupted_labels = compute_function_labels(slope, intercept, noise_std_dev, features)\n\n\n# Plot the dataset\nplt.figure(figsize=(10,7))\nplt.plot(features, labels, color='r', label=\"True values\")\nplt.scatter(features, noise_corrupted_labels, label=\"Noise corrupted values\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Labels\")\nplt.title(\"Real function along with noisy targets\")\nplt.legend();\n\n\n\n\n\n\n6.3 Step 2: Posterior over the parameters \nWe finished setting up our probabilistic model. Next, we want to use this model and our dataset \\(\\mathcal{X, Y}\\) to estimate the parameter posterior \\(p(\\boldsymbol{\\theta} | \\mathcal{X, Y})\\). Keep in mind that we don’t compute point estimates of the parameters. Instead, we determine the mean and variance of the (Gaussian) posterior distribution and use this entire distribution when making predictions.\nWe can estimate the parameter posterior using Bayes theorem: \\[\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y})=\\frac{p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\mathcal{Y} \\mid \\mathcal{X})}\n\\]\n\n\\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})\\) is the likelihood function, \\(p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{y} \\mid \\boldsymbol{\\Phi} \\boldsymbol{\\theta}, \\sigma^{2} \\boldsymbol{I}\\right)\\)\n\\(p(\\boldsymbol{\\theta})\\) is the prior distribution, \\(p(\\boldsymbol{\\theta})=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{0}, \\boldsymbol{S}_{0}\\right)\\)\n\\(p(\\mathcal{Y} \\mid \\mathcal{X})=\\int p(\\mathcal{Y} \\mid \\mathcal{X}, \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta}\\) is the evidence which ensures that the posterior is normalized (that is, that it integrates to 1).\n\nThe parameter posterior can be estimated in closed form (for proof see theorem 9.1 in the book Mathematics for Machine Learning): \\[\n\\begin{aligned}\np(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) &=\\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\\\\n\\boldsymbol{S}_{N} &=\\left(\\boldsymbol{S}_{0}^{-1}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{\\Phi}\\right)^{-1} \\\\\n\\boldsymbol{m}_{N} &=\\boldsymbol{S}_{N}\\left(\\boldsymbol{S}_{0}^{-1} \\boldsymbol{m}_{0}+\\sigma^{-2} \\boldsymbol{\\Phi}^{\\top} \\boldsymbol{y}\\right)\n\\end{aligned}\n\\]\nComing back to our BayesLinearRegression class we need to add a method which allows us to update the posterior distribution given a dataset.\n\nfrom scipy.stats import multivariate_normal\nfrom scipy.stats import norm as univariate_normal\nimport numpy as np\n\nclass BayesianLinearRegression:\n    \"\"\" Bayesian linear regression\n    \n    Args:\n        prior_mean: Mean values of the prior distribution (m_0)\n        prior_cov: Covariance matrix of the prior distribution (S_0)\n        noise_var: Variance of the noise distribution\n    \"\"\"\n    \n    def __init__(self, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: float):\n        self.prior_mean = prior_mean[:, np.newaxis] # column vector of shape (1, d)\n        self.prior_cov = prior_cov # matrix of shape (d, d)\n        # We initalize the prior distribution over the parameters using the given mean and covariance matrix\n        # In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)\n        self.prior = multivariate_normal(prior_mean, prior_cov)\n        \n        # We also know the variance of the noise\n        self.noise_var = noise_var # single float value\n        self.noise_precision = 1 / noise_var\n        \n        # Before performing any inference the parameter posterior equals the parameter prior\n        self.param_posterior = self.prior\n        # Accordingly, the posterior mean and covariance equal the prior mean and variance\n        self.post_mean = self.prior_mean # corresponds to m_N in formulas\n        self.post_cov = self.prior_cov # corresponds to S_N in formulas\n        \n    def update_posterior(self, features: np.ndarray, targets: np.ndarray):\n        \"\"\"\n        Update the posterior distribution given new features and targets\n        \n        Args:\n            features: numpy array of features\n            targets: numpy array of targets\n        \"\"\"\n        # Reshape targets to allow correct matrix multiplication\n        # Input shape is (N,) but we need (N, 1)\n        targets = targets[:, np.newaxis]\n        \n        # Compute the design matrix, shape (N, 2)\n        design_matrix = self.compute_design_matrix(features)\n\n        # Update the covariance matrix, shape (2, 2)\n        design_matrix_dot_product = design_matrix.T.dot(design_matrix)\n        inv_prior_cov = np.linalg.inv(self.prior_cov)\n        self.post_cov = np.linalg.inv(inv_prior_cov +  self.noise_precision * design_matrix_dot_product)\n        \n        # Update the mean, shape (2, 1)\n        self.post_mean = self.post_cov.dot( \n                         inv_prior_cov.dot(self.prior_mean) + \n                         self.noise_precision * design_matrix.T.dot(targets))\n\n        \n        # Update the posterior distribution\n        self.param_posterior = multivariate_normal(self.post_mean.flatten(), self.post_cov)\n                \n    def compute_design_matrix(self, features: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Compute the design matrix. To keep things simple we use simple linear\n        regression and add the value phi_0 = 1 to our input data.\n        \n        Args:\n            features: numpy array of features\n        Returns:\n            design_matrix: numpy array of transformed features\n            \n        &gt;&gt;&gt; compute_design_matrix(np.array([2, 3]))\n        np.array([[1., 2.], [1., 3.])\n        \"\"\"\n        n_samples = len(features)\n        phi_0 = np.ones(n_samples)\n        design_matrix = np.stack((phi_0, features), axis=1)\n        return design_matrix\n    \n \n    def predict(self, features: np.ndarray):\n        \"\"\"\n        Compute predictive posterior given new datapoint\n        \n        Args:\n            features: 1d numpy array of features\n        Returns:\n            pred_posterior: predictive posterior distribution\n        \"\"\"\n        design_matrix = self.compute_design_matrix(features)\n        \n        pred_mean = design_matrix.dot(self.post_mean)\n        pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n        \n        pred_posterior = univariate_normal(loc=pred_mean.flatten(), scale=pred_cov**0.5)\n        return pred_posterior\n\n\n\n6.4 Visualizing the parameter posterior \nTo ensure that our implementation is correct we can visualize how the posterior over the parameters changes as the model sees more data. We will visualize the distribution using a contour plot - a method for visualizing three-dimensional functions. In our case we want to visualize the density of our bi-variate Gaussian for each point (that is, each slope/intercept combination). The plot below shows an example which illustrates how the lines and colours of a contour plot correspond to a Gaussian distribution:\n\nAs we can see, the density is highest in the yellow regions decreasing when moving further out into the green and blue parts. This should give you a better understanding of contour plots.\nTo analyze our Bayesian linear regression class we will start by initializing a new model. We can visualize its prior distribution over the parameters before the model has seen any real data.\n\n# Initialize BLR model\nprior_mean = np.array([0, 0])\nprior_cov = 1/2 * np.identity(2)\nblr = BayesianLinearRegression(prior_mean, prior_cov, noise_var)\n\ndef plot_param_posterior(lower_bound, upper_bound, blr, title):\n    fig = plt.figure()\n    mesh_features, mesh_labels = np.mgrid[lower_bound:upper_bound:.01, lower_bound:upper_bound:.01]\n    pos = np.dstack((mesh_features, mesh_labels))\n    plt.contourf(mesh_features, mesh_labels, blr.param_posterior.pdf(pos), levels=15)\n    plt.scatter(intercept, slope, color='red', label=\"True parameter values\")\n    plt.title(title)\n    plt.xlabel(\"Intercept\")\n    plt.ylabel(\"Slope\")\n    plt.legend();\n    \n# Visualize parameter prior distribution\nplot_param_posterior(lower_bound, upper_bound, blr, title=\"Prior parameter distribution\")\n\n\n\n\nThe plot above illustrates both the prior parameter distribution and the true parameter values that we want to find. If our model works correctly, the posterior distribution should become more narrow and move closer to the true parameter values as the model sees more datapoints. This can be visualized with contour plots, too! Below we update the posterior distribution iteratively as the model sees more and more data. The contour plots for each step show how the parameter posterior develops and converges close to the true values in the end.\n\nn_points_lst = [1, 5, 10, 50, 100, 200, 500, 1000]\nprevious_n_points = 0\nfor n_points in n_points_lst:\n    train_features = features[previous_n_points:n_points]\n    train_labels = noise_corrupted_labels[previous_n_points:n_points]\n    blr.update_posterior(train_features, train_labels)\n    \n    # Visualize updated parameter posterior distribution\n    plot_param_posterior(lower_bound, \n                         upper_bound, \n                         blr, \n                         title=f\"Updated parameter distribution using {n_points} datapoints\")\n    \n    previous_n_points = n_points\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.5 Step 3: Posterior predictive distribution \nGiven the posterior distribution over the parameters we can determine the predictive distribution (= posterior over the outputs) for a new input \\((\\boldsymbol{x}_*, y_*)\\). This is the distribution we are really interested in. A trained model is not particularly useful when we can’t use it to make predictions, right?\nThe posterior predictive distribution looks as follows:\n\\[\n\\begin{aligned}\np\\left(y_{*} \\mid \\mathcal{X}, \\mathcal{Y}, \\boldsymbol{x}_{*}\\right) &=\\int p\\left(y_{*} \\mid \\boldsymbol{x}_{*}, \\boldsymbol{\\theta}\\right) p(\\boldsymbol{\\theta} \\mid \\mathcal{X}, \\mathcal{Y}) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\int \\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{\\theta}, \\sigma^{2}\\right) \\mathcal{N}\\left(\\boldsymbol{\\theta} \\mid \\boldsymbol{m}_{N}, \\boldsymbol{S}_{N}\\right) \\mathrm{d} \\boldsymbol{\\theta} \\\\\n&=\\mathcal{N}\\left(y_{*} \\mid \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{m}_{N}, \\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\right)\n\\end{aligned}\n\\]\nFirst of all: note that the predictive posterior for a new input \\(\\boldsymbol{x}_{*}\\) is a univariate Gaussian distribution. We can see that the mean of the distribution is given by the product of the design matrix for the new example (\\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right)\\)) and the mean of the parameter posterior (\\(\\boldsymbol{m}_{N}\\)). The variance \\((\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)+\\sigma^{2}\\)) of the predictive posterior has two parts: 1. \\(\\sigma^{2}\\): The variance of the noise 2. \\(\\boldsymbol{\\phi}^{\\top}\\left(\\boldsymbol{x}_{*}\\right) \\boldsymbol{S}_{N} \\boldsymbol{\\phi}\\left(\\boldsymbol{x}_{*}\\right)\\): The posterior uncertainty associated with the parameters \\(\\boldsymbol{\\theta}\\)\nLet’s add a predict method to our BayesianLinearRegression class which computes the predictive posterior for a new input (you will find the method in the class definition above):\n\ndef predict(self, features: np.ndarray):\n    \"\"\"\n    Compute predictive posterior given new datapoint\n\n    Args:\n        features: 1d numpy array of features\n    Returns:\n        pred_posterior: predictive posterior distribution\n    \"\"\"\n    design_matrix = self.compute_design_matrix(features)\n\n    pred_mean = design_matrix.dot(self.post_mean)\n    pred_cov = design_matrix.dot(self.post_cov.dot(design_matrix.T)) + self.noise_var\n\n    pred_posterior = univariate_normal(pred_mean.flatten(), pred_cov)\n    return pred_posterior\n\n\n\n6.6 Visualizing the predictive posterior \nOur original dataset follows a simple linear function. After training the model it should be able to predict labels for new datapoints, even if they lie beyond the range from [-1.5, 1.5]. But how can we get from the predictive distribution that our model computes to actual labels? That’s easy: we sample from the predictive posterior.\nTo make sure that we are all on the same page: given a new input example our Bayesian linear regression model predicts not a single label but a distribution over possible labels. This distribution is Gaussian. We can get actual labels by sampling from this distribution.\nThe code below implements and visualizes this: - We create some test features for which we want predictions - Each feature is given to the trained BLR model which returns a univariate Gaussian distribution over possible labels (pred_posterior = blr.predict(np.array([feat]))) - We sample from this distribution (sample_predicted_labels = pred_posterior.rvs(size=sample_size)) - The predicted labels are saved in a format that makes it easy to plot them - Finally, we plot each input feature, its true label and the sampled predictions. Remember: the samples are generated from the predictive posterior returned by the predict method. Think of a Gaussian distribution plotted along the y-axis for each feature. We visualize this with a histogram: more likely values close to the mean will be sampled more often than less likely values.\n\nimport pandas as pd\nimport seaborn as sns\n\nall_rows = []\nsample_size = 1000\ntest_features = [-2, -1, 0, 1, 2]\nall_labels = []\n\nfor feat in test_features:\n    true_label = compute_function_labels(slope, intercept, 0, np.array([feat]))\n    all_labels.append(true_label)\n    pred_posterior = blr.predict(np.array([feat]))\n    sample_predicted_labels = pred_posterior.rvs(size=sample_size)\n    for label in sample_predicted_labels:\n        all_rows.append([feat, label])\n        \nall_data = pd.DataFrame(all_rows, columns=[\"feature\", \"label\"]) \nsns.displot(data=all_data, x=\"feature\", y=\"label\")\nplt.scatter(x=test_features, y=all_labels, color=\"red\", label=\"True values\")\nplt.title(\"Predictive posterior distributions\")\nplt.legend()\nplt.plot();"
  },
  {
    "objectID": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "href": "demo_notebooks/bayesian_linear_regression.html#sources-and-further-reading",
    "title": "Bayesian linear regression",
    "section": "Sources and further reading ",
    "text": "Sources and further reading \nThe basis for this notebook is chapter 9.2 of the book Mathematics for Machine Learning. I can highly recommend to read through chapter 9 to get a deeper understanding of (Bayesian) linear regression.\nYou will find explanations and an implementation of simple linear regression in the notebook on linear regression"
  },
  {
    "objectID": "demo_notebooks/ISR_CNN_UNET.html",
    "href": "demo_notebooks/ISR_CNN_UNET.html",
    "title": "Drawing the model (using ONNX and Netron)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport torch\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n# Download some MNIST to demonstrate super-resolution\nfrom torchvision import datasets, transforms\nmnist = datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor())\nmnist_test = datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor())\n\n\n\n# Displaying an image\ndef show_image(img):\n    plt.imshow(img.permute(1, 2, 0).squeeze(), cmap='gray')\n    plt.axis('off')\n\n# Displaying a batch of images in 1 row and n columns\ndef show_batch(batch):\n    fig, ax = plt.subplots(1, len(batch), figsize=(20, 20))\n    for i, img in enumerate(batch):\n        ax[i].imshow(img.permute(1, 2, 0).squeeze(), cmap='gray')\n        ax[i].axis('off')\n    \n\n\nshow_image(mnist[0][0])\n\n\n\n\n\nshow_batch(torch.stack([mnist[i][0] for i in range(10)]))\n\n\n\n\n\nmnist[0][0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Downsample the images\ndownsample = transforms.Resize(7)\n\n# First 10000 images X\nmnist_small = [downsample(mnist[i][0]) for i in range(10000)]\nmnist_small = torch.stack(mnist_small)\n\n# First 10000 images Y\nmnist_large = torch.stack([mnist[i][0] for i in range(10000)])\n\n# Test set X\nmnist_test_small = [downsample(mnist_test[i][0]) for i in range(10000)]\nmnist_test_small = torch.stack(mnist_test_small)\n\n# Test set Y\nmnist_test_large = torch.stack([mnist_test[i][0] for i in range(10000)])\n\nC:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n\n\n\n# Show the downsampled images and the original images side-by-side\n\nshow_batch(torch.stack([mnist_small[i] for i in range(10)]))\nplt.figure()\nshow_batch(torch.stack([mnist[i][0] for i in range(10)]))\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\nmnist_small.shape, mnist.data.shape\n\n(torch.Size([10000, 1, 7, 7]), torch.Size([60000, 28, 28]))\n\n\n\nimport torch\nimport torch.nn as nn\n\nclass SinActivation(nn.Module):\n    def forward(self, x):\n        return torch.sin(x)\n\n# Create an instance of the custom SinActivation module\nsin_activation = SinActivation()\n\nclass UNet(nn.Module):\n    def __init__(self, activation=sin_activation):\n        super(UNet, self).__init__()\n\n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # Input: (batch_size, 1, 7, 7), Output: (batch_size, 16, 7, 7)\n            # Use the custom activation function\n            activation,\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # Input: (batch_size, 16, 7, 7), Output: (batch_size, 32, 7, 7)\n            activation,\n            nn.MaxPool2d(kernel_size=2, stride=2)  # Input: (batch_size, 32, 7, 7), Output: (batch_size, 32, 3, 3)\n        )\n\n        # Bottleneck\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # Input: (batch_size, 32, 3, 3), Output: (batch_size, 64, 3, 3)\n            activation,\n        )\n\n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=4, padding=0),  # Input: (batch_size, 64, 3, 3), Output: (batch_size, 32, 12, 12)\n            activation,\n            # Input (batch_size, 32, 12, 12), Output: (batch_size, 16, 12, 12)\n            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=0),\n            activation,\n            # Input (batch_size, 16, 12, 12), Output: (batch_size, 1, 28, 28)\n            nn.ConvTranspose2d(16, 1, kernel_size=4, stride=2, padding=1)\n            \n            )\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.encoder(x)\n\n        # Bottleneck\n        x = self.bottleneck(x1)\n\n        # Decoder\n        x = self.decoder(x)\n\n        return x\n\n# Create an instance of the modified UNet model\nmodel = UNet(nn.GELU())\n\n# Print the model architecture with input and output shape\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\ndummy_input = torch.randn(input_size)\noutput = model(dummy_input)\nprint(model)\nprint(f\"Input shape: {input_size}\")\nprint(f\"Output shape: {output.shape}\")\n\nUNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): GELU(approximate='none')\n    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): GELU(approximate='none')\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (bottleneck): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): GELU(approximate='none')\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(4, 4))\n    (1): GELU(approximate='none')\n    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n    (3): GELU(approximate='none')\n    (4): ConvTranspose2d(16, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n  )\n)\nInput shape: (1, 1, 7, 7)\nOutput shape: torch.Size([1, 1, 28, 28])\n\n\n\n#Provide an example input to the model\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\ndummy_input = torch.randn(input_size)\n\n# Export the model to ONNX\nonnx_path = \"unet_model.onnx\"\ntorch.onnx.export(model, dummy_input, onnx_path, verbose=False)\n\nprint(\"Model exported to ONNX successfully.\")\n\n============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\nModel exported to ONNX successfully.\n\n\n\n# Input to the model is a batch of 1-channel 7x7 images\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\n\n# Create an instance of the modified UNet model\n\n# Output of the model is a batch of 1-channel 28x28 images\noutput_size = (batch_size, 1, 28, 28)\n\n\n# Input to the model is a batch of 1-channel 7x7 images\nbatch_size = 1\ninput_size = (batch_size, 1, 7, 7)\n\n# Create an instance of the modified UNet model\n\n# Output of the model is a batch of 1-channel 28x28 images\noutput_size = (batch_size, 1, 28, 28)\n\n\n# Create X_train, Y_train, X_test, Y_test\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nX_train = mnist_small.float().to(device)\nY_train = mnist_large.float().to(device)\n\nX_test = mnist_test_small.float().to(device)\nY_test = mnist_test_large.float().to(device)\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n\nmodel = UNet(activation=sin_activation).to(device)\n\n\n# Define the loss function\nloss_fn = nn.MSELoss()\n\n# Define the optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n\n# Number of epochs\n# n_epochs = 5001\nn_epochs = 500\n\n# List to store losses\nlosses = []\n\n# Loop over epochs\nfor epoch in range(n_epochs):\n    # Forward pass\n    Y_pred = model(X_train)\n\n    # Compute Loss\n    loss = loss_fn(Y_pred, Y_train)\n\n    # Print loss\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch+1} loss: {loss.item()}\")\n\n    # Store loss\n    losses.append(loss.item())\n\n    # Zero the gradients\n    optimizer.zero_grad()\n\n    # Backpropagation\n    loss.backward()\n\n    # Update the weights\n    optimizer.step()\n\nEpoch 1 loss: 0.15383368730545044\nEpoch 101 loss: 0.05964525789022446\nEpoch 201 loss: 0.04472550377249718\nEpoch 301 loss: 0.03792179003357887\nEpoch 401 loss: 0.034905895590782166\n\n\n\n# Plot the losses\nplt.plot(losses)\n\n\n\n\n\n# Extract a mini-batch of 10 images\nX_mini = X_train[:10]\nY_mini = Y_train[:10]\n\n# Forward pass\nY_hat = model(X_mini)\n\n# Move the tensors to CPU\nX_mini = X_mini.cpu()\nY_mini = Y_mini.cpu()\nY_hat = Y_hat.cpu()\n\ndef plot_images(X_mini, Y_mini, Y_hat=None):\n\n    # Plot 3 rows\n    rows = 3\n\n    # 10 images X 3 \n    # First row: 10 images from the mini-batch\n    # Second row: 10 ground truth images\n    # Third row: 10 predicted images\n\n    fig, ax = plt.subplots(rows, 10, figsize=(20, 6))\n\n    for i in range(rows):\n        for j in range(10):\n            if i == 0:\n                ax[i][j].imshow(X_mini[j].squeeze(), cmap=\"gray\")\n            elif i == 1:\n                ax[i][j].imshow(Y_mini[j].squeeze(), cmap=\"gray\")\n            else:\n                ax[i][j].imshow(Y_hat[j].detach().squeeze(), cmap=\"gray\")\n\n            ax[i][j].axis(\"off\")\n\n    # Put labels for the three rows using suptitle()\n    fig.suptitle(\"MNIST Image Generation using U-Net\", fontsize=16)\n\n    ax[0][0].set_title(\"Input Images\")\n    ax[1][0].set_title(\"Ground Truth Images\")\n    ax[2][0].set_title(\"Predicted Images\")\n\nplot_images(X_mini, Y_mini, Y_hat)\n\n\n\n\n\n# Get unseen images from the test set\nX_test = mnist_test_small.float().to(device)\nY_test = mnist_test_large.float().to(device)\n\n# Forward pass\nY_hat = model(X_test)\n\nplot_images(X_test.cpu(), Y_test.cpu(), Y_hat.cpu())"
  },
  {
    "objectID": "demo_notebooks/ISR_GAN_Mnist.html",
    "href": "demo_notebooks/ISR_GAN_Mnist.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "from keras.datasets import mnist\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport glob\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, UpSampling2D\nfrom tensorflow.keras.layers import LeakyReLU, Dropout\nfrom tensorflow.keras.models import Sequential, Model, load_model\nimport time\n\n\n\n# a function to format display the losses\ndef hmsString(sec_elapsed):\n    h = int(sec_elapsed / (60 * 60))\n    m = int((sec_elapsed % (60 * 60)) / 60)\n    s = sec_elapsed % 60\n    return \"{}:{:&gt;02}:{:&gt;05.2f}\".format(h, m, s)\n\n# downsample and introduce noise in the images\ndef downSampleAndNoisyfi(X):\n    shape = X[0].shape\n    X_down = []\n    for x_i in X:\n       x_c = cv2.resize(x_i, (shape[0]//4, shape[1]//4), interpolation = cv2.INTER_AREA)\n       x_c = np.clip(x_c+ np.random.normal(0, 5, x_c.shape) , 0, 255).astype('uint8')\n       X_down.append(x_c)\n    X_down = np.array(X_down, dtype = 'uint8')\n    return X_down\n\n\n\n################# CODE FOR GENERATOR BLOCK\ndef Generator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])  \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)  \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    X = Add()([X_shortcut, X])\n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)   \n    X = Add()([X_shortcut, X])\n    X = Activation('relu')(X)\n    X = UpSampling2D(size=2)(X)\n    \n    X_shortcut = X\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 1, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.5)(X)\n    X = Activation('relu')(X)\n    \n    generator_model = Model(inputs=X_input, outputs=X)\n    return generator_model\n\n\n\n################# CODE FOR DISCRIMINATOR BLOCK\ndef Discriminator(input_shape):\n    X_input = Input(input_shape)\n    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X_input)\n    X = Activation('relu')(X)\n    \n    X = Conv2D(filters = 64, kernel_size = (3, 3), strides = (1, 1), padding = 'same')(X)\n    X = BatchNormalization(momentum=0.8)(X)\n    X = Activation('relu')(X)\n    \n    discriminator_model = Model(inputs=X_input, outputs=X)\n    return discriminator_model\n\n\n\n# One step of the test step\n@tf.function\ndef train_step(X, Y, generator, discriminator, generator_optimizer, discriminator_optimizer):\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = generator(X, training=True)\n\n    real_output = discriminator(Y, training=True)\n    fake_output = discriminator(generated_images, training=False)\n\n    gen_loss = tf.keras.losses.MSE(Y, generated_images)\n    disc_loss = tf.keras.losses.MSE(real_output, fake_output)\n    \n\n    gradients_of_generator = gen_tape.gradient(\\\n        gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(\\\n        disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(\n        gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(\n        gradients_of_discriminator, \n        discriminator.trainable_variables))\n  return gen_loss,disc_loss\n\n# The main function to train the GAN\ndef train(X_train, Y_train, generator, discriminator, batch_size=100, epochs=50):\n    generator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    discriminator_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    start = time.time()\n    for epoch in range(epochs):\n        epoch_start = time.time()\n        gen_loss_list = []\n        disc_loss_list = []\n        \n        prev_i = 0\n        for i in range(X_train.shape[0]):\n            if((i+1)%batch_size == 0):\n                t = train_step(X_train[prev_i:i+1], Y_train[prev_i:i+1], generator, discriminator, generator_optimizer, discriminator_optimizer)\n                gen_loss_list.append(t[0])\n                disc_loss_list.append(t[1])\n                prev_i = i+1\n        g_loss = np.sum(np.array(gen_loss_list)) / np.sum(np.array(gen_loss_list).shape)\n        d_loss = np.sum(np.array(disc_loss_list)) / np.sum(np.array(disc_loss_list).shape)\n        \n        epoch_elapsed = time.time()-epoch_start\n        print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {hmsString(epoch_elapsed)}')\n        \n    elapsed = time.time()-start\n    print (f'Training time: {hmsString(elapsed)}')\n    \n\n\n    \n# loading the dataset(the original image are the HR 28*28 images)\n(Y_train, _), (Y_test, _) = mnist.load_data()\n# downsampling and introducing gaussian noise\n# this downsampled and noised dataset is out X or inputs\nX_train = downSampleAndNoisyfi(Y_train)\nX_test = downSampleAndNoisyfi(Y_test)\n\n# introduce a new dimension to the data (None, 28, 28, 1)\nX_test = X_test[..., np.newaxis]\nX_train = X_train[..., np.newaxis]\nY_train = Y_train[..., np.newaxis]\nY_test = Y_test[..., np.newaxis]\n\n# Creating a generator and discriminator model\ngenerator = Generator((7,7,1))\ndiscriminator = Discriminator((28,28,1))\n\n# Showing the summary of generator and discriminator\ngenerator.summary()\ndiscriminator.summary()\n# training with batch size of 100 and for 50 epochs\ntrain(X_train, Y_train, generator, discriminator, 100, 5)#50)\n\n# save the generator model for future use\ngenerator.save(\"mnist_generator_model\")\ngenerator.save(\"mnist_generator_model.h5\")\n\n\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_5 (InputLayer)        [(None, 7, 7, 1)]            0         []                            \n                                                                                                  \n conv2d_20 (Conv2D)          (None, 7, 7, 32)             320       ['input_5[0][0]']             \n                                                                                                  \n batch_normalization_18 (Ba  (None, 7, 7, 32)             128       ['conv2d_20[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_24 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_18[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_21 (Conv2D)          (None, 7, 7, 32)             9248      ['activation_24[0][0]']       \n                                                                                                  \n batch_normalization_19 (Ba  (None, 7, 7, 32)             128       ['conv2d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_25 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_19[0][0]\n                                                                    ']                            \n                                                                                                  \n add_8 (Add)                 (None, 7, 7, 32)             0         ['activation_24[0][0]',       \n                                                                     'activation_25[0][0]']       \n                                                                                                  \n conv2d_22 (Conv2D)          (None, 7, 7, 32)             9248      ['add_8[0][0]']               \n                                                                                                  \n batch_normalization_20 (Ba  (None, 7, 7, 32)             128       ['conv2d_22[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_26 (Activation)  (None, 7, 7, 32)             0         ['batch_normalization_20[0][0]\n                                                                    ']                            \n                                                                                                  \n add_9 (Add)                 (None, 7, 7, 32)             0         ['add_8[0][0]',               \n                                                                     'activation_26[0][0]']       \n                                                                                                  \n activation_27 (Activation)  (None, 7, 7, 32)             0         ['add_9[0][0]']               \n                                                                                                  \n up_sampling2d_4 (UpSamplin  (None, 14, 14, 32)           0         ['activation_27[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n conv2d_23 (Conv2D)          (None, 14, 14, 32)           9248      ['up_sampling2d_4[0][0]']     \n                                                                                                  \n batch_normalization_21 (Ba  (None, 14, 14, 32)           128       ['conv2d_23[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_28 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_21[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_24 (Conv2D)          (None, 14, 14, 32)           9248      ['activation_28[0][0]']       \n                                                                                                  \n batch_normalization_22 (Ba  (None, 14, 14, 32)           128       ['conv2d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_29 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_22[0][0]\n                                                                    ']                            \n                                                                                                  \n add_10 (Add)                (None, 14, 14, 32)           0         ['activation_28[0][0]',       \n                                                                     'activation_29[0][0]']       \n                                                                                                  \n conv2d_25 (Conv2D)          (None, 14, 14, 32)           9248      ['add_10[0][0]']              \n                                                                                                  \n batch_normalization_23 (Ba  (None, 14, 14, 32)           128       ['conv2d_25[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_30 (Activation)  (None, 14, 14, 32)           0         ['batch_normalization_23[0][0]\n                                                                    ']                            \n                                                                                                  \n add_11 (Add)                (None, 14, 14, 32)           0         ['add_10[0][0]',              \n                                                                     'activation_30[0][0]']       \n                                                                                                  \n activation_31 (Activation)  (None, 14, 14, 32)           0         ['add_11[0][0]']              \n                                                                                                  \n up_sampling2d_5 (UpSamplin  (None, 28, 28, 32)           0         ['activation_31[0][0]']       \n g2D)                                                                                             \n                                                                                                  \n conv2d_26 (Conv2D)          (None, 28, 28, 32)           9248      ['up_sampling2d_5[0][0]']     \n                                                                                                  \n batch_normalization_24 (Ba  (None, 28, 28, 32)           128       ['conv2d_26[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_32 (Activation)  (None, 28, 28, 32)           0         ['batch_normalization_24[0][0]\n                                                                    ']                            \n                                                                                                  \n conv2d_27 (Conv2D)          (None, 28, 28, 1)            289       ['activation_32[0][0]']       \n                                                                                                  \n batch_normalization_25 (Ba  (None, 28, 28, 1)            4         ['conv2d_27[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_33 (Activation)  (None, 28, 28, 1)            0         ['batch_normalization_25[0][0]\n                                                                    ']                            \n                                                                                                  \n==================================================================================================\nTotal params: 56997 (222.64 KB)\nTrainable params: 56547 (220.89 KB)\nNon-trainable params: 450 (1.76 KB)\n__________________________________________________________________________________________________\nModel: \"model_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_6 (InputLayer)        [(None, 28, 28, 1)]       0         \n                                                                 \n conv2d_28 (Conv2D)          (None, 28, 28, 32)        320       \n                                                                 \n activation_34 (Activation)  (None, 28, 28, 32)        0         \n                                                                 \n conv2d_29 (Conv2D)          (None, 28, 28, 64)        18496     \n                                                                 \n batch_normalization_26 (Ba  (None, 28, 28, 64)        256       \n tchNormalization)                                               \n                                                                 \n activation_35 (Activation)  (None, 28, 28, 64)        0         \n                                                                 \n=================================================================\nTotal params: 19072 (74.50 KB)\nTrainable params: 18944 (74.00 KB)\nNon-trainable params: 128 (512.00 Byte)\n_________________________________________________________________\nEpoch 1, gen loss=444240933.9259259,disc loss=5967.341931216931, 0:06:59.43\nEpoch 2, gen loss=442459325.6296296,disc loss=2971.5945767195767, 0:06:50.58\nEpoch 3, gen loss=441174395.2592593,disc loss=1824.6559193121693, 0:05:31.16\nEpoch 4, gen loss=439941780.994709,disc loss=1088.3287037037037, 0:05:21.85\nEpoch 5, gen loss=438728021.3333333,disc loss=618.3444527116402, 0:05:26.83\nTraining time: 0:30:09.87\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: mnist_generator_model\\assets\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\nINFO:tensorflow:Assets written to: mnist_generator_model\\assets\nC:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n\n\n\n# testing the model\nY_pred = generator.predict(X_test)\n# showing the first 5 results\nfig,a =  plt.subplots(3,5)\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    a[0][i].imshow(X_test[i])\n    a[0][i].axes.get_xaxis().set_visible(False)\n    a[0][i].axes.get_yaxis().set_visible(False)\n    a[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    a[1][i].imshow(Y_pred[i])\n    a[1][i].axes.get_xaxis().set_visible(False)\n    a[1][i].axes.get_yaxis().set_visible(False)\n    a[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    a[2][i].imshow(Y_test[i])\n    a[2][i].axes.get_xaxis().set_visible(False)\n    a[2][i].axes.get_yaxis().set_visible(False)\n    a[2][i].title.set_text(\"HR: \"+str(i+1)) \n    \n\n313/313 [==============================] - 9s 27ms/step\n\n\n\n\n\n\n# showing the first 5 random results\nimport random\nfigb,ab =  plt.subplots(3,5)\nfigb.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(5):\n    ii = random.randint(0, 10000) \n    \n    ab[0][i].imshow(X_test[ii])\n    ab[0][i].axes.get_xaxis().set_visible(False)\n    ab[0][i].axes.get_yaxis().set_visible(False)\n    ab[0][i].title.set_text(\"LR: \"+str(i+1))\n    \n    ab[1][i].imshow(Y_pred[ii])\n    ab[1][i].axes.get_xaxis().set_visible(False)\n    ab[1][i].axes.get_yaxis().set_visible(False)\n    ab[1][i].title.set_text(\"SR: \"+str(i+1)) \n    \n    ab[2][i].imshow(Y_test[ii])\n    ab[2][i].axes.get_xaxis().set_visible(False)\n    ab[2][i].axes.get_yaxis().set_visible(False)\n    ab[2][i].title.set_text(\"HR: \"+str(i+1))"
  },
  {
    "objectID": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "href": "demo_notebooks/mml_book_latex_symbols.ipynb.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Table of Symbols from the book Mathematics for Machine Learning: https://mml-book.github.io/. Latex was provided by the co-author Cheng Soon Ong (Many Thanks) and edited by Harry Wang: https://github.com/mml-book/mml-book.github.io/issues/634\nSee latex version on overleaf.com: https://www.overleaf.com/read/mnzgdyrsjfsk\n$\n% vector bf: boldface\n% matrix % transpose % inverse\n% set cal: calligraphic letters % dimension, rm: roman typestyle % rank\n% determinant % identity mapping % kernel/nullspace % image\n% generating set\n% tensor % trace\n% lagrangian % likelihood % variance % expectation % covariance\n% given % Gaussian distribution\n% other distributions $\n\n\n\n\n\n\n\nSymbol                             \nTypical Meaning\n\n\n\n\n\\(a,b,c, \\alpha,\\beta,\\gamma\\)\nScalars are lowercase\n\n\n\\(\\mathbf{x},\\mathbf{y},\\mathbf{z}\\)\nVectors are bold lowercase\n\n\n\\(\\mathbf{A},\\mathbf{B},\\mathbf{C}\\)\nMatrices are bold uppercase\n\n\n\\(\\mathbf{x} ^\\top, \\mathbf{A} ^\\top\\)\nTranspose of a vector or matrix\n\n\n\\(\\mathbf{A}^{-1}\\)\nInverse of a matrix\n\n\n\\(\\langle \\mathbf{x}, \\mathbf{y}\\rangle\\)\nInner product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(\\mathbf{x} ^\\top\\mathbf{y}\\)\nDot product of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\)\n\n\n\\(B = (\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3)\\)\n(Ordered) tuple\n\n\n\\(\\mathbf{B} = [\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3]\\)\nMatrix of column vectors stacked horizontally\n\n\n\\(\\mathcal{B} = \\{\\mathbf{b}_1, \\mathbf{b}_2, \\mathbf{b}_3\\}\\)\nSet of vectors (unordered)\n\n\n\\(\\mathbb Z,\\mathbb N\\)\nIntegers and natural numbers, respectively\n\n\n\\(\\mathbb R,\\mathbb C\\)\nReal and complex numbers, respectively\n\n\n\\(\\mathbb R^n\\)\n\\(n\\)-dimensional vector space of real numbers\n\n\n\\(\\forall x\\)\nUniversal quantifier: for all \\(x\\)\n\n\n\\(\\exists x\\)\nExistential quantifier: there exists \\(x\\)\n\n\n\\(a := b\\)\n\\(a\\) is defined as \\(b\\)\n\n\n\\(a =:b\\)\n\\(b\\) is defined as \\(a\\)\n\n\n\\(a\\propto b\\)\n\\(a\\) is proportional to \\(b\\), i.e., \\(a =\\text\\{constant\\}\\cdot b\\)\n\n\n\\(g\\circ f\\)\nFunction composition: \\(g\\) after \\(f\\)\n\n\n\\(\\iff\\)\nIf and only if\n\n\n\\(\\implies\\)\nImplies\n\n\n\\(\\mathcal{A}, \\mathcal{C}\\)\nSets\n\n\n\\(a \\in \\mathcal{A}\\)\n\\(a\\) is an element of set \\(\\mathcal{A}\\)\n\n\n\\(\\emptyset\\)\nEmpty set\n\n\n\\(\\mathcal{A}\\setminus \\mathcal{B}\\)\n\\(\\mathcal{A}\\) without \\(\\mathcal{B}\\): the set of elements in \\(\\mathcal{A}\\) but not in \\(\\mathcal{B}\\)\n\n\n\\(D\\)\nNumber of dimensions; indexed by \\(d=1,\\dots,D\\)\n\n\n\\(N\\)\nNumber of data points; indexed by \\(n=1,\\dots,N\\)\n\n\n\\(\\mathbf{I}_m\\)\nIdentity matrix of size \\(m\\times m\\)\n\n\n\\(\\mathbf{0}_{m,n}\\)\nMatrix of zeros of size \\(m\\times n\\)\n\n\n\\(\\mathbf{1}_{m,n}\\)\nMatrix of ones of size \\(m\\times n\\)\n\n\n\\(\\mathbf{e}_i\\)\nStandardcanonical vector (where \\(i\\) is the component that is \\(1\\))\n\n\n\\(\\mathrm{dim}\\)\nDimensionality of vector space\n\n\n\\(\\mathrm{rk}(\\mathbf{A})\\)\nRank of matrix \\(\\mathbf{A}\\)\n\n\n\\(\\mathrm{Im}(\\Phi)\\)\nImage of linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{ker}(\\Phi)\\)\nKernel (null space) of a linear mapping \\(\\Phi\\)\n\n\n\\(\\mathrm{span}[\\mathbf{b}_1]\\)\nSpan (generating set) of \\(\\mathbf{b}_1\\)\n\n\n\\(\\text{tr}(\\mathbf{A})\\)\nTrace of \\(\\mathbf{A}\\)\n\n\n\\(\\det(\\mathbf{A})\\)\nDeterminant of \\(\\mathbf{A}\\)\n\n\n\\(| \\cdot |\\)\nAbsolute value or determinant (depending on context)\n\n\n\\(\\| {\\cdot} \\|\\)\nNorm; Euclidean, unless specified\n\n\n\\(\\lambda\\)\nEigenvalue or Lagrange multiplier\n\n\n\\(E_\\lambda\\)\nEigenspace corresponding to eigenvalue \\(\\lambda\\)\n\n\n\\(\\mathbf{x} \\perp \\mathbf{y}\\)\nVectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are orthogonal\n\n\n\\(V\\)\nVector space\n\n\n\\(V^\\perp\\)\nOrthogonal complement of vector space \\(V\\)\n\n\n\\(\\sum_{n=1}^N x_n\\)\nSum of the \\(x_n\\): \\(x_1 + \\dotsc + x_N\\)\n\n\n\\(\\prod_{n=1}^N x_n\\)\nProduct of the \\(x_n\\): \\(x_1 \\cdot\\dotsc \\cdot x_N\\)\n\n\n\\(\\mathbf{\\theta}\\)\nParameter vector\n\n\n\\(\\frac{\\partial f}{\\partial x}\\)\nPartial derivative of \\(f\\) with respect to \\(x\\)\n\n\n\\(\\frac{\\mathrm{d}f}{\\mathrm{d}x}\\)\nTotal derivative of \\(f\\) with respect to \\(x\\)\n\n\n$$\nGradient\n\n\n\\(f_* = \\min_x f(x)\\)\nThe smallest function value of \\(f\\)\n\n\n\\(x_* \\in \\arg\\min_x f(x)\\)\nThe value \\(x_*\\) that minimizes \\(f\\) (note: \\(\\arg\\min\\) returns a set of values)\n\n\n\\(\\mathfrak{L}\\)\nLagrangian\n\n\n\\(\\mathcal{L}\\)\nNegative log-likelihood\n\n\n\\(\\binom{n}{k}\\)\nBinomial coefficient, \\(n\\) choose \\(k\\)\n\n\n\\(\\mathbb{V}_X[\\mathbf{x}]\\)\nVariance of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathbb{E}_X[\\mathbf{x}]\\)\nExpectation of \\(\\mathbf{x}\\) with respect to the random variable \\(X\\)\n\n\n\\(\\mathop{\\mathrm{Cov}}_{X,Y}[\\mathbf{x}, \\mathbf{y}]\\)\nCovariance between \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).\n\n\n\\(X \\perp\\kern-5pt \\perp Y\\vert Z\\)\n\\(X\\) is conditionally independent of \\(Y\\) given \\(Z\\)\n\n\n\\(X\\sim p\\)\nRandom variable \\(X\\) is distributed according to \\(p\\)\n\n\n\\(\\mathcal{N}\\big(\\mathbf{\\mu},\\mathbf{\\Sigma}\\big)\\)\nGaussian distribution with mean \\(\\mathbf{\\mu}\\) and covariance \\(\\mathbf{\\Sigma}\\)\n\n\n\\(\\text{Ber}(\\mu)\\)\nBernoulli distribution with parameter \\(\\mu\\)\n\n\n\\(\\text{Bin}(N, \\mu)\\)\nBinomial distribution with parameters \\(N, \\mu\\)\n\n\n\\(\\text{Beta}(\\alpha, \\beta)\\)\nBeta distribution with parameters \\(\\alpha, \\beta\\)\n\n\n\n\nθ\nyn\nσ\nxn\nn = 1, . . . , N\n\n\n$ L(\\theta, σ | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, σ) $ \n\nSyntaxError: invalid syntax (2183852464.py, line 1)\n\n\n\nfrom re import L\n\n\nL(\\theta, \\sigma | x_n, y_n) = \\prod_{n=1}^N p(y_n | x_n, \\theta, \\sigma)"
  },
  {
    "objectID": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "href": "demo_notebooks/sequential-bayesian-learning.ipynb.html",
    "title": "Bayesian Learning in a Linear Basis Function Model",
    "section": "",
    "text": "In this notebook we ilustrate the bayesian learning in a linear basis function model, as well as the sequential update of a posterior distribution.\nTaken from Christopher Bishop’s Pattern Recognition and Machine Learning book (p.155)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal, norm\nfrom numpy.random import seed, uniform, randn\nfrom numpy.linalg import inv\n\n\n%config InlineBackend.figure_format = \"retina\"\n\nWe consider an input \\(x\\), a target variable \\(t\\) and a linear model of the form \\[\n    y(x, {\\bf w}) = w_0 + w_1x\n\\]\n\ndef f(x, a): return a[0] + a[1] * x\n\n\nseed(314)\na = np.array([-0.3, 0.5])\nN = 30\nsigma = 0.2\nX = uniform(-1, 1, (N, 1))\nT = f(X, a) + randn(N, 1) * sigma\n\n\n# plot_sample_w(0, )\n\n\nplt.scatter(X, T, c=\"crimson\")\nplt.grid(alpha=0.5)\nplt.plot(X, f(X,a))\n\n\n\n\nOur goal is to recover the values \\(w_0\\) and \\(w_1\\) from the data.\nRecall: \\[\n    p({\\textbf w}|t) \\propto p(t| {\\textbf w}, \\beta) p(\\bf{w})\n\\]\n\nbeta = (1 / sigma) ** 2 # precision\nalpha = 2.0\n\n\n\\(t \\sim \\mathcal{N}\\left({\\bf w}^Tx, \\beta^{-1}\\right)\\) (The assigned probability for target variables)\n\nThe posterior distribution of \\(\\bf w\\) after \\(N\\) observations is given by\n\\[\n\\begin{align}\n    m_N &= S_N(S_0^{-1}m_0 + \\beta\\Phi^T{\\bf t}) \\\\\n    S_N^{-1} &= S_0^{-1} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\nWith * \\(\\Phi\\in\\mathbb{R}^{N\\times M}\\) * \\({\\bf t}\\in\\mathbb{R}^N\\)\nIf no data has been yet seen, we consider \\[\nw \\sim \\mathcal{N}\\left(0, \\alpha^{-1}\\text{I}\\right)\n\\] Which results in a posterior distribution of the form\n\\[\n\\begin{align}\n    m_N &= \\beta S_N\\Phi^T{\\bf t} \\\\\n    S_N^{-1} &= \\alpha \\text{I} + \\beta\\Phi^T\\Phi\n\\end{align}\n\\]\n\ndef posterior_w(phi, t, S0, m0):\n    \"\"\"\n    Compute the posterior distribution of \n    a Gaussian with known precision and conjugate\n    prior a gaussian\n    \n    Parameters\n    ----------\n    phi: np.array(N, M)\n    t: np.array(N, 1)\n    S0: np.array(M, M)\n        The prior covariance matrix\n    m0: np.array(M, 1)\n        The prior mean vector\n    \"\"\"\n    SN = inv(inv(S0) + beta * Phi.T @ Phi)\n    mN = SN @ (inv(S0) @ m0 + beta * Phi.T @ t)\n    return SN, mN\n\ndef sample_vals(X, T, ix):\n    \"\"\"\n    \n    Returns\n    -------\n    Phi: The linear model transormation\n    t: the target datapoint\n    \"\"\"\n    x_in = X[ix]\n    Phi = np.c_[np.ones_like(x_in), x_in] #  concatenate arrays along the second axis\n    t = T[[ix]]\n    return Phi, t\n\ndef plot_prior(m, S, liminf=-1, limsup=1, step=0.05, ax=plt, **kwargs):\n    grid = np.mgrid[liminf:limsup + step:step, liminf:limsup + step:step]\n    nx = grid.shape[-1]\n    z = multivariate_normal.pdf(grid.T.reshape(-1, 2), mean=m.ravel(), cov=S).reshape(nx, nx).T\n    \n    return ax.contourf(*grid, z, **kwargs)\n\ndef plot_sample_w(mean, cov, size=10, ax=plt):\n    w = np.random.multivariate_normal(mean=mean.ravel(), cov=cov, size=size)\n    x = np.linspace(-1, 1)\n    for wi in w:\n        ax.plot(x, f(x, wi), c=\"tab:blue\", alpha=0.4)\n        \ndef plot_likelihood_obs(X, T, ix, ax=plt):\n    \"\"\"\n    Plot the likelihood function of a single observation\n    \"\"\"\n    W = np.mgrid[-1:1:0.1, -1:1:0.1]\n    x, t = sample_vals(X, T, ix) # ith row\n    mean = W.T.reshape(-1, 2) @ x.T\n\n    likelihood = norm.pdf(t, loc=mean, scale= np.sqrt(1 / beta)).reshape(20, 20).T\n    ax.contourf(*W, likelihood)\n    ax.scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n\n\nSN = np.eye(2) / alpha\nmN = np.zeros((2, 1))\n\nseed(1643)\nN = 20\nnobs = [1, 2, 20]\nix_fig = 1\nfig, ax = plt.subplots(len(nobs) + 1, 3, figsize=(10, 12))\nplot_prior(mN, SN, ax=ax[0,1])\nax[0, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\nax[0, 0].axis(\"off\")\nplot_sample_w(mN, SN, ax=ax[0, 2])\nfor i in range(0, N+1):\n    Phi, t = sample_vals(X, T, i)\n    SN, mN = posterior_w(Phi, t, SN, mN)\n    if i+1 in nobs:\n        plot_likelihood_obs(X, T, i, ax=ax[ix_fig, 0])\n        plot_prior(mN, SN, ax=ax[ix_fig, 1])\n        ax[ix_fig, 1].scatter(-0.3, 0.5, c=\"white\", marker=\"+\")\n        ax[ix_fig, 2].scatter(X[:i + 1], T[:i + 1], c=\"crimson\")\n        ax[ix_fig, 2].set_xlim(-1, 1)\n        ax[ix_fig, 2].set_ylim(-1, 1)\n        for l in range(2):\n            ax[ix_fig, l].set_xlabel(\"$w_0$\")\n            ax[ix_fig, l].set_ylabel(\"$w_1$\")\n        plot_sample_w(mN, SN, ax=ax[ix_fig, 2])\n        ix_fig += 1\n\ntitles = [\"likelihood\", \"prior/posterior\", \"data space\"]\nfor axi, title in zip(ax[0], titles):\n    axi.set_title(title, size=15)\nplt.tight_layout()\n\n\n\n\nIn the limit of an infinite number of datapoints, the posterior distribution would become a delta function centered on the true parameter values, shown by the white cross.\nOther forms of prior over parameters can be considered. For example, the generalized Gaussian:\n\\[\n    p({\\bf w}|\\alpha) = \\left[\\frac{q}{2}\\left(\\frac{q}{2}\\right)^{1/q}\\frac{1}{\\Gamma(1/q)}\\right]^M \\exp\\left(-\\frac{\\alpha}{2}\\sum_{j=1}^M|w_j|^q\\right)\n\\]\nFinding the maximum posterior distribution over \\({\\bf w}\\) corresponds to minimization of the regularized error function given by:\n\\[\n    \\frac{1}{2}\\sum_{n=1}^N\\left(t_n - {\\bf w}^T {\\bf\\phi}({\\bf x}_n)\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_j|^q\n\\]\nThe maximum posterior weight vector \\({\\bf w}_{\\text{MAP}}\\) considering a generalized Gaussian will not be (in every case) the mean, since the mean will not coincide with the mode."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\nXtest.shape\n\n(100, 1)\n\n\n\ntheta_map\n\narray([[-1.26518298],\n       [-0.01298677]])\n\n\n\nPhi.shape\n\n(20, 2)\n\n\n\n# get the MAP estimate\nK = 1 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\ntheta_aug_ml\n\narray([[0.116],\n       [0.499]])\n\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\nprint(ml_prediction.shape)\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n(100, 1)\n\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\nXtest.shape\n\n(100, 1)\n\n\n\ntheta_map\n\narray([[-1.26518298],\n       [-0.01298677]])\n\n\n\nPhi.shape\n\n(20, 2)\n\n\n\n# get the MAP estimate\nK = 1 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "demo_notebooks/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Welcome to my blog! I’m Suraj Jaiswal, an M.Tech student at IIT Gandhinagar, pursuing Computer Science and Engineering under the mentorship of Dr. Nipun Batra.  Here, I’ll share my experiences, research projects, and insights as we delve into the exciting world of technology. IIT Gandhinagar, known for its excellence in education and research, provides the ideal environment for me to explore the limitless possibilities of computer science."
  },
  {
    "objectID": "publications_and_projects/data_folder/demo.html",
    "href": "publications_and_projects/data_folder/demo.html",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "",
    "text": "print('hi')\n\nhi"
  },
  {
    "objectID": "publications_and_projects/index.html",
    "href": "publications_and_projects/index.html",
    "title": "Publications and Projects",
    "section": "",
    "text": "Image-to-Image for Climate Modelling using Auto-Encoders\n\n\n\n\n\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nHypernet and Neural Processes on CelebA\n\n\n\n\n\n\n\nDeep Learning\n\n\nMeta Learning\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\n  \n\n\n\n\nTowards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning\n\n\n\n\n\n\n\nActive Learning\n\n\nDeep Learning\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nSuraj Jaiswal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html",
    "href": "blogs/blogsData/Autoencoder.html",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "",
    "text": "Go throught the link to see the presentation of the project to get the flow.\nGit Repo: link\n\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch \nimport os\nimport torch.utils.data as data\nimport torch.nn as nn\nimport glob \n# from torchsummary import summary\nfrom torchinfo import summary\nfrom tqdm import trange\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cuda:2\")\nprint(device)\ncurrent_device = device #torch.cuda.current_device()\ndevice_name = torch.cuda.get_device_name(current_device)\nprint(f\"Current GPU assigned: {current_device}, Name: {device_name}\")\n\ncuda\nCurrent GPU assigned: cuda, Name: Quadro RTX 5000\n\n\nConsider it a prediction task, use any channel like pressure and/or wind speed inputs to start with, because they are the indicators of horizontal movement of air pollution. Multiple inputs can go as multiple channels of images similar to RGB. Output can be P25 or P10 in cam 120hr files.\n\ndef get_latitudes():\n    lat_start= 76.8499984741211\n    lat_step=0.009999999776482582\n\n    latitudes=[]\n\n    for i in range(80):\n        latitudes.append(lat_start+i*lat_step)\n    \n    latitudes.reverse()\n\n    return latitudes\n\ndef get_longitudes():\n    long_start= 28.200000762939453\n    long_step=0.009999999776482582\n\n    longitudes=[]\n\n    for i in range(80):\n        longitudes.append(long_start+i*long_step)\n    \n    # longitudes.reverse()\n\n    return longitudes\n\nlatitudes=get_latitudes()\nlongitudes=get_longitudes()\n\ndef create_plot(data,hour,var_name):\n    # print(data[var_name].shape) #shape (120, 1, 80, 80)\n    p10_hour=data[var_name]['TSTEP'==hour] # shape (1, 80, 80)\n    p10_hour=p10_hour[0,:,:] # shape (80, 80)\n    plt.imshow(p10_hour)\n    plt.title(f'{var_name} at hour '+str(hour))\n    # plt.colorbar()\n\n    # only show every latitude and longitude of end points\n    # round to 2 decimal places\n    top=latitudes[0]\n    top=round(top,2) \n    bottom=latitudes[-1]\n    bottom=round(bottom,2)\n    left=longitudes[0]\n    left=round(left,2)\n    right=longitudes[-1]\n    right=round(right,2)\n\n    plt.xticks([0,79],[left,right])\n    plt.xlabel('Longitude')\n    plt.yticks([0,79],[top,bottom])\n    plt.ylabel('Latitude')\n    # plt.savefig(f'plots/120/{var_name}_{day}.png')\n    # plt.close()"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#visualizing-96-hr-files",
    "href": "blogs/blogsData/Autoencoder.html#visualizing-96-hr-files",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Visualizing 96 hr files",
    "text": "Visualizing 96 hr files\n\ndata_96 = xr.open_dataset('data/camxmet2d.delhi.20230717.96hours.nc')\ndata_96_df = data_96.to_dataframe().reset_index()\n\n\ndata_96\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (TSTEP: 96, VAR: 14, DATE-TIME: 2, LAY: 1, ROW: 80, COL: 80)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables: (12/15)\n    TFLAG       (TSTEP, VAR, DATE-TIME) int32 2023198 0 ... 2023201 230000\n    TSURF_K     (TSTEP, LAY, ROW, COL) float32 302.3 302.3 302.3 ... 300.6 301.2\n    SNOWEW_M    (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SNOWAGE_HR  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PRATE_MMpH  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CLOUD_OD    (TSTEP, LAY, ROW, COL) float32 62.24 61.67 61.1 ... 37.1 36.78\n    ...          ...\n    SWSFC_WpM2  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SOLM_M3pM3  (TSTEP, LAY, ROW, COL) float32 0.3131 0.3114 ... 0.3278 0.3292\n    CLDTOP_KM   (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CAPE        (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PBL_WRF_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 94.4 120.0\n    PBL_YSU_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 64.43 94.71\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023198\n    CTIME:          73941\n    WDATE:          2023198\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 96VAR: 14DATE-TIME: 2LAY: 1ROW: 80COL: 80Coordinates: (0)Data variables: (15)TFLAG(TSTEP, VAR, DATE-TIME)int322023198 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023198,       0],\n        [2023198,       0],\n        ...,\n        [2023198,       0],\n        [2023198,       0]],\n\n       [[2023198,   10000],\n        [2023198,   10000],\n        ...,\n        [2023198,   10000],\n        [2023198,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)TSURF_K(TSTEP, LAY, ROW, COL)float32302.3 302.3 302.3 ... 300.6 301.2long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                array([[[[302.31464, ..., 301.9288 ],\n         ...,\n         [301.28806, ..., 301.65118]]],\n\n\n       ...,\n\n\n       [[[299.001  , ..., 299.86826],\n         ...,\n         [299.08127, ..., 301.1575 ]]]], dtype=float32)SNOWEW_M(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SNOWAGE_HR(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PRATE_MMpH(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CLOUD_OD(TSTEP, LAY, ROW, COL)float3262.24 61.67 61.1 ... 37.1 36.78long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               array([[[[62.239326, ..., 30.003962],\n         ...,\n         [54.13987 , ..., 43.78004 ]]],\n\n\n       ...,\n\n\n       [[[29.166183, ..., 58.07322 ],\n         ...,\n         [32.563763, ..., 36.776646]]]], dtype=float32)U10_MpS(TSTEP, LAY, ROW, COL)float321.63 1.585 1.54 ... 0.8064 0.8805long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)V10_MpS(TSTEP, LAY, ROW, COL)float320.1487 0.158 ... -1.202 -1.152long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                array([[[[ 0.148722, ...,  0.630212],\n         ...,\n         [-0.328182, ...,  0.625002]]],\n\n\n       ...,\n\n\n       [[[ 0.040896, ..., -1.492097],\n         ...,\n         [-0.365465, ..., -1.152316]]]], dtype=float32)T2_K(TSTEP, LAY, ROW, COL)float32302.6 302.6 302.6 ... 300.4 300.8long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   array([[[[302.57184, ..., 302.88318],\n         ...,\n         [301.1836 , ..., 302.19128]]],\n\n\n       ...,\n\n\n       [[[299.17767, ..., 300.1615 ],\n         ...,\n         [299.30466, ..., 300.77878]]]], dtype=float32)SWSFC_WpM2(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SOLM_M3pM3(TSTEP, LAY, ROW, COL)float320.3131 0.3114 ... 0.3278 0.3292long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             array([[[[0.313128, ..., 0.271955],\n         ...,\n         [0.306425, ..., 0.287092]]],\n\n\n       ...,\n\n\n       [[[0.312972, ..., 0.263079],\n         ...,\n         [0.29241 , ..., 0.32918 ]]]], dtype=float32)CLDTOP_KM(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CAPE(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PBL_WRF_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 94.4 120.0long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              array([[[[ 17.212452, ...,  17.253763],\n         ...,\n         [ 17.153704, ...,  17.189987]]],\n\n\n       ...,\n\n\n       [[[ 33.101303, ...,  30.475212],\n         ...,\n         [ 24.450672, ..., 120.012794]]]], dtype=float32)PBL_YSU_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 64.43 94.71long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              array([[[[17.212452, ..., 17.253763],\n         ...,\n         [17.153704, ..., 17.189987]]],\n\n\n       ...,\n\n\n       [[[17.060207, ..., 17.152937],\n         ...,\n         [17.07223 , ..., 94.71051 ]]]], dtype=float32)Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023198CTIME :73941WDATE :2023198WTIME :73941SDATE :2023198STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\ndata_96_df\n\n\n\n\n\n\n\n\nTSTEP\nVAR\nDATE-TIME\nLAY\nROW\nCOL\nTFLAG\nTSURF_K\nSNOWEW_M\nSNOWAGE_HR\n...\nCLOUD_OD\nU10_MpS\nV10_MpS\nT2_K\nSWSFC_WpM2\nSOLM_M3pM3\nCLDTOP_KM\nCAPE\nPBL_WRF_M\nPBL_YSU_M\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n2023198\n302.314636\n0.0\n0.0\n...\n62.239326\n1.630373\n0.148722\n302.571838\n0.0\n0.313128\n0.0\n0.0\n17.212452\n17.212452\n\n\n1\n0\n0\n0\n0\n0\n1\n2023198\n302.312042\n0.0\n0.0\n...\n61.671093\n1.585281\n0.158011\n302.584351\n0.0\n0.311416\n0.0\n0.0\n17.212652\n17.212652\n\n\n2\n0\n0\n0\n0\n0\n2\n2023198\n302.309479\n0.0\n0.0\n...\n61.102859\n1.540188\n0.167299\n302.596832\n0.0\n0.309703\n0.0\n0.0\n17.212852\n17.212852\n\n\n3\n0\n0\n0\n0\n0\n3\n2023198\n302.306885\n0.0\n0.0\n...\n60.534630\n1.495095\n0.176587\n302.609344\n0.0\n0.307990\n0.0\n0.0\n17.213055\n17.213055\n\n\n4\n0\n0\n0\n0\n0\n4\n2023198\n302.303558\n0.0\n0.0\n...\n59.912636\n1.450304\n0.184773\n302.621948\n0.0\n0.306286\n0.0\n0.0\n17.213356\n17.213356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17203195\n95\n13\n1\n0\n79\n75\n230000\n298.979095\n0.0\n0.0\n...\n37.972748\n0.600130\n-1.341342\n299.389465\n0.0\n0.324286\n0.0\n0.0\n26.661556\n17.082989\n\n\n17203196\n95\n13\n1\n0\n79\n76\n230000\n299.372223\n0.0\n0.0\n...\n37.747482\n0.658128\n-1.301898\n299.638062\n0.0\n0.324927\n0.0\n0.0\n43.123440\n17.094244\n\n\n17203197\n95\n13\n1\n0\n79\n77\n230000\n299.967468\n0.0\n0.0\n...\n37.423786\n0.732263\n-1.252025\n300.018402\n0.0\n0.326345\n0.0\n0.0\n68.759804\n17.112419\n\n\n17203198\n95\n13\n1\n0\n79\n78\n230000\n300.562744\n0.0\n0.0\n...\n37.100090\n0.806398\n-1.202152\n300.398743\n0.0\n0.327763\n0.0\n0.0\n94.396065\n64.426094\n\n\n17203199\n95\n13\n1\n0\n79\n79\n230000\n301.157501\n0.0\n0.0\n...\n36.776646\n0.880477\n-1.152316\n300.778778\n0.0\n0.329180\n0.0\n0.0\n120.012794\n94.710510\n\n\n\n\n17203200 rows × 21 columns\n\n\n\n\ndata_96_df.columns\n\nIndex(['TSTEP', 'VAR', 'DATE-TIME', 'LAY', 'ROW', 'COL', 'TFLAG', 'TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M'],\n      dtype='object')\n\n\n\ndata_96['U10_MpS'] #shape (96, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'U10_MpS' (TSTEP: 96, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  U10_MpS         \n    units:      ppmV            \n    var_desc:   VARIABLE U10_MpS                                             ...xarray.DataArray'U10_MpS'TSTEP: 96LAY: 1ROW: 80COL: 801.63 1.585 1.54 1.495 1.45 ... 0.6001 0.6581 0.7323 0.8064 0.8805array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                \n\n\n\ndata_96['U10_MpS'].shape , data_96['U10_MpS']['TSTEP'==0].shape, data_96['U10_MpS']['TSTEP'==0][0].shape\n\n((96, 1, 80, 80), (1, 80, 80), (80, 80))\n\n\n\nplt.imshow(data_96['U10_MpS']['TSTEP'==0][0]) #shape (80, 80)\n\n&lt;matplotlib.image.AxesImage at 0x7f76abc7cf70&gt;\n\n\n\n\n\n\ncreate_plot(data = data_96, hour = 1, var_name = 'U10_MpS')"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#visualizing-120-hr-files",
    "href": "blogs/blogsData/Autoencoder.html#visualizing-120-hr-files",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Visualizing 120 hr files",
    "text": "Visualizing 120 hr files\n\ndata_120 = xr.open_dataset('data/camx120hr_merged_20230717.nc')\ndata_120_df = data_120.to_dataframe().reset_index() \n\n\ndata_120 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 120, LAY: 1, ROW: 80, COL: 80, VAR: 9, DATE-TIME: 2)\nDimensions without coordinates: TSTEP, LAY, ROW, COL, VAR, DATE-TIME\nData variables:\n    P10      (TSTEP, LAY, ROW, COL) float32 23.86 23.86 24.07 ... 10.1 10.1\n    P25      (TSTEP, LAY, ROW, COL) float32 19.24 19.24 19.62 ... 9.63 9.63\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 2023197 0 2023197 ... 2023201 230000\nAttributes: (12/34)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023197\n    CTIME:          83911\n    WDATE:          2023197\n    ...             ...\n    GDNAM:          ????????????????\n    UPNAM:          CAMXMETOU       \n    VAR-LIST:       P10             P25             \n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc ca...\n    NCO:            netCDF Operators version 4.9.1 (Homepage = http://nco.sf....xarray.DatasetDimensions:TSTEP: 120LAY: 1ROW: 80COL: 80VAR: 9DATE-TIME: 2Coordinates: (0)Data variables: (3)P10(TSTEP, LAY, ROW, COL)float3223.86 23.86 24.07 ... 10.1 10.1long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)P25(TSTEP, LAY, ROW, COL)float3219.24 19.24 19.62 ... 9.63 9.63long_name :FPRM            units :micrograms/m**3 var_desc :VARIABLE FPRM                                                                   array([[[[19.240587, ..., 26.00578 ],\n         ...,\n         [31.570122, ..., 17.510971]]],\n\n\n       ...,\n\n\n       [[[22.36091 , ..., 16.678066],\n         ...,\n         [18.63693 , ...,  9.629862]]]], dtype=float32)TFLAG(TSTEP, VAR, DATE-TIME)int322023197 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023197,       0],\n        [2023197,       0],\n        ...,\n        [2023197,       0],\n        [2023197,       0]],\n\n       [[2023197,   10000],\n        [2023197,   10000],\n        ...,\n        [2023197,   10000],\n        [2023197,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)Indexes: (0)Attributes: (34)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023197CTIME :83911WDATE :2023197WTIME :83911SDATE :2023197STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :2GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMXMETOU       VAR-LIST :P10             P25             FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc camxout.2023.07.17.nc camxout.2023.07.18.nc camxout.2023.07.19.nc camxout.2023.07.20.nc camx120hr.nc\nNCO :netCDF Operators version 4.9.1 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n\n\n\ndata_120_df\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n23.857107\n19.240587\n2023197\n\n\n1\n0\n0\n0\n0\n0\n1\n23.857107\n19.240587\n0\n\n\n2\n0\n0\n0\n0\n1\n0\n23.857107\n19.240587\n2023197\n\n\n3\n0\n0\n0\n0\n1\n1\n23.857107\n19.240587\n0\n\n\n4\n0\n0\n0\n0\n2\n0\n23.857107\n19.240587\n2023197\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13823995\n119\n0\n79\n79\n6\n1\n10.097546\n9.629862\n230000\n\n\n13823996\n119\n0\n79\n79\n7\n0\n10.097546\n9.629862\n2023201\n\n\n13823997\n119\n0\n79\n79\n7\n1\n10.097546\n9.629862\n230000\n\n\n13823998\n119\n0\n79\n79\n8\n0\n10.097546\n9.629862\n2023201\n\n\n13823999\n119\n0\n79\n79\n8\n1\n10.097546\n9.629862\n230000\n\n\n\n\n13824000 rows × 9 columns\n\n\n\n\ndata_120_df.describe()\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\ncount\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n\n\nmean\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n4.101470e+01\n2.856279e+01\n1.069100e+06\n\n\nstd\n3.463981e+01\n0.0\n2.309221e+01\n2.309221e+01\n2.581989e+00\n0.5\n2.361775e+01\n1.582030e+01\n9.553543e+05\n\n\nmin\n0.000000e+00\n0.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.0\n1.111396e+00\n8.114247e-01\n0.000000e+00\n\n\n25%\n2.975000e+01\n0.0\n1.975000e+01\n1.975000e+01\n2.000000e+00\n0.0\n2.546643e+01\n1.880392e+01\n1.175000e+05\n\n\n50%\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n3.590978e+01\n2.589731e+01\n1.126598e+06\n\n\n75%\n8.925000e+01\n0.0\n5.925000e+01\n5.925000e+01\n6.000000e+00\n1.0\n5.103806e+01\n3.433007e+01\n2.023199e+06\n\n\nmax\n1.190000e+02\n0.0\n7.900000e+01\n7.900000e+01\n8.000000e+00\n1.0\n6.492913e+02\n6.130998e+02\n2.023201e+06\n\n\n\n\n\n\n\n\ndata_120['COL']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'COL' (COL: 80)&gt;\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])\nDimensions without coordinates: COLxarray.DataArray'COL'COL: 800 1 2 3 4 5 6 7 8 9 10 11 12 ... 68 69 70 71 72 73 74 75 76 77 78 79array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\n\ndata_120['P10'] #shape (120, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (TSTEP: 120, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'TSTEP: 120LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 9.551 9.664 9.808 10.1 10.1array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1]# shape 1x80x80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)\nDimensions without coordinates: LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 18.47 18.77 18.69 18.51 18.51array([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1][0] # shape 80x80\nplt.imshow(data_120['P25']['TSTEP'==1][0])#, vmin=0, vmax=100)\n\n&lt;matplotlib.image.AxesImage at 0x7f190d573e80&gt;\n\n\n\n\n\n\ncreate_plot(data_120,1,'P25')"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#model-defination",
    "href": "blogs/blogsData/Autoencoder.html#model-defination",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Autoencoder_MLP(nn.Module):\n    def __init__(self):\n        super(Autoencoder_MLP, self).__init__()\n        \n        # Define the layers\n        self.flatten = nn.Flatten()  # Flatten the 2D input matrix\n        self.fc1 = nn.Linear(80*80, 1024)  # Fully connected layer 1\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 1024)  # Fully connected layer 2\n        self.fc4 = nn.Linear(1024, 80*80)  # Fully connected layer 2\n        self.relu = nn.ReLU()  # Activation function\n\n    def forward(self, x):\n        # Flatten the input\n        x = self.flatten(x)\n        \n        # Forward pass through the fully connected layers\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.fc4(x)\n\n        # Reshape the output to match the 2D matrix size\n        x = x.view(-1, 80, 80)\n        \n        return x\n\n\nsummary(Autoencoder_MLP(), input_size=(1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_MLP                          [1, 80, 80]               --\n├─Flatten: 1-1                           [1, 6400]                 --\n├─Linear: 1-2                            [1, 1024]                 6,554,624\n├─ReLU: 1-3                              [1, 1024]                 --\n├─Linear: 1-4                            [1, 512]                  524,800\n├─ReLU: 1-5                              [1, 512]                  --\n├─Linear: 1-6                            [1, 1024]                 525,312\n├─ReLU: 1-7                              [1, 1024]                 --\n├─Linear: 1-8                            [1, 6400]                 6,560,000\n==========================================================================================\nTotal params: 14,164,736\nTrainable params: 14,164,736\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 14.16\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 0.07\nParams size (MB): 56.66\nEstimated Total Size (MB): 56.76\n=========================================================================================="
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-single-channel-input-and-outputp25",
    "href": "blogs/blogsData/Autoencoder.html#training-single-channel-input-and-outputp25",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\nX.shape, y.shape\n\n((1656, 14, 80, 80), (1656, 2, 80, 80))\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25 = []\ny_channel = 0 \n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 405.4315\nEpoch [21/200] Loss: 214.5978\nEpoch [41/200] Loss: 215.0236\nEpoch [61/200] Loss: 207.2482\nEpoch [81/200] Loss: 206.7534\nEpoch [101/200] Loss: 213.1840\nEpoch [121/200] Loss: 207.5252\nEpoch [141/200] Loss: 208.2888\nEpoch [161/200] Loss: 209.6494\nEpoch [181/200] Loss: 209.7600\nAverage Test Loss: 190.0971\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 464.9160\nEpoch [21/200] Loss: 206.1055\nEpoch [41/200] Loss: 210.0196\nEpoch [61/200] Loss: 209.2211\nEpoch [81/200] Loss: 211.8091\nEpoch [101/200] Loss: 205.4592\nEpoch [121/200] Loss: 203.9590\nEpoch [141/200] Loss: 208.5030\nEpoch [161/200] Loss: 207.1320\nEpoch [181/200] Loss: 204.3017\nAverage Test Loss: 189.7174\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 460.0334\nEpoch [21/200] Loss: 207.1043\nEpoch [41/200] Loss: 206.5882\nEpoch [61/200] Loss: 208.3192\nEpoch [81/200] Loss: 205.4035\nEpoch [101/200] Loss: 209.7349\nEpoch [121/200] Loss: 205.5515\nEpoch [141/200] Loss: 206.1994\nEpoch [161/200] Loss: 205.4001\nEpoch [181/200] Loss: 206.3801\nAverage Test Loss: 190.0263\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 476.9623\nEpoch [21/200] Loss: 207.3108\nEpoch [41/200] Loss: 208.4855\nEpoch [61/200] Loss: 206.4735\nEpoch [81/200] Loss: 205.5895\nEpoch [101/200] Loss: 205.6536\nEpoch [121/200] Loss: 205.9680\nEpoch [141/200] Loss: 205.7129\nEpoch [161/200] Loss: 208.6799\nEpoch [181/200] Loss: 205.6486\nAverage Test Loss: 189.6717\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 599.1686\nEpoch [21/200] Loss: 454.3787\nEpoch [41/200] Loss: 149.9270\nEpoch [61/200] Loss: 23.7072\nEpoch [81/200] Loss: 7.4556\nEpoch [101/200] Loss: 18.4050\nEpoch [121/200] Loss: 3.0840\nEpoch [141/200] Loss: 100.6344\nEpoch [161/200] Loss: 25.9631\nEpoch [181/200] Loss: 38.8335\nAverage Test Loss: 9.8902\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 373.7542\nEpoch [21/200] Loss: 51.2008\nEpoch [41/200] Loss: 37.7057\nEpoch [61/200] Loss: 25.3702\nEpoch [81/200] Loss: 0.2791\nEpoch [101/200] Loss: 20.1649\nEpoch [121/200] Loss: 0.2297\nEpoch [141/200] Loss: 84.8078\nEpoch [161/200] Loss: 2.4612\nEpoch [181/200] Loss: 1.4404\nAverage Test Loss: 0.7598\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 399.2307\nEpoch [21/200] Loss: 1.1263\nEpoch [41/200] Loss: 0.0137\nEpoch [61/200] Loss: 0.0820\nEpoch [81/200] Loss: 2.3733\nEpoch [101/200] Loss: 0.0002\nEpoch [121/200] Loss: 0.0113\nEpoch [141/200] Loss: 0.0393\nEpoch [161/200] Loss: 11.0771\nEpoch [181/200] Loss: 0.0003\nAverage Test Loss: 0.0507\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 412.2657\nEpoch [21/200] Loss: 210.9129\nEpoch [41/200] Loss: 209.2686\nEpoch [61/200] Loss: 206.7048\nEpoch [81/200] Loss: 206.0187\nEpoch [101/200] Loss: 209.1361\nEpoch [121/200] Loss: 212.1444\nEpoch [141/200] Loss: 206.8286\nEpoch [161/200] Loss: 205.8083\nEpoch [181/200] Loss: 208.7421\nAverage Test Loss: 190.9819\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 485.4561\nEpoch [21/200] Loss: 210.1752\nEpoch [41/200] Loss: 207.2874\nEpoch [61/200] Loss: 207.9026\nEpoch [81/200] Loss: 206.2404\nEpoch [101/200] Loss: 205.9271\nEpoch [121/200] Loss: 205.6481\nEpoch [141/200] Loss: 208.2400\nEpoch [161/200] Loss: 205.4081\nEpoch [181/200] Loss: 205.1859\nAverage Test Loss: 199.1508\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 305.6402\nEpoch [21/200] Loss: 176.6024\nEpoch [41/200] Loss: 161.6851\nEpoch [61/200] Loss: 157.7673\nEpoch [81/200] Loss: 138.7664\nEpoch [101/200] Loss: 132.1217\nEpoch [121/200] Loss: 109.4900\nEpoch [141/200] Loss: 104.2382\nEpoch [161/200] Loss: 69.5860\nEpoch [181/200] Loss: 50.0248\nAverage Test Loss: 42.1182\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 461.0668\nEpoch [21/200] Loss: 205.2268\nEpoch [41/200] Loss: 213.0740\nEpoch [61/200] Loss: 206.9702\nEpoch [81/200] Loss: 205.7769\nEpoch [101/200] Loss: 210.3533\nEpoch [121/200] Loss: 206.2578\nEpoch [141/200] Loss: 207.2727\nEpoch [161/200] Loss: 205.3765\nEpoch [181/200] Loss: 206.2520\nAverage Test Loss: 192.2221\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 468.8439\nEpoch [21/200] Loss: 207.2851\nEpoch [41/200] Loss: 211.7429\nEpoch [61/200] Loss: 205.8536\nEpoch [81/200] Loss: 205.3124\nEpoch [101/200] Loss: 207.8155\nEpoch [121/200] Loss: 208.3488\nEpoch [141/200] Loss: 206.0120\nEpoch [161/200] Loss: 205.9442\nEpoch [181/200] Loss: 204.1215\nAverage Test Loss: 190.8858\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 287.4021\nEpoch [21/200] Loss: 209.5118\nEpoch [41/200] Loss: 209.2537\nEpoch [61/200] Loss: 206.0375\nEpoch [81/200] Loss: 207.1711\nEpoch [101/200] Loss: 205.2734\nEpoch [121/200] Loss: 210.4286\nEpoch [141/200] Loss: 207.5243\nEpoch [161/200] Loss: 208.9856\nEpoch [181/200] Loss: 204.6973\nAverage Test Loss: 191.9669\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 323.2034\nEpoch [21/200] Loss: 207.3925\nEpoch [41/200] Loss: 210.0555\nEpoch [61/200] Loss: 206.0877\nEpoch [81/200] Loss: 207.9315\nEpoch [101/200] Loss: 207.4798\nEpoch [121/200] Loss: 206.1072\nEpoch [141/200] Loss: 207.4259\nEpoch [161/200] Loss: 209.3318\nEpoch [181/200] Loss: 207.3006\nAverage Test Loss: 194.3565\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25\n\n[190.09705144708806,\n 189.7173545143821,\n 190.02632834694603,\n 189.67173489657316,\n 9.890190774744207,\n 0.7598354017192667,\n 0.050707115029746834,\n 190.98193498091265,\n 199.15079567649147,\n 42.118152445012875,\n 192.22213883833453,\n 190.88581431995738,\n 191.9668634588068,\n 194.3564910888672]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 0.050707115029746834)\n('U10_MpS', 0.7598354017192667)\n('CLOUD_OD', 9.890190774744207)\n('SOLM_M3pM3', 42.118152445012875)\n('PRATE_MMpH', 189.67173489657316)\n('SNOWEW_M', 189.7173545143821)\n('SNOWAGE_HR', 190.02632834694603)\n('TSURF_K', 190.09705144708806)\n('CAPE', 190.88581431995738)\n('T2_K', 190.98193498091265)\n('PBL_WRF_M', 191.9668634588068)\n('CLDTOP_KM', 192.22213883833453)\n('PBL_YSU_M', 194.3564910888672)\n('SWSFC_WpM2', 199.15079567649147)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'CLOUD_OD']"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "href": "blogs/blogsData/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\ntest_loss_list_P_10 = []\ny_channel = 1\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 689.7490\nEpoch [21/200] Loss: 418.6676\nEpoch [41/200] Loss: 410.9685\nEpoch [61/200] Loss: 404.8066\nEpoch [81/200] Loss: 400.5351\nEpoch [101/200] Loss: 401.0250\nEpoch [121/200] Loss: 401.0659\nEpoch [141/200] Loss: 403.6432\nEpoch [161/200] Loss: 402.2360\nEpoch [181/200] Loss: 407.2921\nAverage Test Loss: 401.8882\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1031.9999\nEpoch [21/200] Loss: 405.7656\nEpoch [41/200] Loss: 402.2811\nEpoch [61/200] Loss: 403.8337\nEpoch [81/200] Loss: 398.2591\nEpoch [101/200] Loss: 409.4181\nEpoch [121/200] Loss: 396.3737\nEpoch [141/200] Loss: 395.1861\nEpoch [161/200] Loss: 403.0066\nEpoch [181/200] Loss: 397.6094\nAverage Test Loss: 361.2384\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1050.1397\nEpoch [21/200] Loss: 397.1798\nEpoch [41/200] Loss: 401.0453\nEpoch [61/200] Loss: 401.7171\nEpoch [81/200] Loss: 400.2689\nEpoch [101/200] Loss: 396.7101\nEpoch [121/200] Loss: 404.5816\nEpoch [141/200] Loss: 397.7231\nEpoch [161/200] Loss: 394.6489\nEpoch [181/200] Loss: 399.8899\nAverage Test Loss: 362.0641\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1034.3143\nEpoch [21/200] Loss: 399.8861\nEpoch [41/200] Loss: 399.7876\nEpoch [61/200] Loss: 395.9112\nEpoch [81/200] Loss: 400.4803\nEpoch [101/200] Loss: 396.5552\nEpoch [121/200] Loss: 401.0467\nEpoch [141/200] Loss: 406.4987\nEpoch [161/200] Loss: 401.3759\nEpoch [181/200] Loss: 397.4671\nAverage Test Loss: 361.1263\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1214.4916\nEpoch [21/200] Loss: 725.7131\nEpoch [41/200] Loss: 220.5272\nEpoch [61/200] Loss: 23.5934\nEpoch [81/200] Loss: 8.8063\nEpoch [101/200] Loss: 274.7881\nEpoch [121/200] Loss: 218.0335\nEpoch [141/200] Loss: 232.4360\nEpoch [161/200] Loss: 212.6520\nEpoch [181/200] Loss: 195.4785\nAverage Test Loss: 180.2198\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 822.4214\nEpoch [21/200] Loss: 182.0370\nEpoch [41/200] Loss: 50.0221\nEpoch [61/200] Loss: 60.5883\nEpoch [81/200] Loss: 37.8001\nEpoch [101/200] Loss: 2.6386\nEpoch [121/200] Loss: 0.0629\nEpoch [141/200] Loss: 76.5106\nEpoch [161/200] Loss: 41.2225\nEpoch [181/200] Loss: 4.6194\nAverage Test Loss: 0.6711\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1160.5402\nEpoch [21/200] Loss: 8.7026\nEpoch [41/200] Loss: 0.3045\nEpoch [61/200] Loss: 0.1493\nEpoch [81/200] Loss: 0.1235\nEpoch [101/200] Loss: 5.4404\nEpoch [121/200] Loss: 0.0000\nEpoch [141/200] Loss: 10.2387\nEpoch [161/200] Loss: 0.2644\nEpoch [181/200] Loss: 0.0284\nAverage Test Loss: 0.6849\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 646.6076\nEpoch [21/200] Loss: 412.2161\nEpoch [41/200] Loss: 407.6458\nEpoch [61/200] Loss: 403.7684\nEpoch [81/200] Loss: 399.9570\nEpoch [101/200] Loss: 397.7386\nEpoch [121/200] Loss: 402.0990\nEpoch [141/200] Loss: 397.6674\nEpoch [161/200] Loss: 398.1612\nEpoch [181/200] Loss: 403.3343\nAverage Test Loss: 360.5232\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1027.1539\nEpoch [21/200] Loss: 403.9408\nEpoch [41/200] Loss: 401.9444\nEpoch [61/200] Loss: 399.7967\nEpoch [81/200] Loss: 401.7070\nEpoch [101/200] Loss: 404.1586\nEpoch [121/200] Loss: 398.2909\nEpoch [141/200] Loss: 396.0758\nEpoch [161/200] Loss: 396.4020\nEpoch [181/200] Loss: 402.5728\nAverage Test Loss: 362.6420\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 696.3634\nEpoch [21/200] Loss: 331.1351\nEpoch [41/200] Loss: 296.5541\nEpoch [61/200] Loss: 301.9112\nEpoch [81/200] Loss: 272.0759\nEpoch [101/200] Loss: 251.3878\nEpoch [121/200] Loss: 232.4279\nEpoch [141/200] Loss: 170.0628\nEpoch [161/200] Loss: 138.8842\nEpoch [181/200] Loss: 108.4864\nAverage Test Loss: 110.6638\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1017.6373\nEpoch [21/200] Loss: 400.4651\nEpoch [41/200] Loss: 405.2223\nEpoch [61/200] Loss: 402.9269\nEpoch [81/200] Loss: 400.7290\nEpoch [101/200] Loss: 400.5154\nEpoch [121/200] Loss: 404.1065\nEpoch [141/200] Loss: 403.8573\nEpoch [161/200] Loss: 397.0463\nEpoch [181/200] Loss: 398.1819\nAverage Test Loss: 361.3055\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1029.8392\nEpoch [21/200] Loss: 401.8889\nEpoch [41/200] Loss: 399.1993\nEpoch [61/200] Loss: 400.2419\nEpoch [81/200] Loss: 396.8849\nEpoch [101/200] Loss: 395.3639\nEpoch [121/200] Loss: 401.5739\nEpoch [141/200] Loss: 403.1417\nEpoch [161/200] Loss: 402.3704\nEpoch [181/200] Loss: 400.0658\nAverage Test Loss: 361.0597\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 536.0374\nEpoch [21/200] Loss: 401.9832\nEpoch [41/200] Loss: 402.4697\nEpoch [61/200] Loss: 404.1890\nEpoch [81/200] Loss: 400.4949\nEpoch [101/200] Loss: 399.0872\nEpoch [121/200] Loss: 398.4192\nEpoch [141/200] Loss: 400.4667\nEpoch [161/200] Loss: 398.1382\nEpoch [181/200] Loss: 408.3430\nAverage Test Loss: 360.3921\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 538.3320\nEpoch [21/200] Loss: 405.8272\nEpoch [41/200] Loss: 430.2409\nEpoch [61/200] Loss: 398.9199\nEpoch [81/200] Loss: 404.8572\nEpoch [101/200] Loss: 397.2876\nEpoch [121/200] Loss: 403.3052\nEpoch [141/200] Loss: 401.7307\nEpoch [161/200] Loss: 396.1157\nEpoch [181/200] Loss: 399.0870\nAverage Test Loss: 361.4502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10\n\n[401.88824185458094,\n 361.23838112571025,\n 362.0641340775923,\n 361.1262997713956,\n 180.2197820490057,\n 0.6710819737477736,\n 0.6848962740464644,\n 360.5232141668146,\n 362.6420177112926,\n 110.6638252951882,\n 361.3054504394531,\n 361.05967018821025,\n 360.39211342551494,\n 361.45016063343394]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 0.6710819737477736)\n('V10_MpS', 0.6848962740464644)\n('SOLM_M3pM3', 110.6638252951882)\n('CLOUD_OD', 180.2197820490057)\n('PBL_WRF_M', 360.39211342551494)\n('T2_K', 360.5232141668146)\n('CAPE', 361.05967018821025)\n('PRATE_MMpH', 361.1262997713956)\n('SNOWEW_M', 361.23838112571025)\n('CLDTOP_KM', 361.3054504394531)\n('PBL_YSU_M', 361.45016063343394)\n('SNOWAGE_HR', 362.0641340775923)\n('SWSFC_WpM2', 362.6420177112926)\n('TSURF_K', 401.88824185458094)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#insights",
    "href": "blogs/blogsData/Autoencoder.html#insights",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10, test_loss_list_P_25)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10, 'P25': test_loss_list_P_25, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_MLP_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n0.7\n0.1\n0.4\n\n\n5\nU10_MpS\n0.7\n0.8\n0.7\n\n\n9\nSOLM_M3pM3\n110.7\n42.1\n76.4\n\n\n4\nCLOUD_OD\n180.2\n9.9\n95.1\n\n\n3\nPRATE_MMpH\n361.1\n189.7\n275.4\n\n\n1\nSNOWEW_M\n361.2\n189.7\n275.5\n\n\n7\nT2_K\n360.5\n191.0\n275.8\n\n\n11\nCAPE\n361.1\n190.9\n276.0\n\n\n2\nSNOWAGE_HR\n362.1\n190.0\n276.0\n\n\n12\nPBL_WRF_M\n360.4\n192.0\n276.2\n\n\n10\nCLDTOP_KM\n361.3\n192.2\n276.8\n\n\n13\nPBL_YSU_M\n361.5\n194.4\n277.9\n\n\n8\nSWSFC_WpM2\n362.6\n199.2\n280.9\n\n\n0\nTSURF_K\n401.9\n190.1\n296.0"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#model-defination-1",
    "href": "blogs/blogsData/Autoencoder.html#model-defination-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__() \n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid \n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential( \n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        ) \n        \n    def forward(self, x):\n        return self.net(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n\n        )\n    def forward(self, x):\n        return self.decoder(x)\n\nclass Autoencoder_CNN(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_CNN, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim \n        self.activation = activation    \n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        \n    \n    def forward(self, x):\n        x = self.encoder(x)\n        # print(x.shape)\n        x = self.decoder(x)\n        return x\n\n\nmodel_auto = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_CNN(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 1024]              --\n│    └─Sequential: 2-1                   [1656, 1024]              --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 6400]       6,560,000\n        Unflatten-15           [-1, 64, 10, 10]               0\n  ConvTranspose2d-16           [-1, 32, 20, 20]          18,464\n             GELU-17           [-1, 32, 20, 20]               0\n           Conv2d-18           [-1, 32, 20, 20]           9,248\n             GELU-19           [-1, 32, 20, 20]               0\n  ConvTranspose2d-20           [-1, 16, 40, 40]           4,624\n             GELU-21           [-1, 16, 40, 40]               0\n           Conv2d-22           [-1, 16, 40, 40]           2,320\n             GELU-23           [-1, 16, 40, 40]               0\n  ConvTranspose2d-24            [-1, 2, 80, 80]             290\n          Decoder-25            [-1, 2, 80, 80]               0\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.80\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.44\n----------------------------------------------------------------"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "href": "blogs/blogsData/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "training on all channel and predicting on all channel",
    "text": "training on all channel and predicting on all channel\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 50 #200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 10 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/50] Loss: 1620.3361\nAverage Test Loss: 377.9475\nEpoch [11/50] Loss: 249.2443\nAverage Test Loss: 228.7919\nEpoch [21/50] Loss: 168.6332\nAverage Test Loss: 172.4825\nEpoch [31/50] Loss: 102.0034\nAverage Test Loss: 111.1758\nEpoch [41/50] Loss: 65.1520\nAverage Test Loss: 60.7862\n\n\n  2%|▏         | 1/50 [00:04&lt;04:03,  4.96s/it] 22%|██▏       | 11/50 [00:58&lt;03:15,  5.00s/it] 42%|████▏     | 21/50 [01:33&lt;01:40,  3.45s/it] 62%|██████▏   | 31/50 [02:21&lt;01:14,  3.90s/it] 80%|████████  | 40/50 [02:49&lt;00:32,  3.22s/it] 82%|████████▏ | 41/50 [02:53&lt;00:30,  3.39s/it]100%|██████████| 50/50 [03:30&lt;00:00,  4.22s/it]\n\n\n\n\n\n\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nAverage Test Loss: 20.4597\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in14_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "href": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output (P25)",
    "text": "Single channel input single channel output (P25)\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto = []\ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 462.4725\nEpoch [21/200] Loss: 212.0043\nEpoch [41/200] Loss: 213.3173\nEpoch [61/200] Loss: 234.0997\nEpoch [81/200] Loss: 211.7991\nEpoch [101/200] Loss: 216.9750\nEpoch [121/200] Loss: 211.1435\nEpoch [141/200] Loss: 214.2518\nEpoch [161/200] Loss: 209.3667\nEpoch [181/200] Loss: 208.2786\nAverage Test Loss: 196.8076\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 540.4606\nEpoch [21/200] Loss: 216.1673\nEpoch [41/200] Loss: 211.7699\nEpoch [61/200] Loss: 211.9133\nEpoch [81/200] Loss: 214.4790\nEpoch [101/200] Loss: 212.5737\nEpoch [121/200] Loss: 212.8330\nEpoch [141/200] Loss: 209.0824\nEpoch [161/200] Loss: 207.7827\nEpoch [181/200] Loss: 208.2993\nAverage Test Loss: 191.6095\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 590.1837\nEpoch [21/200] Loss: 211.4096\nEpoch [41/200] Loss: 213.1281\nEpoch [61/200] Loss: 208.5330\nEpoch [81/200] Loss: 209.0068\nEpoch [101/200] Loss: 211.9243\nEpoch [121/200] Loss: 212.2428\nEpoch [141/200] Loss: 210.3188\nEpoch [161/200] Loss: 211.6509\nEpoch [181/200] Loss: 212.8352\nAverage Test Loss: 191.7548\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 750.4891\nEpoch [21/200] Loss: 213.9414\nEpoch [41/200] Loss: 214.3703\nEpoch [61/200] Loss: 209.9882\nEpoch [81/200] Loss: 210.4570\nEpoch [101/200] Loss: 211.3801\nEpoch [121/200] Loss: 214.6792\nEpoch [141/200] Loss: 210.8055\nEpoch [161/200] Loss: 214.0414\nEpoch [181/200] Loss: 206.5140\nAverage Test Loss: 194.9402\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 563.3871\nEpoch [21/200] Loss: 63.2729\nEpoch [41/200] Loss: 23.8580\nEpoch [61/200] Loss: 15.2393\nEpoch [81/200] Loss: 11.0129\nEpoch [101/200] Loss: 9.3018\nEpoch [121/200] Loss: 8.1090\nEpoch [141/200] Loss: 7.4336\nEpoch [161/200] Loss: 8.8601\nEpoch [181/200] Loss: 6.1420\nAverage Test Loss: 7.4988\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 684.8967\nEpoch [21/200] Loss: 111.6166\nEpoch [41/200] Loss: 22.6558\nEpoch [61/200] Loss: 10.9635\nEpoch [81/200] Loss: 12.9050\nEpoch [101/200] Loss: 6.2923\nEpoch [121/200] Loss: 9.0327\nEpoch [141/200] Loss: 4.1764\nEpoch [161/200] Loss: 3.8160\nEpoch [181/200] Loss: 3.3957\nAverage Test Loss: 3.1739\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 609.8626\nEpoch [21/200] Loss: 58.6357\nEpoch [41/200] Loss: 14.3107\nEpoch [61/200] Loss: 10.6218\nEpoch [81/200] Loss: 6.9871\nEpoch [101/200] Loss: 5.3444\nEpoch [121/200] Loss: 4.6127\nEpoch [141/200] Loss: 4.0642\nEpoch [161/200] Loss: 3.5636\nEpoch [181/200] Loss: 3.2190\nAverage Test Loss: 2.9050\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 458.4550\nEpoch [21/200] Loss: 213.3441\nEpoch [41/200] Loss: 211.7652\nEpoch [61/200] Loss: 212.4970\nEpoch [81/200] Loss: 210.7104\nEpoch [101/200] Loss: 208.7177\nEpoch [121/200] Loss: 212.2414\nEpoch [141/200] Loss: 207.2175\nEpoch [161/200] Loss: 211.3121\nEpoch [181/200] Loss: 208.7926\nAverage Test Loss: 191.9081\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 760.0155\nEpoch [21/200] Loss: 224.7744\nEpoch [41/200] Loss: 215.4670\nEpoch [61/200] Loss: 214.6593\nEpoch [81/200] Loss: 211.3864\nEpoch [101/200] Loss: 214.5846\nEpoch [121/200] Loss: 215.2816\nEpoch [141/200] Loss: 210.3355\nEpoch [161/200] Loss: 213.1056\nEpoch [181/200] Loss: 209.3009\nAverage Test Loss: 193.2200\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 518.8583\nEpoch [21/200] Loss: 183.2842\nEpoch [41/200] Loss: 106.5493\nEpoch [61/200] Loss: 47.8360\nEpoch [81/200] Loss: 17.4837\nEpoch [101/200] Loss: 12.6859\nEpoch [121/200] Loss: 9.7521\nEpoch [141/200] Loss: 8.4768\nEpoch [161/200] Loss: 7.1218\nEpoch [181/200] Loss: 7.4880\nAverage Test Loss: 5.5514\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 603.6223\nEpoch [21/200] Loss: 216.0129\nEpoch [41/200] Loss: 212.5011\nEpoch [61/200] Loss: 215.2624\nEpoch [81/200] Loss: 210.4688\nEpoch [101/200] Loss: 211.2793\nEpoch [121/200] Loss: 208.5935\nEpoch [141/200] Loss: 209.1620\nEpoch [161/200] Loss: 208.9249\nEpoch [181/200] Loss: 207.8753\nAverage Test Loss: 205.4745\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 480.7086\nEpoch [21/200] Loss: 215.4105\nEpoch [41/200] Loss: 211.5271\nEpoch [61/200] Loss: 211.8129\nEpoch [81/200] Loss: 208.5153\nEpoch [101/200] Loss: 208.6479\nEpoch [121/200] Loss: 215.5420\nEpoch [141/200] Loss: 207.9001\nEpoch [161/200] Loss: 206.7139\nEpoch [181/200] Loss: 208.8671\nAverage Test Loss: 191.5815\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 487.6684\nEpoch [21/200] Loss: 210.6665\nEpoch [41/200] Loss: 211.8588\nEpoch [61/200] Loss: 209.3883\nEpoch [81/200] Loss: 210.8873\nEpoch [101/200] Loss: 209.3165\nEpoch [121/200] Loss: 208.2383\nEpoch [141/200] Loss: 208.4831\nEpoch [161/200] Loss: 209.2067\nEpoch [181/200] Loss: 207.0104\nAverage Test Loss: 190.8408\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 538.0652\nEpoch [21/200] Loss: 213.2395\nEpoch [41/200] Loss: 209.5626\nEpoch [61/200] Loss: 218.8836\nEpoch [81/200] Loss: 213.1157\nEpoch [101/200] Loss: 211.7407\nEpoch [121/200] Loss: 214.4044\nEpoch [141/200] Loss: 209.6131\nEpoch [161/200] Loss: 209.1906\nEpoch [181/200] Loss: 207.2547\nAverage Test Loss: 196.5285\n\n\n  0%|          | 1/200 [00:00&lt;02:14,  1.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:24&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.58it/s] 50%|█████     | 101/200 [00:28&lt;00:27,  3.61it/s] 60%|██████    | 121/200 [00:34&lt;00:21,  3.60it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.48it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.43it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.47it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.37it/s] 60%|██████    | 121/200 [00:35&lt;00:21,  3.76it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.42it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.61it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.63it/s] 10%|█         | 21/200 [00:05&lt;00:46,  3.82it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:37,  3.14it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.40it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.49it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.52it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.57it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.46it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.34it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.37it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.45it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.52it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.32it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.49it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.42it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.10it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:36&lt;00:23,  3.31it/s] 70%|███████   | 141/200 [00:42&lt;00:17,  3.30it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.46it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.18it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.42it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.33it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.36it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.35it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.37it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.44it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto\n\n[196.80755892666903,\n 191.6094970703125,\n 191.75477183948863,\n 194.94017861106178,\n 7.4987756772474805,\n 3.1738592061129483,\n 2.9050224044106225,\n 191.90811157226562,\n 193.21996238014916,\n 5.551396109841087,\n 205.47452198375356,\n 191.58150828968394,\n 190.84084944291547,\n 196.5285311612216]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 2.9050224044106225)\n('U10_MpS', 3.1738592061129483)\n('SOLM_M3pM3', 5.551396109841087)\n('CLOUD_OD', 7.4987756772474805)\n('PBL_WRF_M', 190.84084944291547)\n('CAPE', 191.58150828968394)\n('SNOWEW_M', 191.6094970703125)\n('SNOWAGE_HR', 191.75477183948863)\n('T2_K', 191.90811157226562)\n('SWSFC_WpM2', 193.21996238014916)\n('PRATE_MMpH', 194.94017861106178)\n('PBL_YSU_M', 196.5285311612216)\n('TSURF_K', 196.80755892666903)\n('CLDTOP_KM', 205.47452198375356)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1000, 1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1000, 1, 80, 80]         --\n├─Encoder: 1-1                           [1000, 512]               --\n│    └─Sequential: 2-1                   [1000, 512]               --\n│    │    └─Conv2d: 3-1                  [1000, 8, 40, 40]         80\n│    │    └─GELU: 3-2                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-3                  [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-4                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-5                  [1000, 16, 20, 20]        1,168\n│    │    └─GELU: 3-6                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-8                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1000, 32, 10, 10]        4,640\n│    │    └─GELU: 3-10                   [1000, 32, 10, 10]        --\n│    │    └─Flatten: 3-11                [1000, 3200]              --\n│    │    └─Linear: 3-12                 [1000, 512]               1,638,912\n├─Decoder: 1-2                           [1000, 1, 80, 80]         --\n│    └─Sequential: 2-2                   [1000, 1, 80, 80]         --\n│    │    └─Linear: 3-13                 [1000, 3200]              1,641,600\n│    │    └─Unflatten: 3-14              [1000, 32, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1000, 16, 20, 20]        4,624\n│    │    └─GELU: 3-16                   [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-18                   [1000, 16, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1000, 8, 40, 40]         1,160\n│    │    └─GELU: 3-20                   [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-21                 [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-22                   [1000, 8, 40, 40]         --\n│    │    └─ConvTranspose2d: 3-23        [1000, 1, 80, 80]         73\n==========================================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 12.24\n==========================================================================================\nInput size (MB): 25.60\nForward/backward pass size (MB): 720.90\nParams size (MB): 13.19\nEstimated Total Size (MB): 759.69\n=========================================================================================="
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "href": "blogs/blogsData/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input multiple channel output (P10)",
    "text": "Single channel input multiple channel output (P10)\n\ntest_loss_list_P_10_conv_auto = []\ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 831.9463\nEpoch [21/200] Loss: 413.6695\nEpoch [41/200] Loss: 416.1795\nEpoch [61/200] Loss: 414.0692\nEpoch [81/200] Loss: 408.8671\nEpoch [101/200] Loss: 408.6212\nEpoch [121/200] Loss: 408.1124\nEpoch [141/200] Loss: 407.2696\nEpoch [161/200] Loss: 419.0648\nEpoch [181/200] Loss: 411.9767\nAverage Test Loss: 367.9784\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1193.0016\nEpoch [21/200] Loss: 413.8656\nEpoch [41/200] Loss: 416.9739\nEpoch [61/200] Loss: 405.7197\nEpoch [81/200] Loss: 414.4462\nEpoch [101/200] Loss: 403.9555\nEpoch [121/200] Loss: 406.6566\nEpoch [141/200] Loss: 404.8177\nEpoch [161/200] Loss: 414.5372\nEpoch [181/200] Loss: 418.9334\nAverage Test Loss: 373.6601\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.4868\nEpoch [21/200] Loss: 410.2950\nEpoch [41/200] Loss: 414.8993\nEpoch [61/200] Loss: 405.8627\nEpoch [81/200] Loss: 416.4907\nEpoch [101/200] Loss: 413.8539\nEpoch [121/200] Loss: 406.4541\nEpoch [141/200] Loss: 404.2276\nEpoch [161/200] Loss: 407.0577\nEpoch [181/200] Loss: 402.6227\nAverage Test Loss: 393.0328\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1028.3382\nEpoch [21/200] Loss: 434.1988\nEpoch [41/200] Loss: 412.9816\nEpoch [61/200] Loss: 405.9959\nEpoch [81/200] Loss: 404.1220\nEpoch [101/200] Loss: 415.6019\nEpoch [121/200] Loss: 403.8031\nEpoch [141/200] Loss: 407.2635\nEpoch [161/200] Loss: 404.3506\nEpoch [181/200] Loss: 409.8351\nAverage Test Loss: 367.3246\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1599.0141\nEpoch [21/200] Loss: 192.7198\nEpoch [41/200] Loss: 68.0225\nEpoch [61/200] Loss: 44.4028\nEpoch [81/200] Loss: 32.4964\nEpoch [101/200] Loss: 25.7723\nEpoch [121/200] Loss: 23.6108\nEpoch [141/200] Loss: 19.3957\nEpoch [161/200] Loss: 19.0788\nEpoch [181/200] Loss: 21.9598\nAverage Test Loss: 18.7717\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1023.2784\nEpoch [21/200] Loss: 280.1561\nEpoch [41/200] Loss: 60.3793\nEpoch [61/200] Loss: 29.8831\nEpoch [81/200] Loss: 19.7393\nEpoch [101/200] Loss: 16.8795\nEpoch [121/200] Loss: 12.9342\nEpoch [141/200] Loss: 11.2801\nEpoch [161/200] Loss: 10.1249\nEpoch [181/200] Loss: 11.3411\nAverage Test Loss: 8.8163\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1439.9822\nEpoch [21/200] Loss: 142.6731\nEpoch [41/200] Loss: 39.2041\nEpoch [61/200] Loss: 20.6722\nEpoch [81/200] Loss: 16.5665\nEpoch [101/200] Loss: 12.4099\nEpoch [121/200] Loss: 10.8300\nEpoch [141/200] Loss: 9.6913\nEpoch [161/200] Loss: 8.9197\nEpoch [181/200] Loss: 10.7642\nAverage Test Loss: 23.4499\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1037.4250\nEpoch [21/200] Loss: 413.2349\nEpoch [41/200] Loss: 411.9822\nEpoch [61/200] Loss: 406.0331\nEpoch [81/200] Loss: 408.5417\nEpoch [101/200] Loss: 403.6246\nEpoch [121/200] Loss: 409.9414\nEpoch [141/200] Loss: 402.6003\nEpoch [161/200] Loss: 401.2209\nEpoch [181/200] Loss: 403.0725\nAverage Test Loss: 366.0120\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1358.9732\nEpoch [21/200] Loss: 419.2162\nEpoch [41/200] Loss: 413.3607\nEpoch [61/200] Loss: 409.3754\nEpoch [81/200] Loss: 411.0923\nEpoch [101/200] Loss: 404.1099\nEpoch [121/200] Loss: 409.2167\nEpoch [141/200] Loss: 408.5509\nEpoch [161/200] Loss: 410.1073\nEpoch [181/200] Loss: 407.5864\nAverage Test Loss: 364.5863\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1237.3417\nEpoch [21/200] Loss: 337.7638\nEpoch [41/200] Loss: 223.9836\nEpoch [61/200] Loss: 98.1059\nEpoch [81/200] Loss: 46.6351\nEpoch [101/200] Loss: 31.9773\nEpoch [121/200] Loss: 26.2758\nEpoch [141/200] Loss: 19.9056\nEpoch [161/200] Loss: 17.7226\nEpoch [181/200] Loss: 15.4444\nAverage Test Loss: 14.0181\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1227.4362\nEpoch [21/200] Loss: 430.1239\nEpoch [41/200] Loss: 414.0475\nEpoch [61/200] Loss: 414.5093\nEpoch [81/200] Loss: 412.3910\nEpoch [101/200] Loss: 413.5266\nEpoch [121/200] Loss: 408.5565\nEpoch [141/200] Loss: 404.2547\nEpoch [161/200] Loss: 417.1860\nEpoch [181/200] Loss: 408.9205\nAverage Test Loss: 367.3491\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1362.6006\nEpoch [21/200] Loss: 417.5348\nEpoch [41/200] Loss: 409.7356\nEpoch [61/200] Loss: 411.1969\nEpoch [81/200] Loss: 413.3693\nEpoch [101/200] Loss: 414.5004\nEpoch [121/200] Loss: 405.8445\nEpoch [141/200] Loss: 404.8460\nEpoch [161/200] Loss: 415.0352\nEpoch [181/200] Loss: 414.2297\nAverage Test Loss: 366.4497\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1069.1285\nEpoch [21/200] Loss: 408.2721\nEpoch [41/200] Loss: 408.3345\nEpoch [61/200] Loss: 411.1250\nEpoch [81/200] Loss: 406.6349\nEpoch [101/200] Loss: 409.1261\nEpoch [121/200] Loss: 403.4630\nEpoch [141/200] Loss: 405.4774\nEpoch [161/200] Loss: 403.7881\nEpoch [181/200] Loss: 408.1804\nAverage Test Loss: 367.3865\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1065.3006\nEpoch [21/200] Loss: 443.1136\nEpoch [41/200] Loss: 407.0351\nEpoch [61/200] Loss: 409.3611\nEpoch [81/200] Loss: 403.8964\nEpoch [101/200] Loss: 402.2005\nEpoch [121/200] Loss: 402.1708\nEpoch [141/200] Loss: 403.3740\nEpoch [161/200] Loss: 398.9786\nEpoch [181/200] Loss: 408.3548\nAverage Test Loss: 380.6446\n\n\n  0%|          | 1/200 [00:00&lt;01:02,  3.18it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.38it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.36it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.19it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.36it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.41it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.36it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.37it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:53,  3.73it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.44it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.35it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.35it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.37it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:10,  3.56it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.53it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.50it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:38,  3.66it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.51it/s] 50%|█████     | 101/200 [00:28&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:34&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.41it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.41it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:52,  3.82it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.45it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.46it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.36it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.34it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.34it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.46it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.30it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.50it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.43it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.55it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.52it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.46it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.45it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.44it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto\n\n[367.97836858575994,\n 373.660117409446,\n 393.0328174937855,\n 367.32460992986506,\n 18.771740219809793,\n 8.816288991407914,\n 23.44990175420588,\n 366.0119906338778,\n 364.58628151633525,\n 14.018102992664684,\n 367.34911554509944,\n 366.4497375488281,\n 367.38653841885656,\n 380.64462835138494]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 8.816288991407914)\n('SOLM_M3pM3', 14.018102992664684)\n('CLOUD_OD', 18.771740219809793)\n('V10_MpS', 23.44990175420588)\n('SWSFC_WpM2', 364.58628151633525)\n('T2_K', 366.0119906338778)\n('CAPE', 366.4497375488281)\n('PRATE_MMpH', 367.32460992986506)\n('CLDTOP_KM', 367.34911554509944)\n('PBL_WRF_M', 367.38653841885656)\n('TSURF_K', 367.97836858575994)\n('SNOWEW_M', 373.660117409446)\n('PBL_YSU_M', 380.64462835138494)\n('SNOWAGE_HR', 393.0328174937855)\nLowest 3 names: ['U10_MpS', 'SOLM_M3pM3', 'CLOUD_OD']"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#insights-1",
    "href": "blogs/blogsData/Autoencoder.html#insights-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto, test_loss_list_P_25_conv_auto)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto, 'P25': test_loss_list_P_25_conv_auto, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n5\nU10_MpS\n8.8\n3.2\n6.0\n\n\n9\nSOLM_M3pM3\n14.0\n5.6\n9.8\n\n\n4\nCLOUD_OD\n18.8\n7.5\n13.1\n\n\n6\nV10_MpS\n23.4\n2.9\n13.2\n\n\n8\nSWSFC_WpM2\n364.6\n193.2\n278.9\n\n\n7\nT2_K\n366.0\n191.9\n279.0\n\n\n11\nCAPE\n366.4\n191.6\n279.0\n\n\n12\nPBL_WRF_M\n367.4\n190.8\n279.1\n\n\n3\nPRATE_MMpH\n367.3\n194.9\n281.1\n\n\n0\nTSURF_K\n368.0\n196.8\n282.4\n\n\n1\nSNOWEW_M\n373.7\n191.6\n282.6\n\n\n10\nCLDTOP_KM\n367.3\n205.5\n286.4\n\n\n13\nPBL_YSU_M\n380.6\n196.5\n288.6\n\n\n2\nSNOWAGE_HR\n393.0\n191.8\n292.4"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "href": "blogs/blogsData/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on top 4 channel and predicting on all channel CONV",
    "text": "Training on top 4 channel and predicting on all channel CONV\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 80\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/80] Loss: 1343.9761\nAverage Test Loss: 313.2759\nEpoch [21/80] Loss: 51.4848\nAverage Test Loss: 41.3688\nEpoch [41/80] Loss: 19.3960\nAverage Test Loss: 15.7720\nEpoch [61/80] Loss: 10.1627\nAverage Test Loss: 6.6679\nAverage Test Loss: 3.3631\n\n\n  1%|▏         | 1/80 [00:02&lt;02:40,  2.03s/it] 26%|██▋       | 21/80 [00:35&lt;01:40,  1.71s/it] 51%|█████▏    | 41/80 [01:09&lt;01:07,  1.72s/it] 76%|███████▋  | 61/80 [01:43&lt;00:32,  1.71s/it]100%|██████████| 80/80 [02:15&lt;00:00,  1.70s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in4_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#model-defination-2",
    "href": "blogs/blogsData/Autoencoder.html#model-defination-2",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n        )\n    def forward(self, x):\n        return self.decoder(x)\n    \nclass Autoencoder_UNET(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_UNET, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n    # def forward(self, x):\n    #     x = self.encoder(x)\n    #     # print(x.shape)\n    #     x = self.decoder(x)\n    #     return x\n    def forward(self, x):\n        conv1 = self.encoder.net[0]\n        activation1 = self.encoder.net[1]\n        conv2 = self.encoder.net[2]\n        activation2 = self.encoder.net[3]\n        conv3 = self.encoder.net[4]\n        activation3 = self.encoder.net[5]\n        conv4 = self.encoder.net[6]\n        activation4 = self.encoder.net[7]\n        conv5 = self.encoder.net[8]\n        activation5 = self.encoder.net[9]\n        flatten = self.encoder.net[10]\n        linear = self.encoder.net[11]\n        lineart = self.decoder.decoder[0]\n        unflattent = self.decoder.decoder[1]\n        convt2 = self.decoder.decoder[2]\n        activation7 = self.decoder.decoder[3]\n        convt3 = self.decoder.decoder[4]\n        activation8 = self.decoder.decoder[5]\n        convt4 = self.decoder.decoder[6]\n        activation9 = self.decoder.decoder[7]\n        convt5 = self.decoder.decoder[8]\n        activation10 = self.decoder.decoder[9]\n        convt6 = self.decoder.decoder[10]\n        x1 = activation1(conv1(x))\n        x2 = activation2(conv2(x1))\n        x3 = activation3(conv3(x2))\n        x4 = activation4(conv4(x3))\n        x5 = activation5(conv5(x4))\n        x6 = flatten(x5)\n        x7 = linear(x6)\n        x8 = lineart(x7)\n        x9 = unflattent(x8)\n        x10 = activation7(convt2(x9+x5))\n        x11 = activation8(convt3(x10+x4))\n        x12 = activation9(convt4(x11+x3))\n        x13 = activation10(convt5(x12+x2))\n        x13 = convt6(x13+x1)\n        return x13\n\n\nmodel_auto = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_UNET(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n           Linear-13                 [-1, 6400]       6,560,000\n        Unflatten-14           [-1, 64, 10, 10]               0\n  ConvTranspose2d-15           [-1, 32, 20, 20]          18,464\n             GELU-16           [-1, 32, 20, 20]               0\n           Conv2d-17           [-1, 32, 20, 20]           9,248\n             GELU-18           [-1, 32, 20, 20]               0\n  ConvTranspose2d-19           [-1, 16, 40, 40]           4,624\n             GELU-20           [-1, 16, 40, 40]               0\n           Conv2d-21           [-1, 16, 40, 40]           2,320\n             GELU-22           [-1, 16, 40, 40]               0\n  ConvTranspose2d-23            [-1, 2, 80, 80]             290\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.69\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.34\n----------------------------------------------------------------"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "href": "blogs/blogsData/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on all channel and predicting on all channel UNET",
    "text": "Training on all channel and predicting on all channel UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/200] Loss: 3185.9086\nEpoch [21/200] Loss: 238.7220\nEpoch [41/200] Loss: 90.1479\nEpoch [61/200] Loss: 19.3867\nEpoch [81/200] Loss: 7.6679\nEpoch [101/200] Loss: 5.9376\nEpoch [121/200] Loss: 7.9944\nEpoch [141/200] Loss: 5.3282\nEpoch [161/200] Loss: 4.2981\nEpoch [181/200] Loss: 4.3096\nAverage Test Loss: 3.9575\n\n\n  0%|          | 1/200 [00:03&lt;11:37,  3.50s/it] 10%|█         | 21/200 [01:08&lt;10:15,  3.44s/it] 20%|██        | 41/200 [02:19&lt;09:16,  3.50s/it] 30%|███       | 61/200 [03:44&lt;09:57,  4.30s/it] 40%|████      | 81/200 [05:06&lt;08:00,  4.04s/it] 50%|█████     | 101/200 [06:30&lt;07:00,  4.24s/it] 60%|██████    | 121/200 [07:57&lt;05:25,  4.12s/it] 70%|███████   | 141/200 [09:21&lt;03:59,  4.06s/it] 80%|████████  | 161/200 [10:38&lt;02:34,  3.97s/it] 90%|█████████ | 181/200 [11:58&lt;01:13,  3.87s/it]100%|██████████| 200/200 [13:15&lt;00:00,  3.98s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in14_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "href": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output(P25) UNET",
    "text": "Single channel input single channel output(P25) UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto_unet = [] \ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 2015.0104\nEpoch [21/200] Loss: 224.4622\nEpoch [41/200] Loss: 217.3994\nEpoch [61/200] Loss: 213.6307\nEpoch [81/200] Loss: 211.9555\nEpoch [101/200] Loss: 212.0086\nEpoch [121/200] Loss: 210.0710\nEpoch [141/200] Loss: 216.9347\nEpoch [161/200] Loss: 214.3995\nEpoch [181/200] Loss: 209.9813\nAverage Test Loss: 192.8667\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 720.3883\nEpoch [21/200] Loss: 215.2669\nEpoch [41/200] Loss: 212.9412\nEpoch [61/200] Loss: 210.6425\nEpoch [81/200] Loss: 209.0133\nEpoch [101/200] Loss: 216.0626\nEpoch [121/200] Loss: 209.8225\nEpoch [141/200] Loss: 209.2404\nEpoch [161/200] Loss: 212.0243\nEpoch [181/200] Loss: 208.9202\nAverage Test Loss: 191.8743\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 656.6864\nEpoch [21/200] Loss: 215.9682\nEpoch [41/200] Loss: 211.3083\nEpoch [61/200] Loss: 213.7716\nEpoch [81/200] Loss: 210.4161\nEpoch [101/200] Loss: 208.3461\nEpoch [121/200] Loss: 210.5009\nEpoch [141/200] Loss: 210.7708\nEpoch [161/200] Loss: 208.3222\nEpoch [181/200] Loss: 208.7196\nAverage Test Loss: 198.2665\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 598.2960\nEpoch [21/200] Loss: 227.3699\nEpoch [41/200] Loss: 219.5535\nEpoch [61/200] Loss: 210.1886\nEpoch [81/200] Loss: 210.2708\nEpoch [101/200] Loss: 209.0613\nEpoch [121/200] Loss: 208.2215\nEpoch [141/200] Loss: 207.7972\nEpoch [161/200] Loss: 213.5013\nEpoch [181/200] Loss: 208.5548\nAverage Test Loss: 199.3592\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 586.9568\nEpoch [21/200] Loss: 62.2030\nEpoch [41/200] Loss: 21.8947\nEpoch [61/200] Loss: 13.4883\nEpoch [81/200] Loss: 13.7934\nEpoch [101/200] Loss: 8.8402\nEpoch [121/200] Loss: 8.3218\nEpoch [141/200] Loss: 8.3719\nEpoch [161/200] Loss: 7.2083\nEpoch [181/200] Loss: 6.5652\nAverage Test Loss: 7.9540\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 473.9317\nEpoch [21/200] Loss: 69.0522\nEpoch [41/200] Loss: 18.4941\nEpoch [61/200] Loss: 10.7839\nEpoch [81/200] Loss: 7.7059\nEpoch [101/200] Loss: 6.4899\nEpoch [121/200] Loss: 5.4837\nEpoch [141/200] Loss: 7.5561\nEpoch [161/200] Loss: 3.8554\nEpoch [181/200] Loss: 4.0530\nAverage Test Loss: 3.1830\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 561.4590\nEpoch [21/200] Loss: 71.1724\nEpoch [41/200] Loss: 19.3733\nEpoch [61/200] Loss: 9.0035\nEpoch [81/200] Loss: 7.7880\nEpoch [101/200] Loss: 5.6728\nEpoch [121/200] Loss: 8.2339\nEpoch [141/200] Loss: 3.8740\nEpoch [161/200] Loss: 4.9030\nEpoch [181/200] Loss: 4.0399\nAverage Test Loss: 3.4703\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 4816.9447\nEpoch [21/200] Loss: 234.2117\nEpoch [41/200] Loss: 222.6848\nEpoch [61/200] Loss: 215.6618\nEpoch [81/200] Loss: 213.7606\nEpoch [101/200] Loss: 213.5892\nEpoch [121/200] Loss: 211.2940\nEpoch [141/200] Loss: 213.9061\nEpoch [161/200] Loss: 214.1695\nEpoch [181/200] Loss: 208.9214\nAverage Test Loss: 192.4815\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 512.7773\nEpoch [21/200] Loss: 214.1751\nEpoch [41/200] Loss: 212.7058\nEpoch [61/200] Loss: 214.4303\nEpoch [81/200] Loss: 212.5624\nEpoch [101/200] Loss: 208.8632\nEpoch [121/200] Loss: 210.4221\nEpoch [141/200] Loss: 209.8110\nEpoch [161/200] Loss: 207.3880\nEpoch [181/200] Loss: 207.7816\nAverage Test Loss: 191.0656\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 574.8718\nEpoch [21/200] Loss: 181.5824\nEpoch [41/200] Loss: 115.5675\nEpoch [61/200] Loss: 39.5182\nEpoch [81/200] Loss: 16.1863\nEpoch [101/200] Loss: 11.7081\nEpoch [121/200] Loss: 8.9398\nEpoch [141/200] Loss: 8.2249\nEpoch [161/200] Loss: 6.6518\nEpoch [181/200] Loss: 5.9480\nAverage Test Loss: 5.3015\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 647.1522\nEpoch [21/200] Loss: 215.3608\nEpoch [41/200] Loss: 211.6647\nEpoch [61/200] Loss: 209.6907\nEpoch [81/200] Loss: 211.7445\nEpoch [101/200] Loss: 207.4063\nEpoch [121/200] Loss: 207.9890\nEpoch [141/200] Loss: 207.9899\nEpoch [161/200] Loss: 209.2992\nEpoch [181/200] Loss: 213.8687\nAverage Test Loss: 191.1090\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 589.8784\nEpoch [21/200] Loss: 213.6897\nEpoch [41/200] Loss: 215.2983\nEpoch [61/200] Loss: 216.7662\nEpoch [81/200] Loss: 213.0695\nEpoch [101/200] Loss: 214.7085\nEpoch [121/200] Loss: 213.2036\nEpoch [141/200] Loss: 212.2645\nEpoch [161/200] Loss: 207.4999\nEpoch [181/200] Loss: 213.0907\nAverage Test Loss: 191.7055\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 613.1753\nEpoch [21/200] Loss: 215.8198\nEpoch [41/200] Loss: 211.0306\nEpoch [61/200] Loss: 210.1862\nEpoch [81/200] Loss: 208.5808\nEpoch [101/200] Loss: 208.4701\nEpoch [121/200] Loss: 209.8240\nEpoch [141/200] Loss: 208.0958\nEpoch [161/200] Loss: 209.3367\nEpoch [181/200] Loss: 207.0369\nAverage Test Loss: 192.7273\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 419.9077\nEpoch [21/200] Loss: 210.7698\nEpoch [41/200] Loss: 208.6584\nEpoch [61/200] Loss: 210.8284\nEpoch [81/200] Loss: 208.8175\nEpoch [101/200] Loss: 206.5321\nEpoch [121/200] Loss: 207.8562\nEpoch [141/200] Loss: 207.3433\nEpoch [161/200] Loss: 206.2741\nEpoch [181/200] Loss: 207.5728\nAverage Test Loss: 194.3204\n\n\n  0%|          | 0/200 [00:00&lt;?, ?it/s]  0%|          | 1/200 [00:00&lt;01:04,  3.09it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.11it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.20it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.18it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.19it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.26it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.15it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.26it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.22it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.17it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:11,  3.26it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.17it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.18it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.14it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.14it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.18it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.25it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.16it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.25it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.33it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.30it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.21it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.27it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.29it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.23it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.23it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.22it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.22it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.15it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.23it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.26it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.34it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.28it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.22it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.26it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.25it/s] 40%|████      | 81/200 [00:24&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.26it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.24it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.19it/s] 30%|███       | 61/200 [00:19&lt;00:44,  3.10it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.17it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.20it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.30it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.14it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:35,  3.31it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.30it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.34it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.23it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.27it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:38,  3.11it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.18it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.19it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:57&lt;00:06,  3.01it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto_unet\n\n[192.86672002618963,\n 191.87428977272728,\n 198.26652665571734,\n 199.35923628373578,\n 7.954025506973267,\n 3.18301400271329,\n 3.4703480113636362,\n 192.4814786044034,\n 191.06564331054688,\n 5.301476088437167,\n 191.10903930664062,\n 191.70547346635297,\n 192.72732405229047,\n 194.32039434259588]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 3.18301400271329)\n('V10_MpS', 3.4703480113636362)\n('SOLM_M3pM3', 5.301476088437167)\n('CLOUD_OD', 7.954025506973267)\n('SWSFC_WpM2', 191.06564331054688)\n('CLDTOP_KM', 191.10903930664062)\n('CAPE', 191.70547346635297)\n('SNOWEW_M', 191.87428977272728)\n('T2_K', 192.4814786044034)\n('PBL_WRF_M', 192.72732405229047)\n('TSURF_K', 192.86672002618963)\n('PBL_YSU_M', 194.32039434259588)\n('SNOWAGE_HR', 198.26652665571734)\n('PRATE_MMpH', 199.35923628373578)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1, 80, 80))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                  [-1, 512]       1,638,912\n           Linear-13                 [-1, 3200]       1,641,600\n        Unflatten-14           [-1, 32, 10, 10]               0\n  ConvTranspose2d-15           [-1, 16, 20, 20]           4,624\n             GELU-16           [-1, 16, 20, 20]               0\n           Conv2d-17           [-1, 16, 20, 20]           2,320\n             GELU-18           [-1, 16, 20, 20]               0\n  ConvTranspose2d-19            [-1, 8, 40, 40]           1,160\n             GELU-20            [-1, 8, 40, 40]               0\n           Conv2d-21            [-1, 8, 40, 40]             584\n             GELU-22            [-1, 8, 40, 40]               0\n  ConvTranspose2d-23            [-1, 1, 80, 80]              73\n================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.35\nParams size (MB): 12.58\nEstimated Total Size (MB): 13.95\n----------------------------------------------------------------\n\n\n\nsummary(model, input_size=(1, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                 [-1, 1024]       3,277,824\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 3200]       3,280,000\n        Unflatten-15           [-1, 32, 10, 10]               0\n  ConvTranspose2d-16           [-1, 16, 20, 20]           4,624\n             GELU-17           [-1, 16, 20, 20]               0\n           Conv2d-18           [-1, 16, 20, 20]           2,320\n             GELU-19           [-1, 16, 20, 20]               0\n  ConvTranspose2d-20            [-1, 8, 40, 40]           1,160\n             GELU-21            [-1, 8, 40, 40]               0\n           Conv2d-22            [-1, 8, 40, 40]             584\n             GELU-23            [-1, 8, 40, 40]               0\n  ConvTranspose2d-24            [-1, 1, 80, 80]              73\n          Decoder-25            [-1, 1, 80, 80]               0\n================================================================\nTotal params: 6,575,377\nTrainable params: 6,575,377\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.41\nParams size (MB): 25.08\nEstimated Total Size (MB): 26.51\n----------------------------------------------------------------"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "href": "blogs/blogsData/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output(P10) UNET",
    "text": "Single channel input single channel output(P10) UNET\n\ntest_loss_list_P_10_conv_auto_unet = [] \ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 9347.3935\nEpoch [21/200] Loss: 445.2209\nEpoch [41/200] Loss: 421.8352\nEpoch [61/200] Loss: 411.4913\nEpoch [81/200] Loss: 406.5355\nEpoch [101/200] Loss: 410.0266\nEpoch [121/200] Loss: 403.3963\nEpoch [141/200] Loss: 402.2838\nEpoch [161/200] Loss: 404.5083\nEpoch [181/200] Loss: 408.9846\nAverage Test Loss: 393.9739\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1062.0485\nEpoch [21/200] Loss: 425.7831\nEpoch [41/200] Loss: 414.1087\nEpoch [61/200] Loss: 407.9692\nEpoch [81/200] Loss: 409.0262\nEpoch [101/200] Loss: 413.5665\nEpoch [121/200] Loss: 406.8342\nEpoch [141/200] Loss: 401.0203\nEpoch [161/200] Loss: 403.7887\nEpoch [181/200] Loss: 412.0851\nAverage Test Loss: 384.9228\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1334.1772\nEpoch [21/200] Loss: 432.3582\nEpoch [41/200] Loss: 416.2745\nEpoch [61/200] Loss: 412.7553\nEpoch [81/200] Loss: 411.6587\nEpoch [101/200] Loss: 409.5164\nEpoch [121/200] Loss: 409.2917\nEpoch [141/200] Loss: 414.7216\nEpoch [161/200] Loss: 405.0483\nEpoch [181/200] Loss: 404.8374\nAverage Test Loss: 366.9726\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1228.4596\nEpoch [21/200] Loss: 418.4353\nEpoch [41/200] Loss: 411.4351\nEpoch [61/200] Loss: 411.3802\nEpoch [81/200] Loss: 408.2070\nEpoch [101/200] Loss: 402.7285\nEpoch [121/200] Loss: 405.0596\nEpoch [141/200] Loss: 401.6986\nEpoch [161/200] Loss: 401.9078\nEpoch [181/200] Loss: 405.1741\nAverage Test Loss: 397.5330\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1086.5345\nEpoch [21/200] Loss: 115.0784\nEpoch [41/200] Loss: 55.1418\nEpoch [61/200] Loss: 35.0410\nEpoch [81/200] Loss: 25.9328\nEpoch [101/200] Loss: 22.8977\nEpoch [121/200] Loss: 22.6624\nEpoch [141/200] Loss: 18.5878\nEpoch [161/200] Loss: 16.7638\nEpoch [181/200] Loss: 18.7935\nAverage Test Loss: 16.7694\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1370.2549\nEpoch [21/200] Loss: 253.0794\nEpoch [41/200] Loss: 73.3127\nEpoch [61/200] Loss: 34.2972\nEpoch [81/200] Loss: 23.2712\nEpoch [101/200] Loss: 18.8157\nEpoch [121/200] Loss: 17.0560\nEpoch [141/200] Loss: 13.7983\nEpoch [161/200] Loss: 12.2436\nEpoch [181/200] Loss: 11.2890\nAverage Test Loss: 16.9951\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1202.3667\nEpoch [21/200] Loss: 284.9583\nEpoch [41/200] Loss: 49.5132\nEpoch [61/200] Loss: 23.7217\nEpoch [81/200] Loss: 16.5870\nEpoch [101/200] Loss: 13.4451\nEpoch [121/200] Loss: 11.5799\nEpoch [141/200] Loss: 10.4765\nEpoch [161/200] Loss: 13.5249\nEpoch [181/200] Loss: 9.1645\nAverage Test Loss: 8.1653\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1344.0557\nEpoch [21/200] Loss: 410.2120\nEpoch [41/200] Loss: 409.9936\nEpoch [61/200] Loss: 403.0425\nEpoch [81/200] Loss: 413.2495\nEpoch [101/200] Loss: 410.5454\nEpoch [121/200] Loss: 408.9813\nEpoch [141/200] Loss: 404.3268\nEpoch [161/200] Loss: 402.1354\nEpoch [181/200] Loss: 403.1094\nAverage Test Loss: 370.9240\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1138.2640\nEpoch [21/200] Loss: 428.3663\nEpoch [41/200] Loss: 428.3100\nEpoch [61/200] Loss: 411.9817\nEpoch [81/200] Loss: 406.6649\nEpoch [101/200] Loss: 406.5036\nEpoch [121/200] Loss: 404.4717\nEpoch [141/200] Loss: 413.3260\nEpoch [161/200] Loss: 404.8024\nEpoch [181/200] Loss: 408.2683\nAverage Test Loss: 375.5593\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1171.9761\nEpoch [21/200] Loss: 311.1736\nEpoch [41/200] Loss: 213.8882\nEpoch [61/200] Loss: 86.1118\nEpoch [81/200] Loss: 43.9884\nEpoch [101/200] Loss: 33.4366\nEpoch [121/200] Loss: 26.7442\nEpoch [141/200] Loss: 25.1023\nEpoch [161/200] Loss: 19.1235\nEpoch [181/200] Loss: 16.5218\nAverage Test Loss: 17.8878\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1209.7468\nEpoch [21/200] Loss: 418.6483\nEpoch [41/200] Loss: 411.9460\nEpoch [61/200] Loss: 416.4541\nEpoch [81/200] Loss: 410.3450\nEpoch [101/200] Loss: 404.6752\nEpoch [121/200] Loss: 411.7702\nEpoch [141/200] Loss: 404.7753\nEpoch [161/200] Loss: 410.6092\nEpoch [181/200] Loss: 406.9762\nAverage Test Loss: 364.9331\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1286.6324\nEpoch [21/200] Loss: 416.3279\nEpoch [41/200] Loss: 412.8959\nEpoch [61/200] Loss: 432.9754\nEpoch [81/200] Loss: 423.2503\nEpoch [101/200] Loss: 407.6195\nEpoch [121/200] Loss: 405.0848\nEpoch [141/200] Loss: 418.5362\nEpoch [161/200] Loss: 398.3066\nEpoch [181/200] Loss: 399.7189\nAverage Test Loss: 366.6939\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1160.2099\nEpoch [21/200] Loss: 410.9463\nEpoch [41/200] Loss: 407.0937\nEpoch [61/200] Loss: 406.6331\nEpoch [81/200] Loss: 405.4273\nEpoch [101/200] Loss: 406.6442\nEpoch [121/200] Loss: 405.2662\nEpoch [141/200] Loss: 400.4130\nEpoch [161/200] Loss: 403.3931\nEpoch [181/200] Loss: 409.0273\nAverage Test Loss: 364.0550\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.6661\nEpoch [21/200] Loss: 410.8530\nEpoch [41/200] Loss: 408.6113\nEpoch [61/200] Loss: 400.7242\nEpoch [81/200] Loss: 407.2048\nEpoch [101/200] Loss: 407.0074\nEpoch [121/200] Loss: 405.1491\nEpoch [141/200] Loss: 428.9850\nEpoch [161/200] Loss: 411.7955\nEpoch [181/200] Loss: 412.6084\nAverage Test Loss: 375.2351\n\n\n  0%|          | 1/200 [00:00&lt;01:04,  3.10it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.24it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.20it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.14it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.23it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.16it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.13it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.19it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.09it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.29it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.11it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:07,  2.95it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.19it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.30it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.27it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.26it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.27it/s] 60%|██████    | 121/200 [00:37&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.15it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.25it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.31it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.27it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:44,  3.12it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.21it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.14it/s] 70%|███████   | 141/200 [00:43&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.26it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.29it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:39&lt;00:33,  2.34it/s] 70%|███████   | 141/200 [00:47&lt;00:25,  2.32it/s] 80%|████████  | 161/200 [00:55&lt;00:15,  2.44it/s] 90%|█████████ | 181/200 [01:03&lt;00:06,  3.06it/s]100%|██████████| 200/200 [01:10&lt;00:00,  2.85it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.25it/s] 10%|█         | 21/200 [00:08&lt;01:31,  1.95it/s] 20%|██        | 41/200 [00:15&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:21&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:27&lt;00:34,  3.47it/s] 50%|█████     | 101/200 [00:33&lt;00:32,  3.06it/s] 60%|██████    | 121/200 [00:41&lt;00:22,  3.51it/s] 70%|███████   | 141/200 [00:46&lt;00:16,  3.63it/s] 80%|████████  | 161/200 [00:52&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:58&lt;00:05,  3.47it/s]100%|██████████| 200/200 [01:03&lt;00:00,  3.14it/s]\n  0%|          | 1/200 [00:00&lt;01:13,  2.70it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.49it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.49it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.49it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.47it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.52it/s] 80%|████████  | 161/200 [00:46&lt;00:10,  3.61it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:34&lt;00:50,  1.95it/s] 60%|██████    | 121/200 [00:48&lt;00:21,  3.66it/s] 70%|███████   | 141/200 [00:54&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:59&lt;00:12,  3.17it/s] 90%|█████████ | 181/200 [01:05&lt;00:05,  3.38it/s]100%|██████████| 200/200 [01:12&lt;00:00,  2.74it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.31it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.48it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.49it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.50it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.49it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.43it/s]\n  0%|          | 1/200 [00:00&lt;01:05,  3.05it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.56it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.46it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.53it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:26,  3.68it/s] 60%|██████    | 121/200 [00:34&lt;00:22,  3.56it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.55it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.49it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.57it/s] 50%|█████     | 101/200 [00:29&lt;00:27,  3.55it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.47it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.64it/s] 10%|█         | 21/200 [00:06&lt;00:49,  3.63it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:36&lt;00:25,  3.12it/s] 70%|███████   | 141/200 [00:42&lt;00:18,  3.27it/s] 80%|████████  | 161/200 [00:48&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:54&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:00&lt;00:00,  3.29it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto_unet\n\n[393.97390192205256,\n 384.9228349165483,\n 366.9725619229403,\n 397.5330338911577,\n 16.76939444108443,\n 16.99508944424716,\n 8.165278738195246,\n 370.9240167791193,\n 375.55933172052556,\n 17.88784339211204,\n 364.93308327414775,\n 366.69390869140625,\n 364.0550065474077,\n 375.23513239080256]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 8.165278738195246)\n('CLOUD_OD', 16.76939444108443)\n('U10_MpS', 16.99508944424716)\n('SOLM_M3pM3', 17.88784339211204)\n('PBL_WRF_M', 364.0550065474077)\n('CLDTOP_KM', 364.93308327414775)\n('CAPE', 366.69390869140625)\n('SNOWAGE_HR', 366.9725619229403)\n('T2_K', 370.9240167791193)\n('PBL_YSU_M', 375.23513239080256)\n('SWSFC_WpM2', 375.55933172052556)\n('SNOWEW_M', 384.9228349165483)\n('TSURF_K', 393.97390192205256)\n('PRATE_MMpH', 397.5330338911577)\nLowest 3 names: ['V10_MpS', 'CLOUD_OD', 'U10_MpS']"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#insights-2",
    "href": "blogs/blogsData/Autoencoder.html#insights-2",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto_unet, test_loss_list_P_25_conv_auto_unet)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto_unet, 'P25': test_loss_list_P_25_conv_auto_unet, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto_unet.csv', index=False)\n\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n8.2\n3.5\n5.8\n\n\n5\nU10_MpS\n17.0\n3.2\n10.1\n\n\n9\nSOLM_M3pM3\n17.9\n5.3\n11.6\n\n\n4\nCLOUD_OD\n16.8\n8.0\n12.4\n\n\n10\nCLDTOP_KM\n364.9\n191.1\n278.0\n\n\n12\nPBL_WRF_M\n364.1\n192.7\n278.4\n\n\n11\nCAPE\n366.7\n191.7\n279.2\n\n\n7\nT2_K\n370.9\n192.5\n281.7\n\n\n2\nSNOWAGE_HR\n367.0\n198.3\n282.6\n\n\n8\nSWSFC_WpM2\n375.6\n191.1\n283.3\n\n\n13\nPBL_YSU_M\n375.2\n194.3\n284.8\n\n\n1\nSNOWEW_M\n384.9\n191.9\n288.4\n\n\n0\nTSURF_K\n394.0\n192.9\n293.4\n\n\n3\nPRATE_MMpH\n397.5\n199.4\n298.4"
  },
  {
    "objectID": "blogs/blogsData/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "href": "blogs/blogsData/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on top 4 channel and predicting on all channel UNET",
    "text": "Training on top 4 channel and predicting on all channel UNET\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 100\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/100] Loss: 519.4141\nEpoch [21/100] Loss: 10.5276\nEpoch [41/100] Loss: 2.6655\nEpoch [61/100] Loss: 5.3309\nEpoch [81/100] Loss: 2.3606\nAverage Test Loss: 0.9041\n\n\n  1%|          | 1/100 [00:01&lt;03:17,  1.99s/it] 21%|██        | 21/100 [00:36&lt;02:15,  1.72s/it] 41%|████      | 41/100 [01:10&lt;01:41,  1.72s/it] 61%|██████    | 61/100 [01:44&lt;01:06,  1.72s/it] 81%|████████  | 81/100 [02:19&lt;00:32,  1.72s/it]100%|██████████| 100/100 [02:52&lt;00:00,  1.72s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in4_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html",
    "href": "publications_and_projects/data_folder/Autoencoder.html",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "",
    "text": "camx.png\n\n\n\nGo throught the link to see the presentation of the project to get the flow.\nGit Repo: link\n\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch \nimport os\nimport torch.utils.data as data\nimport torch.nn as nn\nimport glob \n# from torchsummary import summary\nfrom torchinfo import summary\nfrom tqdm import trange\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cuda:2\")\nprint(device)\ncurrent_device = device #torch.cuda.current_device()\ndevice_name = torch.cuda.get_device_name(current_device)\nprint(f\"Current GPU assigned: {current_device}, Name: {device_name}\")\n\ncuda\nCurrent GPU assigned: cuda, Name: Quadro RTX 5000\n\n\nConsider it a prediction task, use any channel like pressure and/or wind speed inputs to start with, because they are the indicators of horizontal movement of air pollution. Multiple inputs can go as multiple channels of images similar to RGB. Output can be P25 or P10 in cam 120hr files.\n\ndef get_latitudes():\n    lat_start= 76.8499984741211\n    lat_step=0.009999999776482582\n\n    latitudes=[]\n\n    for i in range(80):\n        latitudes.append(lat_start+i*lat_step)\n    \n    latitudes.reverse()\n\n    return latitudes\n\ndef get_longitudes():\n    long_start= 28.200000762939453\n    long_step=0.009999999776482582\n\n    longitudes=[]\n\n    for i in range(80):\n        longitudes.append(long_start+i*long_step)\n    \n    # longitudes.reverse()\n\n    return longitudes\n\nlatitudes=get_latitudes()\nlongitudes=get_longitudes()\n\ndef create_plot(data,hour,var_name):\n    # print(data[var_name].shape) #shape (120, 1, 80, 80)\n    p10_hour=data[var_name]['TSTEP'==hour] # shape (1, 80, 80)\n    p10_hour=p10_hour[0,:,:] # shape (80, 80)\n    plt.imshow(p10_hour)\n    plt.title(f'{var_name} at hour '+str(hour))\n    # plt.colorbar()\n\n    # only show every latitude and longitude of end points\n    # round to 2 decimal places\n    top=latitudes[0]\n    top=round(top,2) \n    bottom=latitudes[-1]\n    bottom=round(bottom,2)\n    left=longitudes[0]\n    left=round(left,2)\n    right=longitudes[-1]\n    right=round(right,2)\n\n    plt.xticks([0,79],[left,right])\n    plt.xlabel('Longitude')\n    plt.yticks([0,79],[top,bottom])\n    plt.ylabel('Latitude')\n    # plt.savefig(f'plots/120/{var_name}_{day}.png')\n    # plt.close()"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#visualizing-96-hr-files",
    "href": "publications_and_projects/data_folder/Autoencoder.html#visualizing-96-hr-files",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Visualizing 96 hr files",
    "text": "Visualizing 96 hr files\n\ndata_96 = xr.open_dataset('data/camxmet2d.delhi.20230717.96hours.nc')\ndata_96_df = data_96.to_dataframe().reset_index()\n\n\ndata_96\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (TSTEP: 96, VAR: 14, DATE-TIME: 2, LAY: 1, ROW: 80, COL: 80)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables: (12/15)\n    TFLAG       (TSTEP, VAR, DATE-TIME) int32 2023198 0 ... 2023201 230000\n    TSURF_K     (TSTEP, LAY, ROW, COL) float32 302.3 302.3 302.3 ... 300.6 301.2\n    SNOWEW_M    (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SNOWAGE_HR  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PRATE_MMpH  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CLOUD_OD    (TSTEP, LAY, ROW, COL) float32 62.24 61.67 61.1 ... 37.1 36.78\n    ...          ...\n    SWSFC_WpM2  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SOLM_M3pM3  (TSTEP, LAY, ROW, COL) float32 0.3131 0.3114 ... 0.3278 0.3292\n    CLDTOP_KM   (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CAPE        (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PBL_WRF_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 94.4 120.0\n    PBL_YSU_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 64.43 94.71\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023198\n    CTIME:          73941\n    WDATE:          2023198\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 96VAR: 14DATE-TIME: 2LAY: 1ROW: 80COL: 80Coordinates: (0)Data variables: (15)TFLAG(TSTEP, VAR, DATE-TIME)int322023198 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023198,       0],\n        [2023198,       0],\n        ...,\n        [2023198,       0],\n        [2023198,       0]],\n\n       [[2023198,   10000],\n        [2023198,   10000],\n        ...,\n        [2023198,   10000],\n        [2023198,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)TSURF_K(TSTEP, LAY, ROW, COL)float32302.3 302.3 302.3 ... 300.6 301.2long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                array([[[[302.31464, ..., 301.9288 ],\n         ...,\n         [301.28806, ..., 301.65118]]],\n\n\n       ...,\n\n\n       [[[299.001  , ..., 299.86826],\n         ...,\n         [299.08127, ..., 301.1575 ]]]], dtype=float32)SNOWEW_M(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SNOWAGE_HR(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PRATE_MMpH(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CLOUD_OD(TSTEP, LAY, ROW, COL)float3262.24 61.67 61.1 ... 37.1 36.78long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               array([[[[62.239326, ..., 30.003962],\n         ...,\n         [54.13987 , ..., 43.78004 ]]],\n\n\n       ...,\n\n\n       [[[29.166183, ..., 58.07322 ],\n         ...,\n         [32.563763, ..., 36.776646]]]], dtype=float32)U10_MpS(TSTEP, LAY, ROW, COL)float321.63 1.585 1.54 ... 0.8064 0.8805long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)V10_MpS(TSTEP, LAY, ROW, COL)float320.1487 0.158 ... -1.202 -1.152long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                array([[[[ 0.148722, ...,  0.630212],\n         ...,\n         [-0.328182, ...,  0.625002]]],\n\n\n       ...,\n\n\n       [[[ 0.040896, ..., -1.492097],\n         ...,\n         [-0.365465, ..., -1.152316]]]], dtype=float32)T2_K(TSTEP, LAY, ROW, COL)float32302.6 302.6 302.6 ... 300.4 300.8long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   array([[[[302.57184, ..., 302.88318],\n         ...,\n         [301.1836 , ..., 302.19128]]],\n\n\n       ...,\n\n\n       [[[299.17767, ..., 300.1615 ],\n         ...,\n         [299.30466, ..., 300.77878]]]], dtype=float32)SWSFC_WpM2(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SOLM_M3pM3(TSTEP, LAY, ROW, COL)float320.3131 0.3114 ... 0.3278 0.3292long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             array([[[[0.313128, ..., 0.271955],\n         ...,\n         [0.306425, ..., 0.287092]]],\n\n\n       ...,\n\n\n       [[[0.312972, ..., 0.263079],\n         ...,\n         [0.29241 , ..., 0.32918 ]]]], dtype=float32)CLDTOP_KM(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CAPE(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PBL_WRF_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 94.4 120.0long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              array([[[[ 17.212452, ...,  17.253763],\n         ...,\n         [ 17.153704, ...,  17.189987]]],\n\n\n       ...,\n\n\n       [[[ 33.101303, ...,  30.475212],\n         ...,\n         [ 24.450672, ..., 120.012794]]]], dtype=float32)PBL_YSU_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 64.43 94.71long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              array([[[[17.212452, ..., 17.253763],\n         ...,\n         [17.153704, ..., 17.189987]]],\n\n\n       ...,\n\n\n       [[[17.060207, ..., 17.152937],\n         ...,\n         [17.07223 , ..., 94.71051 ]]]], dtype=float32)Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023198CTIME :73941WDATE :2023198WTIME :73941SDATE :2023198STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\ndata_96_df\n\n\n\n\n\n\n\n\nTSTEP\nVAR\nDATE-TIME\nLAY\nROW\nCOL\nTFLAG\nTSURF_K\nSNOWEW_M\nSNOWAGE_HR\n...\nCLOUD_OD\nU10_MpS\nV10_MpS\nT2_K\nSWSFC_WpM2\nSOLM_M3pM3\nCLDTOP_KM\nCAPE\nPBL_WRF_M\nPBL_YSU_M\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n2023198\n302.314636\n0.0\n0.0\n...\n62.239326\n1.630373\n0.148722\n302.571838\n0.0\n0.313128\n0.0\n0.0\n17.212452\n17.212452\n\n\n1\n0\n0\n0\n0\n0\n1\n2023198\n302.312042\n0.0\n0.0\n...\n61.671093\n1.585281\n0.158011\n302.584351\n0.0\n0.311416\n0.0\n0.0\n17.212652\n17.212652\n\n\n2\n0\n0\n0\n0\n0\n2\n2023198\n302.309479\n0.0\n0.0\n...\n61.102859\n1.540188\n0.167299\n302.596832\n0.0\n0.309703\n0.0\n0.0\n17.212852\n17.212852\n\n\n3\n0\n0\n0\n0\n0\n3\n2023198\n302.306885\n0.0\n0.0\n...\n60.534630\n1.495095\n0.176587\n302.609344\n0.0\n0.307990\n0.0\n0.0\n17.213055\n17.213055\n\n\n4\n0\n0\n0\n0\n0\n4\n2023198\n302.303558\n0.0\n0.0\n...\n59.912636\n1.450304\n0.184773\n302.621948\n0.0\n0.306286\n0.0\n0.0\n17.213356\n17.213356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17203195\n95\n13\n1\n0\n79\n75\n230000\n298.979095\n0.0\n0.0\n...\n37.972748\n0.600130\n-1.341342\n299.389465\n0.0\n0.324286\n0.0\n0.0\n26.661556\n17.082989\n\n\n17203196\n95\n13\n1\n0\n79\n76\n230000\n299.372223\n0.0\n0.0\n...\n37.747482\n0.658128\n-1.301898\n299.638062\n0.0\n0.324927\n0.0\n0.0\n43.123440\n17.094244\n\n\n17203197\n95\n13\n1\n0\n79\n77\n230000\n299.967468\n0.0\n0.0\n...\n37.423786\n0.732263\n-1.252025\n300.018402\n0.0\n0.326345\n0.0\n0.0\n68.759804\n17.112419\n\n\n17203198\n95\n13\n1\n0\n79\n78\n230000\n300.562744\n0.0\n0.0\n...\n37.100090\n0.806398\n-1.202152\n300.398743\n0.0\n0.327763\n0.0\n0.0\n94.396065\n64.426094\n\n\n17203199\n95\n13\n1\n0\n79\n79\n230000\n301.157501\n0.0\n0.0\n...\n36.776646\n0.880477\n-1.152316\n300.778778\n0.0\n0.329180\n0.0\n0.0\n120.012794\n94.710510\n\n\n\n\n17203200 rows × 21 columns\n\n\n\n\ndata_96_df.columns\n\nIndex(['TSTEP', 'VAR', 'DATE-TIME', 'LAY', 'ROW', 'COL', 'TFLAG', 'TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M'],\n      dtype='object')\n\n\n\ndata_96['U10_MpS'] #shape (96, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'U10_MpS' (TSTEP: 96, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  U10_MpS         \n    units:      ppmV            \n    var_desc:   VARIABLE U10_MpS                                             ...xarray.DataArray'U10_MpS'TSTEP: 96LAY: 1ROW: 80COL: 801.63 1.585 1.54 1.495 1.45 ... 0.6001 0.6581 0.7323 0.8064 0.8805array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                \n\n\n\ndata_96['U10_MpS'].shape , data_96['U10_MpS']['TSTEP'==0].shape, data_96['U10_MpS']['TSTEP'==0][0].shape\n\n((96, 1, 80, 80), (1, 80, 80), (80, 80))\n\n\n\nplt.imshow(data_96['U10_MpS']['TSTEP'==0][0]) #shape (80, 80)\n\n&lt;matplotlib.image.AxesImage at 0x7f76abc7cf70&gt;\n\n\n\n\n\n\ncreate_plot(data = data_96, hour = 1, var_name = 'U10_MpS')"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#visualizing-120-hr-files",
    "href": "publications_and_projects/data_folder/Autoencoder.html#visualizing-120-hr-files",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Visualizing 120 hr files",
    "text": "Visualizing 120 hr files\n\ndata_120 = xr.open_dataset('data/camx120hr_merged_20230717.nc')\ndata_120_df = data_120.to_dataframe().reset_index() \n\n\ndata_120 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 120, LAY: 1, ROW: 80, COL: 80, VAR: 9, DATE-TIME: 2)\nDimensions without coordinates: TSTEP, LAY, ROW, COL, VAR, DATE-TIME\nData variables:\n    P10      (TSTEP, LAY, ROW, COL) float32 23.86 23.86 24.07 ... 10.1 10.1\n    P25      (TSTEP, LAY, ROW, COL) float32 19.24 19.24 19.62 ... 9.63 9.63\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 2023197 0 2023197 ... 2023201 230000\nAttributes: (12/34)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023197\n    CTIME:          83911\n    WDATE:          2023197\n    ...             ...\n    GDNAM:          ????????????????\n    UPNAM:          CAMXMETOU       \n    VAR-LIST:       P10             P25             \n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc ca...\n    NCO:            netCDF Operators version 4.9.1 (Homepage = http://nco.sf....xarray.DatasetDimensions:TSTEP: 120LAY: 1ROW: 80COL: 80VAR: 9DATE-TIME: 2Coordinates: (0)Data variables: (3)P10(TSTEP, LAY, ROW, COL)float3223.86 23.86 24.07 ... 10.1 10.1long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)P25(TSTEP, LAY, ROW, COL)float3219.24 19.24 19.62 ... 9.63 9.63long_name :FPRM            units :micrograms/m**3 var_desc :VARIABLE FPRM                                                                   array([[[[19.240587, ..., 26.00578 ],\n         ...,\n         [31.570122, ..., 17.510971]]],\n\n\n       ...,\n\n\n       [[[22.36091 , ..., 16.678066],\n         ...,\n         [18.63693 , ...,  9.629862]]]], dtype=float32)TFLAG(TSTEP, VAR, DATE-TIME)int322023197 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023197,       0],\n        [2023197,       0],\n        ...,\n        [2023197,       0],\n        [2023197,       0]],\n\n       [[2023197,   10000],\n        [2023197,   10000],\n        ...,\n        [2023197,   10000],\n        [2023197,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)Indexes: (0)Attributes: (34)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023197CTIME :83911WDATE :2023197WTIME :83911SDATE :2023197STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :2GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMXMETOU       VAR-LIST :P10             P25             FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc camxout.2023.07.17.nc camxout.2023.07.18.nc camxout.2023.07.19.nc camxout.2023.07.20.nc camx120hr.nc\nNCO :netCDF Operators version 4.9.1 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n\n\n\ndata_120_df\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n23.857107\n19.240587\n2023197\n\n\n1\n0\n0\n0\n0\n0\n1\n23.857107\n19.240587\n0\n\n\n2\n0\n0\n0\n0\n1\n0\n23.857107\n19.240587\n2023197\n\n\n3\n0\n0\n0\n0\n1\n1\n23.857107\n19.240587\n0\n\n\n4\n0\n0\n0\n0\n2\n0\n23.857107\n19.240587\n2023197\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13823995\n119\n0\n79\n79\n6\n1\n10.097546\n9.629862\n230000\n\n\n13823996\n119\n0\n79\n79\n7\n0\n10.097546\n9.629862\n2023201\n\n\n13823997\n119\n0\n79\n79\n7\n1\n10.097546\n9.629862\n230000\n\n\n13823998\n119\n0\n79\n79\n8\n0\n10.097546\n9.629862\n2023201\n\n\n13823999\n119\n0\n79\n79\n8\n1\n10.097546\n9.629862\n230000\n\n\n\n\n13824000 rows × 9 columns\n\n\n\n\ndata_120_df.describe()\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\ncount\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n\n\nmean\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n4.101470e+01\n2.856279e+01\n1.069100e+06\n\n\nstd\n3.463981e+01\n0.0\n2.309221e+01\n2.309221e+01\n2.581989e+00\n0.5\n2.361775e+01\n1.582030e+01\n9.553543e+05\n\n\nmin\n0.000000e+00\n0.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.0\n1.111396e+00\n8.114247e-01\n0.000000e+00\n\n\n25%\n2.975000e+01\n0.0\n1.975000e+01\n1.975000e+01\n2.000000e+00\n0.0\n2.546643e+01\n1.880392e+01\n1.175000e+05\n\n\n50%\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n3.590978e+01\n2.589731e+01\n1.126598e+06\n\n\n75%\n8.925000e+01\n0.0\n5.925000e+01\n5.925000e+01\n6.000000e+00\n1.0\n5.103806e+01\n3.433007e+01\n2.023199e+06\n\n\nmax\n1.190000e+02\n0.0\n7.900000e+01\n7.900000e+01\n8.000000e+00\n1.0\n6.492913e+02\n6.130998e+02\n2.023201e+06\n\n\n\n\n\n\n\n\ndata_120['COL']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'COL' (COL: 80)&gt;\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])\nDimensions without coordinates: COLxarray.DataArray'COL'COL: 800 1 2 3 4 5 6 7 8 9 10 11 12 ... 68 69 70 71 72 73 74 75 76 77 78 79array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\n\ndata_120['P10'] #shape (120, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (TSTEP: 120, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'TSTEP: 120LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 9.551 9.664 9.808 10.1 10.1array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1]# shape 1x80x80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)\nDimensions without coordinates: LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 18.47 18.77 18.69 18.51 18.51array([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1][0] # shape 80x80\nplt.imshow(data_120['P25']['TSTEP'==1][0])#, vmin=0, vmax=100)\n\n&lt;matplotlib.image.AxesImage at 0x7f190d573e80&gt;\n\n\n\n\n\n\ncreate_plot(data_120,1,'P25')"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#model-defination",
    "href": "publications_and_projects/data_folder/Autoencoder.html#model-defination",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Autoencoder_MLP(nn.Module):\n    def __init__(self):\n        super(Autoencoder_MLP, self).__init__()\n        \n        # Define the layers\n        self.flatten = nn.Flatten()  # Flatten the 2D input matrix\n        self.fc1 = nn.Linear(80*80, 1024)  # Fully connected layer 1\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 1024)  # Fully connected layer 2\n        self.fc4 = nn.Linear(1024, 80*80)  # Fully connected layer 2\n        self.relu = nn.ReLU()  # Activation function\n\n    def forward(self, x):\n        # Flatten the input\n        x = self.flatten(x)\n        \n        # Forward pass through the fully connected layers\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.fc4(x)\n\n        # Reshape the output to match the 2D matrix size\n        x = x.view(-1, 80, 80)\n        \n        return x\n\n\nsummary(Autoencoder_MLP(), input_size=(1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_MLP                          [1, 80, 80]               --\n├─Flatten: 1-1                           [1, 6400]                 --\n├─Linear: 1-2                            [1, 1024]                 6,554,624\n├─ReLU: 1-3                              [1, 1024]                 --\n├─Linear: 1-4                            [1, 512]                  524,800\n├─ReLU: 1-5                              [1, 512]                  --\n├─Linear: 1-6                            [1, 1024]                 525,312\n├─ReLU: 1-7                              [1, 1024]                 --\n├─Linear: 1-8                            [1, 6400]                 6,560,000\n==========================================================================================\nTotal params: 14,164,736\nTrainable params: 14,164,736\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 14.16\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 0.07\nParams size (MB): 56.66\nEstimated Total Size (MB): 56.76\n=========================================================================================="
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-single-channel-input-and-outputp25",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-single-channel-input-and-outputp25",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\nX.shape, y.shape\n\n((1656, 14, 80, 80), (1656, 2, 80, 80))\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25 = []\ny_channel = 0 \n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 405.4315\nEpoch [21/200] Loss: 214.5978\nEpoch [41/200] Loss: 215.0236\nEpoch [61/200] Loss: 207.2482\nEpoch [81/200] Loss: 206.7534\nEpoch [101/200] Loss: 213.1840\nEpoch [121/200] Loss: 207.5252\nEpoch [141/200] Loss: 208.2888\nEpoch [161/200] Loss: 209.6494\nEpoch [181/200] Loss: 209.7600\nAverage Test Loss: 190.0971\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 464.9160\nEpoch [21/200] Loss: 206.1055\nEpoch [41/200] Loss: 210.0196\nEpoch [61/200] Loss: 209.2211\nEpoch [81/200] Loss: 211.8091\nEpoch [101/200] Loss: 205.4592\nEpoch [121/200] Loss: 203.9590\nEpoch [141/200] Loss: 208.5030\nEpoch [161/200] Loss: 207.1320\nEpoch [181/200] Loss: 204.3017\nAverage Test Loss: 189.7174\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 460.0334\nEpoch [21/200] Loss: 207.1043\nEpoch [41/200] Loss: 206.5882\nEpoch [61/200] Loss: 208.3192\nEpoch [81/200] Loss: 205.4035\nEpoch [101/200] Loss: 209.7349\nEpoch [121/200] Loss: 205.5515\nEpoch [141/200] Loss: 206.1994\nEpoch [161/200] Loss: 205.4001\nEpoch [181/200] Loss: 206.3801\nAverage Test Loss: 190.0263\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 476.9623\nEpoch [21/200] Loss: 207.3108\nEpoch [41/200] Loss: 208.4855\nEpoch [61/200] Loss: 206.4735\nEpoch [81/200] Loss: 205.5895\nEpoch [101/200] Loss: 205.6536\nEpoch [121/200] Loss: 205.9680\nEpoch [141/200] Loss: 205.7129\nEpoch [161/200] Loss: 208.6799\nEpoch [181/200] Loss: 205.6486\nAverage Test Loss: 189.6717\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 599.1686\nEpoch [21/200] Loss: 454.3787\nEpoch [41/200] Loss: 149.9270\nEpoch [61/200] Loss: 23.7072\nEpoch [81/200] Loss: 7.4556\nEpoch [101/200] Loss: 18.4050\nEpoch [121/200] Loss: 3.0840\nEpoch [141/200] Loss: 100.6344\nEpoch [161/200] Loss: 25.9631\nEpoch [181/200] Loss: 38.8335\nAverage Test Loss: 9.8902\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 373.7542\nEpoch [21/200] Loss: 51.2008\nEpoch [41/200] Loss: 37.7057\nEpoch [61/200] Loss: 25.3702\nEpoch [81/200] Loss: 0.2791\nEpoch [101/200] Loss: 20.1649\nEpoch [121/200] Loss: 0.2297\nEpoch [141/200] Loss: 84.8078\nEpoch [161/200] Loss: 2.4612\nEpoch [181/200] Loss: 1.4404\nAverage Test Loss: 0.7598\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 399.2307\nEpoch [21/200] Loss: 1.1263\nEpoch [41/200] Loss: 0.0137\nEpoch [61/200] Loss: 0.0820\nEpoch [81/200] Loss: 2.3733\nEpoch [101/200] Loss: 0.0002\nEpoch [121/200] Loss: 0.0113\nEpoch [141/200] Loss: 0.0393\nEpoch [161/200] Loss: 11.0771\nEpoch [181/200] Loss: 0.0003\nAverage Test Loss: 0.0507\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 412.2657\nEpoch [21/200] Loss: 210.9129\nEpoch [41/200] Loss: 209.2686\nEpoch [61/200] Loss: 206.7048\nEpoch [81/200] Loss: 206.0187\nEpoch [101/200] Loss: 209.1361\nEpoch [121/200] Loss: 212.1444\nEpoch [141/200] Loss: 206.8286\nEpoch [161/200] Loss: 205.8083\nEpoch [181/200] Loss: 208.7421\nAverage Test Loss: 190.9819\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 485.4561\nEpoch [21/200] Loss: 210.1752\nEpoch [41/200] Loss: 207.2874\nEpoch [61/200] Loss: 207.9026\nEpoch [81/200] Loss: 206.2404\nEpoch [101/200] Loss: 205.9271\nEpoch [121/200] Loss: 205.6481\nEpoch [141/200] Loss: 208.2400\nEpoch [161/200] Loss: 205.4081\nEpoch [181/200] Loss: 205.1859\nAverage Test Loss: 199.1508\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 305.6402\nEpoch [21/200] Loss: 176.6024\nEpoch [41/200] Loss: 161.6851\nEpoch [61/200] Loss: 157.7673\nEpoch [81/200] Loss: 138.7664\nEpoch [101/200] Loss: 132.1217\nEpoch [121/200] Loss: 109.4900\nEpoch [141/200] Loss: 104.2382\nEpoch [161/200] Loss: 69.5860\nEpoch [181/200] Loss: 50.0248\nAverage Test Loss: 42.1182\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 461.0668\nEpoch [21/200] Loss: 205.2268\nEpoch [41/200] Loss: 213.0740\nEpoch [61/200] Loss: 206.9702\nEpoch [81/200] Loss: 205.7769\nEpoch [101/200] Loss: 210.3533\nEpoch [121/200] Loss: 206.2578\nEpoch [141/200] Loss: 207.2727\nEpoch [161/200] Loss: 205.3765\nEpoch [181/200] Loss: 206.2520\nAverage Test Loss: 192.2221\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 468.8439\nEpoch [21/200] Loss: 207.2851\nEpoch [41/200] Loss: 211.7429\nEpoch [61/200] Loss: 205.8536\nEpoch [81/200] Loss: 205.3124\nEpoch [101/200] Loss: 207.8155\nEpoch [121/200] Loss: 208.3488\nEpoch [141/200] Loss: 206.0120\nEpoch [161/200] Loss: 205.9442\nEpoch [181/200] Loss: 204.1215\nAverage Test Loss: 190.8858\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 287.4021\nEpoch [21/200] Loss: 209.5118\nEpoch [41/200] Loss: 209.2537\nEpoch [61/200] Loss: 206.0375\nEpoch [81/200] Loss: 207.1711\nEpoch [101/200] Loss: 205.2734\nEpoch [121/200] Loss: 210.4286\nEpoch [141/200] Loss: 207.5243\nEpoch [161/200] Loss: 208.9856\nEpoch [181/200] Loss: 204.6973\nAverage Test Loss: 191.9669\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 323.2034\nEpoch [21/200] Loss: 207.3925\nEpoch [41/200] Loss: 210.0555\nEpoch [61/200] Loss: 206.0877\nEpoch [81/200] Loss: 207.9315\nEpoch [101/200] Loss: 207.4798\nEpoch [121/200] Loss: 206.1072\nEpoch [141/200] Loss: 207.4259\nEpoch [161/200] Loss: 209.3318\nEpoch [181/200] Loss: 207.3006\nAverage Test Loss: 194.3565\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25\n\n[190.09705144708806,\n 189.7173545143821,\n 190.02632834694603,\n 189.67173489657316,\n 9.890190774744207,\n 0.7598354017192667,\n 0.050707115029746834,\n 190.98193498091265,\n 199.15079567649147,\n 42.118152445012875,\n 192.22213883833453,\n 190.88581431995738,\n 191.9668634588068,\n 194.3564910888672]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 0.050707115029746834)\n('U10_MpS', 0.7598354017192667)\n('CLOUD_OD', 9.890190774744207)\n('SOLM_M3pM3', 42.118152445012875)\n('PRATE_MMpH', 189.67173489657316)\n('SNOWEW_M', 189.7173545143821)\n('SNOWAGE_HR', 190.02632834694603)\n('TSURF_K', 190.09705144708806)\n('CAPE', 190.88581431995738)\n('T2_K', 190.98193498091265)\n('PBL_WRF_M', 191.9668634588068)\n('CLDTOP_KM', 192.22213883833453)\n('PBL_YSU_M', 194.3564910888672)\n('SWSFC_WpM2', 199.15079567649147)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'CLOUD_OD']"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\ntest_loss_list_P_10 = []\ny_channel = 1\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 689.7490\nEpoch [21/200] Loss: 418.6676\nEpoch [41/200] Loss: 410.9685\nEpoch [61/200] Loss: 404.8066\nEpoch [81/200] Loss: 400.5351\nEpoch [101/200] Loss: 401.0250\nEpoch [121/200] Loss: 401.0659\nEpoch [141/200] Loss: 403.6432\nEpoch [161/200] Loss: 402.2360\nEpoch [181/200] Loss: 407.2921\nAverage Test Loss: 401.8882\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1031.9999\nEpoch [21/200] Loss: 405.7656\nEpoch [41/200] Loss: 402.2811\nEpoch [61/200] Loss: 403.8337\nEpoch [81/200] Loss: 398.2591\nEpoch [101/200] Loss: 409.4181\nEpoch [121/200] Loss: 396.3737\nEpoch [141/200] Loss: 395.1861\nEpoch [161/200] Loss: 403.0066\nEpoch [181/200] Loss: 397.6094\nAverage Test Loss: 361.2384\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1050.1397\nEpoch [21/200] Loss: 397.1798\nEpoch [41/200] Loss: 401.0453\nEpoch [61/200] Loss: 401.7171\nEpoch [81/200] Loss: 400.2689\nEpoch [101/200] Loss: 396.7101\nEpoch [121/200] Loss: 404.5816\nEpoch [141/200] Loss: 397.7231\nEpoch [161/200] Loss: 394.6489\nEpoch [181/200] Loss: 399.8899\nAverage Test Loss: 362.0641\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1034.3143\nEpoch [21/200] Loss: 399.8861\nEpoch [41/200] Loss: 399.7876\nEpoch [61/200] Loss: 395.9112\nEpoch [81/200] Loss: 400.4803\nEpoch [101/200] Loss: 396.5552\nEpoch [121/200] Loss: 401.0467\nEpoch [141/200] Loss: 406.4987\nEpoch [161/200] Loss: 401.3759\nEpoch [181/200] Loss: 397.4671\nAverage Test Loss: 361.1263\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1214.4916\nEpoch [21/200] Loss: 725.7131\nEpoch [41/200] Loss: 220.5272\nEpoch [61/200] Loss: 23.5934\nEpoch [81/200] Loss: 8.8063\nEpoch [101/200] Loss: 274.7881\nEpoch [121/200] Loss: 218.0335\nEpoch [141/200] Loss: 232.4360\nEpoch [161/200] Loss: 212.6520\nEpoch [181/200] Loss: 195.4785\nAverage Test Loss: 180.2198\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 822.4214\nEpoch [21/200] Loss: 182.0370\nEpoch [41/200] Loss: 50.0221\nEpoch [61/200] Loss: 60.5883\nEpoch [81/200] Loss: 37.8001\nEpoch [101/200] Loss: 2.6386\nEpoch [121/200] Loss: 0.0629\nEpoch [141/200] Loss: 76.5106\nEpoch [161/200] Loss: 41.2225\nEpoch [181/200] Loss: 4.6194\nAverage Test Loss: 0.6711\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1160.5402\nEpoch [21/200] Loss: 8.7026\nEpoch [41/200] Loss: 0.3045\nEpoch [61/200] Loss: 0.1493\nEpoch [81/200] Loss: 0.1235\nEpoch [101/200] Loss: 5.4404\nEpoch [121/200] Loss: 0.0000\nEpoch [141/200] Loss: 10.2387\nEpoch [161/200] Loss: 0.2644\nEpoch [181/200] Loss: 0.0284\nAverage Test Loss: 0.6849\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 646.6076\nEpoch [21/200] Loss: 412.2161\nEpoch [41/200] Loss: 407.6458\nEpoch [61/200] Loss: 403.7684\nEpoch [81/200] Loss: 399.9570\nEpoch [101/200] Loss: 397.7386\nEpoch [121/200] Loss: 402.0990\nEpoch [141/200] Loss: 397.6674\nEpoch [161/200] Loss: 398.1612\nEpoch [181/200] Loss: 403.3343\nAverage Test Loss: 360.5232\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1027.1539\nEpoch [21/200] Loss: 403.9408\nEpoch [41/200] Loss: 401.9444\nEpoch [61/200] Loss: 399.7967\nEpoch [81/200] Loss: 401.7070\nEpoch [101/200] Loss: 404.1586\nEpoch [121/200] Loss: 398.2909\nEpoch [141/200] Loss: 396.0758\nEpoch [161/200] Loss: 396.4020\nEpoch [181/200] Loss: 402.5728\nAverage Test Loss: 362.6420\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 696.3634\nEpoch [21/200] Loss: 331.1351\nEpoch [41/200] Loss: 296.5541\nEpoch [61/200] Loss: 301.9112\nEpoch [81/200] Loss: 272.0759\nEpoch [101/200] Loss: 251.3878\nEpoch [121/200] Loss: 232.4279\nEpoch [141/200] Loss: 170.0628\nEpoch [161/200] Loss: 138.8842\nEpoch [181/200] Loss: 108.4864\nAverage Test Loss: 110.6638\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1017.6373\nEpoch [21/200] Loss: 400.4651\nEpoch [41/200] Loss: 405.2223\nEpoch [61/200] Loss: 402.9269\nEpoch [81/200] Loss: 400.7290\nEpoch [101/200] Loss: 400.5154\nEpoch [121/200] Loss: 404.1065\nEpoch [141/200] Loss: 403.8573\nEpoch [161/200] Loss: 397.0463\nEpoch [181/200] Loss: 398.1819\nAverage Test Loss: 361.3055\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1029.8392\nEpoch [21/200] Loss: 401.8889\nEpoch [41/200] Loss: 399.1993\nEpoch [61/200] Loss: 400.2419\nEpoch [81/200] Loss: 396.8849\nEpoch [101/200] Loss: 395.3639\nEpoch [121/200] Loss: 401.5739\nEpoch [141/200] Loss: 403.1417\nEpoch [161/200] Loss: 402.3704\nEpoch [181/200] Loss: 400.0658\nAverage Test Loss: 361.0597\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 536.0374\nEpoch [21/200] Loss: 401.9832\nEpoch [41/200] Loss: 402.4697\nEpoch [61/200] Loss: 404.1890\nEpoch [81/200] Loss: 400.4949\nEpoch [101/200] Loss: 399.0872\nEpoch [121/200] Loss: 398.4192\nEpoch [141/200] Loss: 400.4667\nEpoch [161/200] Loss: 398.1382\nEpoch [181/200] Loss: 408.3430\nAverage Test Loss: 360.3921\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 538.3320\nEpoch [21/200] Loss: 405.8272\nEpoch [41/200] Loss: 430.2409\nEpoch [61/200] Loss: 398.9199\nEpoch [81/200] Loss: 404.8572\nEpoch [101/200] Loss: 397.2876\nEpoch [121/200] Loss: 403.3052\nEpoch [141/200] Loss: 401.7307\nEpoch [161/200] Loss: 396.1157\nEpoch [181/200] Loss: 399.0870\nAverage Test Loss: 361.4502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10\n\n[401.88824185458094,\n 361.23838112571025,\n 362.0641340775923,\n 361.1262997713956,\n 180.2197820490057,\n 0.6710819737477736,\n 0.6848962740464644,\n 360.5232141668146,\n 362.6420177112926,\n 110.6638252951882,\n 361.3054504394531,\n 361.05967018821025,\n 360.39211342551494,\n 361.45016063343394]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 0.6710819737477736)\n('V10_MpS', 0.6848962740464644)\n('SOLM_M3pM3', 110.6638252951882)\n('CLOUD_OD', 180.2197820490057)\n('PBL_WRF_M', 360.39211342551494)\n('T2_K', 360.5232141668146)\n('CAPE', 361.05967018821025)\n('PRATE_MMpH', 361.1262997713956)\n('SNOWEW_M', 361.23838112571025)\n('CLDTOP_KM', 361.3054504394531)\n('PBL_YSU_M', 361.45016063343394)\n('SNOWAGE_HR', 362.0641340775923)\n('SWSFC_WpM2', 362.6420177112926)\n('TSURF_K', 401.88824185458094)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#insights",
    "href": "publications_and_projects/data_folder/Autoencoder.html#insights",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10, test_loss_list_P_25)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10, 'P25': test_loss_list_P_25, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_MLP_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n0.7\n0.1\n0.4\n\n\n5\nU10_MpS\n0.7\n0.8\n0.7\n\n\n9\nSOLM_M3pM3\n110.7\n42.1\n76.4\n\n\n4\nCLOUD_OD\n180.2\n9.9\n95.1\n\n\n3\nPRATE_MMpH\n361.1\n189.7\n275.4\n\n\n1\nSNOWEW_M\n361.2\n189.7\n275.5\n\n\n7\nT2_K\n360.5\n191.0\n275.8\n\n\n11\nCAPE\n361.1\n190.9\n276.0\n\n\n2\nSNOWAGE_HR\n362.1\n190.0\n276.0\n\n\n12\nPBL_WRF_M\n360.4\n192.0\n276.2\n\n\n10\nCLDTOP_KM\n361.3\n192.2\n276.8\n\n\n13\nPBL_YSU_M\n361.5\n194.4\n277.9\n\n\n8\nSWSFC_WpM2\n362.6\n199.2\n280.9\n\n\n0\nTSURF_K\n401.9\n190.1\n296.0"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#model-defination-1",
    "href": "publications_and_projects/data_folder/Autoencoder.html#model-defination-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__() \n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid \n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential( \n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        ) \n        \n    def forward(self, x):\n        return self.net(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n\n        )\n    def forward(self, x):\n        return self.decoder(x)\n\nclass Autoencoder_CNN(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_CNN, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim \n        self.activation = activation    \n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        \n    \n    def forward(self, x):\n        x = self.encoder(x)\n        # print(x.shape)\n        x = self.decoder(x)\n        return x\n\n\nmodel_auto = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_CNN(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 1024]              --\n│    └─Sequential: 2-1                   [1656, 1024]              --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 6400]       6,560,000\n        Unflatten-15           [-1, 64, 10, 10]               0\n  ConvTranspose2d-16           [-1, 32, 20, 20]          18,464\n             GELU-17           [-1, 32, 20, 20]               0\n           Conv2d-18           [-1, 32, 20, 20]           9,248\n             GELU-19           [-1, 32, 20, 20]               0\n  ConvTranspose2d-20           [-1, 16, 40, 40]           4,624\n             GELU-21           [-1, 16, 40, 40]               0\n           Conv2d-22           [-1, 16, 40, 40]           2,320\n             GELU-23           [-1, 16, 40, 40]               0\n  ConvTranspose2d-24            [-1, 2, 80, 80]             290\n          Decoder-25            [-1, 2, 80, 80]               0\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.80\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.44\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "training on all channel and predicting on all channel",
    "text": "training on all channel and predicting on all channel\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 50 #200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 10 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/50] Loss: 1620.3361\nAverage Test Loss: 377.9475\nEpoch [11/50] Loss: 249.2443\nAverage Test Loss: 228.7919\nEpoch [21/50] Loss: 168.6332\nAverage Test Loss: 172.4825\nEpoch [31/50] Loss: 102.0034\nAverage Test Loss: 111.1758\nEpoch [41/50] Loss: 65.1520\nAverage Test Loss: 60.7862\n\n\n  2%|▏         | 1/50 [00:04&lt;04:03,  4.96s/it] 22%|██▏       | 11/50 [00:58&lt;03:15,  5.00s/it] 42%|████▏     | 21/50 [01:33&lt;01:40,  3.45s/it] 62%|██████▏   | 31/50 [02:21&lt;01:14,  3.90s/it] 80%|████████  | 40/50 [02:49&lt;00:32,  3.22s/it] 82%|████████▏ | 41/50 [02:53&lt;00:30,  3.39s/it]100%|██████████| 50/50 [03:30&lt;00:00,  4.22s/it]\n\n\n\n\n\n\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nAverage Test Loss: 20.4597\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in14_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "href": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output (P25)",
    "text": "Single channel input single channel output (P25)\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto = []\ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 462.4725\nEpoch [21/200] Loss: 212.0043\nEpoch [41/200] Loss: 213.3173\nEpoch [61/200] Loss: 234.0997\nEpoch [81/200] Loss: 211.7991\nEpoch [101/200] Loss: 216.9750\nEpoch [121/200] Loss: 211.1435\nEpoch [141/200] Loss: 214.2518\nEpoch [161/200] Loss: 209.3667\nEpoch [181/200] Loss: 208.2786\nAverage Test Loss: 196.8076\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 540.4606\nEpoch [21/200] Loss: 216.1673\nEpoch [41/200] Loss: 211.7699\nEpoch [61/200] Loss: 211.9133\nEpoch [81/200] Loss: 214.4790\nEpoch [101/200] Loss: 212.5737\nEpoch [121/200] Loss: 212.8330\nEpoch [141/200] Loss: 209.0824\nEpoch [161/200] Loss: 207.7827\nEpoch [181/200] Loss: 208.2993\nAverage Test Loss: 191.6095\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 590.1837\nEpoch [21/200] Loss: 211.4096\nEpoch [41/200] Loss: 213.1281\nEpoch [61/200] Loss: 208.5330\nEpoch [81/200] Loss: 209.0068\nEpoch [101/200] Loss: 211.9243\nEpoch [121/200] Loss: 212.2428\nEpoch [141/200] Loss: 210.3188\nEpoch [161/200] Loss: 211.6509\nEpoch [181/200] Loss: 212.8352\nAverage Test Loss: 191.7548\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 750.4891\nEpoch [21/200] Loss: 213.9414\nEpoch [41/200] Loss: 214.3703\nEpoch [61/200] Loss: 209.9882\nEpoch [81/200] Loss: 210.4570\nEpoch [101/200] Loss: 211.3801\nEpoch [121/200] Loss: 214.6792\nEpoch [141/200] Loss: 210.8055\nEpoch [161/200] Loss: 214.0414\nEpoch [181/200] Loss: 206.5140\nAverage Test Loss: 194.9402\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 563.3871\nEpoch [21/200] Loss: 63.2729\nEpoch [41/200] Loss: 23.8580\nEpoch [61/200] Loss: 15.2393\nEpoch [81/200] Loss: 11.0129\nEpoch [101/200] Loss: 9.3018\nEpoch [121/200] Loss: 8.1090\nEpoch [141/200] Loss: 7.4336\nEpoch [161/200] Loss: 8.8601\nEpoch [181/200] Loss: 6.1420\nAverage Test Loss: 7.4988\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 684.8967\nEpoch [21/200] Loss: 111.6166\nEpoch [41/200] Loss: 22.6558\nEpoch [61/200] Loss: 10.9635\nEpoch [81/200] Loss: 12.9050\nEpoch [101/200] Loss: 6.2923\nEpoch [121/200] Loss: 9.0327\nEpoch [141/200] Loss: 4.1764\nEpoch [161/200] Loss: 3.8160\nEpoch [181/200] Loss: 3.3957\nAverage Test Loss: 3.1739\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 609.8626\nEpoch [21/200] Loss: 58.6357\nEpoch [41/200] Loss: 14.3107\nEpoch [61/200] Loss: 10.6218\nEpoch [81/200] Loss: 6.9871\nEpoch [101/200] Loss: 5.3444\nEpoch [121/200] Loss: 4.6127\nEpoch [141/200] Loss: 4.0642\nEpoch [161/200] Loss: 3.5636\nEpoch [181/200] Loss: 3.2190\nAverage Test Loss: 2.9050\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 458.4550\nEpoch [21/200] Loss: 213.3441\nEpoch [41/200] Loss: 211.7652\nEpoch [61/200] Loss: 212.4970\nEpoch [81/200] Loss: 210.7104\nEpoch [101/200] Loss: 208.7177\nEpoch [121/200] Loss: 212.2414\nEpoch [141/200] Loss: 207.2175\nEpoch [161/200] Loss: 211.3121\nEpoch [181/200] Loss: 208.7926\nAverage Test Loss: 191.9081\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 760.0155\nEpoch [21/200] Loss: 224.7744\nEpoch [41/200] Loss: 215.4670\nEpoch [61/200] Loss: 214.6593\nEpoch [81/200] Loss: 211.3864\nEpoch [101/200] Loss: 214.5846\nEpoch [121/200] Loss: 215.2816\nEpoch [141/200] Loss: 210.3355\nEpoch [161/200] Loss: 213.1056\nEpoch [181/200] Loss: 209.3009\nAverage Test Loss: 193.2200\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 518.8583\nEpoch [21/200] Loss: 183.2842\nEpoch [41/200] Loss: 106.5493\nEpoch [61/200] Loss: 47.8360\nEpoch [81/200] Loss: 17.4837\nEpoch [101/200] Loss: 12.6859\nEpoch [121/200] Loss: 9.7521\nEpoch [141/200] Loss: 8.4768\nEpoch [161/200] Loss: 7.1218\nEpoch [181/200] Loss: 7.4880\nAverage Test Loss: 5.5514\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 603.6223\nEpoch [21/200] Loss: 216.0129\nEpoch [41/200] Loss: 212.5011\nEpoch [61/200] Loss: 215.2624\nEpoch [81/200] Loss: 210.4688\nEpoch [101/200] Loss: 211.2793\nEpoch [121/200] Loss: 208.5935\nEpoch [141/200] Loss: 209.1620\nEpoch [161/200] Loss: 208.9249\nEpoch [181/200] Loss: 207.8753\nAverage Test Loss: 205.4745\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 480.7086\nEpoch [21/200] Loss: 215.4105\nEpoch [41/200] Loss: 211.5271\nEpoch [61/200] Loss: 211.8129\nEpoch [81/200] Loss: 208.5153\nEpoch [101/200] Loss: 208.6479\nEpoch [121/200] Loss: 215.5420\nEpoch [141/200] Loss: 207.9001\nEpoch [161/200] Loss: 206.7139\nEpoch [181/200] Loss: 208.8671\nAverage Test Loss: 191.5815\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 487.6684\nEpoch [21/200] Loss: 210.6665\nEpoch [41/200] Loss: 211.8588\nEpoch [61/200] Loss: 209.3883\nEpoch [81/200] Loss: 210.8873\nEpoch [101/200] Loss: 209.3165\nEpoch [121/200] Loss: 208.2383\nEpoch [141/200] Loss: 208.4831\nEpoch [161/200] Loss: 209.2067\nEpoch [181/200] Loss: 207.0104\nAverage Test Loss: 190.8408\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 538.0652\nEpoch [21/200] Loss: 213.2395\nEpoch [41/200] Loss: 209.5626\nEpoch [61/200] Loss: 218.8836\nEpoch [81/200] Loss: 213.1157\nEpoch [101/200] Loss: 211.7407\nEpoch [121/200] Loss: 214.4044\nEpoch [141/200] Loss: 209.6131\nEpoch [161/200] Loss: 209.1906\nEpoch [181/200] Loss: 207.2547\nAverage Test Loss: 196.5285\n\n\n  0%|          | 1/200 [00:00&lt;02:14,  1.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:24&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.58it/s] 50%|█████     | 101/200 [00:28&lt;00:27,  3.61it/s] 60%|██████    | 121/200 [00:34&lt;00:21,  3.60it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.48it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.43it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.47it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.37it/s] 60%|██████    | 121/200 [00:35&lt;00:21,  3.76it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.42it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.61it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.63it/s] 10%|█         | 21/200 [00:05&lt;00:46,  3.82it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:37,  3.14it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.40it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.49it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.52it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.57it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.46it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.34it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.37it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.45it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.52it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.32it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.49it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.42it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.10it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:36&lt;00:23,  3.31it/s] 70%|███████   | 141/200 [00:42&lt;00:17,  3.30it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.46it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.18it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.42it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.33it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.36it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.35it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.37it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.44it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto\n\n[196.80755892666903,\n 191.6094970703125,\n 191.75477183948863,\n 194.94017861106178,\n 7.4987756772474805,\n 3.1738592061129483,\n 2.9050224044106225,\n 191.90811157226562,\n 193.21996238014916,\n 5.551396109841087,\n 205.47452198375356,\n 191.58150828968394,\n 190.84084944291547,\n 196.5285311612216]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 2.9050224044106225)\n('U10_MpS', 3.1738592061129483)\n('SOLM_M3pM3', 5.551396109841087)\n('CLOUD_OD', 7.4987756772474805)\n('PBL_WRF_M', 190.84084944291547)\n('CAPE', 191.58150828968394)\n('SNOWEW_M', 191.6094970703125)\n('SNOWAGE_HR', 191.75477183948863)\n('T2_K', 191.90811157226562)\n('SWSFC_WpM2', 193.21996238014916)\n('PRATE_MMpH', 194.94017861106178)\n('PBL_YSU_M', 196.5285311612216)\n('TSURF_K', 196.80755892666903)\n('CLDTOP_KM', 205.47452198375356)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1000, 1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1000, 1, 80, 80]         --\n├─Encoder: 1-1                           [1000, 512]               --\n│    └─Sequential: 2-1                   [1000, 512]               --\n│    │    └─Conv2d: 3-1                  [1000, 8, 40, 40]         80\n│    │    └─GELU: 3-2                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-3                  [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-4                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-5                  [1000, 16, 20, 20]        1,168\n│    │    └─GELU: 3-6                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-8                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1000, 32, 10, 10]        4,640\n│    │    └─GELU: 3-10                   [1000, 32, 10, 10]        --\n│    │    └─Flatten: 3-11                [1000, 3200]              --\n│    │    └─Linear: 3-12                 [1000, 512]               1,638,912\n├─Decoder: 1-2                           [1000, 1, 80, 80]         --\n│    └─Sequential: 2-2                   [1000, 1, 80, 80]         --\n│    │    └─Linear: 3-13                 [1000, 3200]              1,641,600\n│    │    └─Unflatten: 3-14              [1000, 32, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1000, 16, 20, 20]        4,624\n│    │    └─GELU: 3-16                   [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-18                   [1000, 16, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1000, 8, 40, 40]         1,160\n│    │    └─GELU: 3-20                   [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-21                 [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-22                   [1000, 8, 40, 40]         --\n│    │    └─ConvTranspose2d: 3-23        [1000, 1, 80, 80]         73\n==========================================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 12.24\n==========================================================================================\nInput size (MB): 25.60\nForward/backward pass size (MB): 720.90\nParams size (MB): 13.19\nEstimated Total Size (MB): 759.69\n=========================================================================================="
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "href": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input multiple channel output (P10)",
    "text": "Single channel input multiple channel output (P10)\n\ntest_loss_list_P_10_conv_auto = []\ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 831.9463\nEpoch [21/200] Loss: 413.6695\nEpoch [41/200] Loss: 416.1795\nEpoch [61/200] Loss: 414.0692\nEpoch [81/200] Loss: 408.8671\nEpoch [101/200] Loss: 408.6212\nEpoch [121/200] Loss: 408.1124\nEpoch [141/200] Loss: 407.2696\nEpoch [161/200] Loss: 419.0648\nEpoch [181/200] Loss: 411.9767\nAverage Test Loss: 367.9784\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1193.0016\nEpoch [21/200] Loss: 413.8656\nEpoch [41/200] Loss: 416.9739\nEpoch [61/200] Loss: 405.7197\nEpoch [81/200] Loss: 414.4462\nEpoch [101/200] Loss: 403.9555\nEpoch [121/200] Loss: 406.6566\nEpoch [141/200] Loss: 404.8177\nEpoch [161/200] Loss: 414.5372\nEpoch [181/200] Loss: 418.9334\nAverage Test Loss: 373.6601\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.4868\nEpoch [21/200] Loss: 410.2950\nEpoch [41/200] Loss: 414.8993\nEpoch [61/200] Loss: 405.8627\nEpoch [81/200] Loss: 416.4907\nEpoch [101/200] Loss: 413.8539\nEpoch [121/200] Loss: 406.4541\nEpoch [141/200] Loss: 404.2276\nEpoch [161/200] Loss: 407.0577\nEpoch [181/200] Loss: 402.6227\nAverage Test Loss: 393.0328\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1028.3382\nEpoch [21/200] Loss: 434.1988\nEpoch [41/200] Loss: 412.9816\nEpoch [61/200] Loss: 405.9959\nEpoch [81/200] Loss: 404.1220\nEpoch [101/200] Loss: 415.6019\nEpoch [121/200] Loss: 403.8031\nEpoch [141/200] Loss: 407.2635\nEpoch [161/200] Loss: 404.3506\nEpoch [181/200] Loss: 409.8351\nAverage Test Loss: 367.3246\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1599.0141\nEpoch [21/200] Loss: 192.7198\nEpoch [41/200] Loss: 68.0225\nEpoch [61/200] Loss: 44.4028\nEpoch [81/200] Loss: 32.4964\nEpoch [101/200] Loss: 25.7723\nEpoch [121/200] Loss: 23.6108\nEpoch [141/200] Loss: 19.3957\nEpoch [161/200] Loss: 19.0788\nEpoch [181/200] Loss: 21.9598\nAverage Test Loss: 18.7717\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1023.2784\nEpoch [21/200] Loss: 280.1561\nEpoch [41/200] Loss: 60.3793\nEpoch [61/200] Loss: 29.8831\nEpoch [81/200] Loss: 19.7393\nEpoch [101/200] Loss: 16.8795\nEpoch [121/200] Loss: 12.9342\nEpoch [141/200] Loss: 11.2801\nEpoch [161/200] Loss: 10.1249\nEpoch [181/200] Loss: 11.3411\nAverage Test Loss: 8.8163\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1439.9822\nEpoch [21/200] Loss: 142.6731\nEpoch [41/200] Loss: 39.2041\nEpoch [61/200] Loss: 20.6722\nEpoch [81/200] Loss: 16.5665\nEpoch [101/200] Loss: 12.4099\nEpoch [121/200] Loss: 10.8300\nEpoch [141/200] Loss: 9.6913\nEpoch [161/200] Loss: 8.9197\nEpoch [181/200] Loss: 10.7642\nAverage Test Loss: 23.4499\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1037.4250\nEpoch [21/200] Loss: 413.2349\nEpoch [41/200] Loss: 411.9822\nEpoch [61/200] Loss: 406.0331\nEpoch [81/200] Loss: 408.5417\nEpoch [101/200] Loss: 403.6246\nEpoch [121/200] Loss: 409.9414\nEpoch [141/200] Loss: 402.6003\nEpoch [161/200] Loss: 401.2209\nEpoch [181/200] Loss: 403.0725\nAverage Test Loss: 366.0120\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1358.9732\nEpoch [21/200] Loss: 419.2162\nEpoch [41/200] Loss: 413.3607\nEpoch [61/200] Loss: 409.3754\nEpoch [81/200] Loss: 411.0923\nEpoch [101/200] Loss: 404.1099\nEpoch [121/200] Loss: 409.2167\nEpoch [141/200] Loss: 408.5509\nEpoch [161/200] Loss: 410.1073\nEpoch [181/200] Loss: 407.5864\nAverage Test Loss: 364.5863\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1237.3417\nEpoch [21/200] Loss: 337.7638\nEpoch [41/200] Loss: 223.9836\nEpoch [61/200] Loss: 98.1059\nEpoch [81/200] Loss: 46.6351\nEpoch [101/200] Loss: 31.9773\nEpoch [121/200] Loss: 26.2758\nEpoch [141/200] Loss: 19.9056\nEpoch [161/200] Loss: 17.7226\nEpoch [181/200] Loss: 15.4444\nAverage Test Loss: 14.0181\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1227.4362\nEpoch [21/200] Loss: 430.1239\nEpoch [41/200] Loss: 414.0475\nEpoch [61/200] Loss: 414.5093\nEpoch [81/200] Loss: 412.3910\nEpoch [101/200] Loss: 413.5266\nEpoch [121/200] Loss: 408.5565\nEpoch [141/200] Loss: 404.2547\nEpoch [161/200] Loss: 417.1860\nEpoch [181/200] Loss: 408.9205\nAverage Test Loss: 367.3491\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1362.6006\nEpoch [21/200] Loss: 417.5348\nEpoch [41/200] Loss: 409.7356\nEpoch [61/200] Loss: 411.1969\nEpoch [81/200] Loss: 413.3693\nEpoch [101/200] Loss: 414.5004\nEpoch [121/200] Loss: 405.8445\nEpoch [141/200] Loss: 404.8460\nEpoch [161/200] Loss: 415.0352\nEpoch [181/200] Loss: 414.2297\nAverage Test Loss: 366.4497\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1069.1285\nEpoch [21/200] Loss: 408.2721\nEpoch [41/200] Loss: 408.3345\nEpoch [61/200] Loss: 411.1250\nEpoch [81/200] Loss: 406.6349\nEpoch [101/200] Loss: 409.1261\nEpoch [121/200] Loss: 403.4630\nEpoch [141/200] Loss: 405.4774\nEpoch [161/200] Loss: 403.7881\nEpoch [181/200] Loss: 408.1804\nAverage Test Loss: 367.3865\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1065.3006\nEpoch [21/200] Loss: 443.1136\nEpoch [41/200] Loss: 407.0351\nEpoch [61/200] Loss: 409.3611\nEpoch [81/200] Loss: 403.8964\nEpoch [101/200] Loss: 402.2005\nEpoch [121/200] Loss: 402.1708\nEpoch [141/200] Loss: 403.3740\nEpoch [161/200] Loss: 398.9786\nEpoch [181/200] Loss: 408.3548\nAverage Test Loss: 380.6446\n\n\n  0%|          | 1/200 [00:00&lt;01:02,  3.18it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.38it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.36it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.19it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.36it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.41it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.36it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.37it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:53,  3.73it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.44it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.35it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.35it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.37it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:10,  3.56it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.53it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.50it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:38,  3.66it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.51it/s] 50%|█████     | 101/200 [00:28&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:34&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.41it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.41it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:52,  3.82it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.45it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.46it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.36it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.34it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.34it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.46it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.30it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.50it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.43it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.55it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.52it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.46it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.45it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.44it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto\n\n[367.97836858575994,\n 373.660117409446,\n 393.0328174937855,\n 367.32460992986506,\n 18.771740219809793,\n 8.816288991407914,\n 23.44990175420588,\n 366.0119906338778,\n 364.58628151633525,\n 14.018102992664684,\n 367.34911554509944,\n 366.4497375488281,\n 367.38653841885656,\n 380.64462835138494]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 8.816288991407914)\n('SOLM_M3pM3', 14.018102992664684)\n('CLOUD_OD', 18.771740219809793)\n('V10_MpS', 23.44990175420588)\n('SWSFC_WpM2', 364.58628151633525)\n('T2_K', 366.0119906338778)\n('CAPE', 366.4497375488281)\n('PRATE_MMpH', 367.32460992986506)\n('CLDTOP_KM', 367.34911554509944)\n('PBL_WRF_M', 367.38653841885656)\n('TSURF_K', 367.97836858575994)\n('SNOWEW_M', 373.660117409446)\n('PBL_YSU_M', 380.64462835138494)\n('SNOWAGE_HR', 393.0328174937855)\nLowest 3 names: ['U10_MpS', 'SOLM_M3pM3', 'CLOUD_OD']"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#insights-1",
    "href": "publications_and_projects/data_folder/Autoencoder.html#insights-1",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto, test_loss_list_P_25_conv_auto)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto, 'P25': test_loss_list_P_25_conv_auto, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n5\nU10_MpS\n8.8\n3.2\n6.0\n\n\n9\nSOLM_M3pM3\n14.0\n5.6\n9.8\n\n\n4\nCLOUD_OD\n18.8\n7.5\n13.1\n\n\n6\nV10_MpS\n23.4\n2.9\n13.2\n\n\n8\nSWSFC_WpM2\n364.6\n193.2\n278.9\n\n\n7\nT2_K\n366.0\n191.9\n279.0\n\n\n11\nCAPE\n366.4\n191.6\n279.0\n\n\n12\nPBL_WRF_M\n367.4\n190.8\n279.1\n\n\n3\nPRATE_MMpH\n367.3\n194.9\n281.1\n\n\n0\nTSURF_K\n368.0\n196.8\n282.4\n\n\n1\nSNOWEW_M\n373.7\n191.6\n282.6\n\n\n10\nCLDTOP_KM\n367.3\n205.5\n286.4\n\n\n13\nPBL_YSU_M\n380.6\n196.5\n288.6\n\n\n2\nSNOWAGE_HR\n393.0\n191.8\n292.4"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on top 4 channel and predicting on all channel CONV",
    "text": "Training on top 4 channel and predicting on all channel CONV\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 80\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/80] Loss: 1343.9761\nAverage Test Loss: 313.2759\nEpoch [21/80] Loss: 51.4848\nAverage Test Loss: 41.3688\nEpoch [41/80] Loss: 19.3960\nAverage Test Loss: 15.7720\nEpoch [61/80] Loss: 10.1627\nAverage Test Loss: 6.6679\nAverage Test Loss: 3.3631\n\n\n  1%|▏         | 1/80 [00:02&lt;02:40,  2.03s/it] 26%|██▋       | 21/80 [00:35&lt;01:40,  1.71s/it] 51%|█████▏    | 41/80 [01:09&lt;01:07,  1.72s/it] 76%|███████▋  | 61/80 [01:43&lt;00:32,  1.71s/it]100%|██████████| 80/80 [02:15&lt;00:00,  1.70s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in4_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#model-defination-2",
    "href": "publications_and_projects/data_folder/Autoencoder.html#model-defination-2",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Model defination",
    "text": "Model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n        )\n    def forward(self, x):\n        return self.decoder(x)\n    \nclass Autoencoder_UNET(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_UNET, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n    # def forward(self, x):\n    #     x = self.encoder(x)\n    #     # print(x.shape)\n    #     x = self.decoder(x)\n    #     return x\n    def forward(self, x):\n        conv1 = self.encoder.net[0]\n        activation1 = self.encoder.net[1]\n        conv2 = self.encoder.net[2]\n        activation2 = self.encoder.net[3]\n        conv3 = self.encoder.net[4]\n        activation3 = self.encoder.net[5]\n        conv4 = self.encoder.net[6]\n        activation4 = self.encoder.net[7]\n        conv5 = self.encoder.net[8]\n        activation5 = self.encoder.net[9]\n        flatten = self.encoder.net[10]\n        linear = self.encoder.net[11]\n        lineart = self.decoder.decoder[0]\n        unflattent = self.decoder.decoder[1]\n        convt2 = self.decoder.decoder[2]\n        activation7 = self.decoder.decoder[3]\n        convt3 = self.decoder.decoder[4]\n        activation8 = self.decoder.decoder[5]\n        convt4 = self.decoder.decoder[6]\n        activation9 = self.decoder.decoder[7]\n        convt5 = self.decoder.decoder[8]\n        activation10 = self.decoder.decoder[9]\n        convt6 = self.decoder.decoder[10]\n        x1 = activation1(conv1(x))\n        x2 = activation2(conv2(x1))\n        x3 = activation3(conv3(x2))\n        x4 = activation4(conv4(x3))\n        x5 = activation5(conv5(x4))\n        x6 = flatten(x5)\n        x7 = linear(x6)\n        x8 = lineart(x7)\n        x9 = unflattent(x8)\n        x10 = activation7(convt2(x9+x5))\n        x11 = activation8(convt3(x10+x4))\n        x12 = activation9(convt4(x11+x3))\n        x13 = activation10(convt5(x12+x2))\n        x13 = convt6(x13+x1)\n        return x13\n\n\nmodel_auto = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_UNET(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n           Linear-13                 [-1, 6400]       6,560,000\n        Unflatten-14           [-1, 64, 10, 10]               0\n  ConvTranspose2d-15           [-1, 32, 20, 20]          18,464\n             GELU-16           [-1, 32, 20, 20]               0\n           Conv2d-17           [-1, 32, 20, 20]           9,248\n             GELU-18           [-1, 32, 20, 20]               0\n  ConvTranspose2d-19           [-1, 16, 40, 40]           4,624\n             GELU-20           [-1, 16, 40, 40]               0\n           Conv2d-21           [-1, 16, 40, 40]           2,320\n             GELU-22           [-1, 16, 40, 40]               0\n  ConvTranspose2d-23            [-1, 2, 80, 80]             290\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.69\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.34\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on all channel and predicting on all channel UNET",
    "text": "Training on all channel and predicting on all channel UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/200] Loss: 3185.9086\nEpoch [21/200] Loss: 238.7220\nEpoch [41/200] Loss: 90.1479\nEpoch [61/200] Loss: 19.3867\nEpoch [81/200] Loss: 7.6679\nEpoch [101/200] Loss: 5.9376\nEpoch [121/200] Loss: 7.9944\nEpoch [141/200] Loss: 5.3282\nEpoch [161/200] Loss: 4.2981\nEpoch [181/200] Loss: 4.3096\nAverage Test Loss: 3.9575\n\n\n  0%|          | 1/200 [00:03&lt;11:37,  3.50s/it] 10%|█         | 21/200 [01:08&lt;10:15,  3.44s/it] 20%|██        | 41/200 [02:19&lt;09:16,  3.50s/it] 30%|███       | 61/200 [03:44&lt;09:57,  4.30s/it] 40%|████      | 81/200 [05:06&lt;08:00,  4.04s/it] 50%|█████     | 101/200 [06:30&lt;07:00,  4.24s/it] 60%|██████    | 121/200 [07:57&lt;05:25,  4.12s/it] 70%|███████   | 141/200 [09:21&lt;03:59,  4.06s/it] 80%|████████  | 161/200 [10:38&lt;02:34,  3.97s/it] 90%|█████████ | 181/200 [11:58&lt;01:13,  3.87s/it]100%|██████████| 200/200 [13:15&lt;00:00,  3.98s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in14_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "href": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output(P25) UNET",
    "text": "Single channel input single channel output(P25) UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto_unet = [] \ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 2015.0104\nEpoch [21/200] Loss: 224.4622\nEpoch [41/200] Loss: 217.3994\nEpoch [61/200] Loss: 213.6307\nEpoch [81/200] Loss: 211.9555\nEpoch [101/200] Loss: 212.0086\nEpoch [121/200] Loss: 210.0710\nEpoch [141/200] Loss: 216.9347\nEpoch [161/200] Loss: 214.3995\nEpoch [181/200] Loss: 209.9813\nAverage Test Loss: 192.8667\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 720.3883\nEpoch [21/200] Loss: 215.2669\nEpoch [41/200] Loss: 212.9412\nEpoch [61/200] Loss: 210.6425\nEpoch [81/200] Loss: 209.0133\nEpoch [101/200] Loss: 216.0626\nEpoch [121/200] Loss: 209.8225\nEpoch [141/200] Loss: 209.2404\nEpoch [161/200] Loss: 212.0243\nEpoch [181/200] Loss: 208.9202\nAverage Test Loss: 191.8743\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 656.6864\nEpoch [21/200] Loss: 215.9682\nEpoch [41/200] Loss: 211.3083\nEpoch [61/200] Loss: 213.7716\nEpoch [81/200] Loss: 210.4161\nEpoch [101/200] Loss: 208.3461\nEpoch [121/200] Loss: 210.5009\nEpoch [141/200] Loss: 210.7708\nEpoch [161/200] Loss: 208.3222\nEpoch [181/200] Loss: 208.7196\nAverage Test Loss: 198.2665\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 598.2960\nEpoch [21/200] Loss: 227.3699\nEpoch [41/200] Loss: 219.5535\nEpoch [61/200] Loss: 210.1886\nEpoch [81/200] Loss: 210.2708\nEpoch [101/200] Loss: 209.0613\nEpoch [121/200] Loss: 208.2215\nEpoch [141/200] Loss: 207.7972\nEpoch [161/200] Loss: 213.5013\nEpoch [181/200] Loss: 208.5548\nAverage Test Loss: 199.3592\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 586.9568\nEpoch [21/200] Loss: 62.2030\nEpoch [41/200] Loss: 21.8947\nEpoch [61/200] Loss: 13.4883\nEpoch [81/200] Loss: 13.7934\nEpoch [101/200] Loss: 8.8402\nEpoch [121/200] Loss: 8.3218\nEpoch [141/200] Loss: 8.3719\nEpoch [161/200] Loss: 7.2083\nEpoch [181/200] Loss: 6.5652\nAverage Test Loss: 7.9540\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 473.9317\nEpoch [21/200] Loss: 69.0522\nEpoch [41/200] Loss: 18.4941\nEpoch [61/200] Loss: 10.7839\nEpoch [81/200] Loss: 7.7059\nEpoch [101/200] Loss: 6.4899\nEpoch [121/200] Loss: 5.4837\nEpoch [141/200] Loss: 7.5561\nEpoch [161/200] Loss: 3.8554\nEpoch [181/200] Loss: 4.0530\nAverage Test Loss: 3.1830\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 561.4590\nEpoch [21/200] Loss: 71.1724\nEpoch [41/200] Loss: 19.3733\nEpoch [61/200] Loss: 9.0035\nEpoch [81/200] Loss: 7.7880\nEpoch [101/200] Loss: 5.6728\nEpoch [121/200] Loss: 8.2339\nEpoch [141/200] Loss: 3.8740\nEpoch [161/200] Loss: 4.9030\nEpoch [181/200] Loss: 4.0399\nAverage Test Loss: 3.4703\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 4816.9447\nEpoch [21/200] Loss: 234.2117\nEpoch [41/200] Loss: 222.6848\nEpoch [61/200] Loss: 215.6618\nEpoch [81/200] Loss: 213.7606\nEpoch [101/200] Loss: 213.5892\nEpoch [121/200] Loss: 211.2940\nEpoch [141/200] Loss: 213.9061\nEpoch [161/200] Loss: 214.1695\nEpoch [181/200] Loss: 208.9214\nAverage Test Loss: 192.4815\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 512.7773\nEpoch [21/200] Loss: 214.1751\nEpoch [41/200] Loss: 212.7058\nEpoch [61/200] Loss: 214.4303\nEpoch [81/200] Loss: 212.5624\nEpoch [101/200] Loss: 208.8632\nEpoch [121/200] Loss: 210.4221\nEpoch [141/200] Loss: 209.8110\nEpoch [161/200] Loss: 207.3880\nEpoch [181/200] Loss: 207.7816\nAverage Test Loss: 191.0656\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 574.8718\nEpoch [21/200] Loss: 181.5824\nEpoch [41/200] Loss: 115.5675\nEpoch [61/200] Loss: 39.5182\nEpoch [81/200] Loss: 16.1863\nEpoch [101/200] Loss: 11.7081\nEpoch [121/200] Loss: 8.9398\nEpoch [141/200] Loss: 8.2249\nEpoch [161/200] Loss: 6.6518\nEpoch [181/200] Loss: 5.9480\nAverage Test Loss: 5.3015\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 647.1522\nEpoch [21/200] Loss: 215.3608\nEpoch [41/200] Loss: 211.6647\nEpoch [61/200] Loss: 209.6907\nEpoch [81/200] Loss: 211.7445\nEpoch [101/200] Loss: 207.4063\nEpoch [121/200] Loss: 207.9890\nEpoch [141/200] Loss: 207.9899\nEpoch [161/200] Loss: 209.2992\nEpoch [181/200] Loss: 213.8687\nAverage Test Loss: 191.1090\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 589.8784\nEpoch [21/200] Loss: 213.6897\nEpoch [41/200] Loss: 215.2983\nEpoch [61/200] Loss: 216.7662\nEpoch [81/200] Loss: 213.0695\nEpoch [101/200] Loss: 214.7085\nEpoch [121/200] Loss: 213.2036\nEpoch [141/200] Loss: 212.2645\nEpoch [161/200] Loss: 207.4999\nEpoch [181/200] Loss: 213.0907\nAverage Test Loss: 191.7055\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 613.1753\nEpoch [21/200] Loss: 215.8198\nEpoch [41/200] Loss: 211.0306\nEpoch [61/200] Loss: 210.1862\nEpoch [81/200] Loss: 208.5808\nEpoch [101/200] Loss: 208.4701\nEpoch [121/200] Loss: 209.8240\nEpoch [141/200] Loss: 208.0958\nEpoch [161/200] Loss: 209.3367\nEpoch [181/200] Loss: 207.0369\nAverage Test Loss: 192.7273\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 419.9077\nEpoch [21/200] Loss: 210.7698\nEpoch [41/200] Loss: 208.6584\nEpoch [61/200] Loss: 210.8284\nEpoch [81/200] Loss: 208.8175\nEpoch [101/200] Loss: 206.5321\nEpoch [121/200] Loss: 207.8562\nEpoch [141/200] Loss: 207.3433\nEpoch [161/200] Loss: 206.2741\nEpoch [181/200] Loss: 207.5728\nAverage Test Loss: 194.3204\n\n\n  0%|          | 0/200 [00:00&lt;?, ?it/s]  0%|          | 1/200 [00:00&lt;01:04,  3.09it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.11it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.20it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.18it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.19it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.26it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.15it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.26it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.22it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.17it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:11,  3.26it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.17it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.18it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.14it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.14it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.18it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.25it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.16it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.25it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.33it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.30it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.21it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.27it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.29it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.23it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.23it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.22it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.22it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.15it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.23it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.26it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.34it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.28it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.22it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.26it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.25it/s] 40%|████      | 81/200 [00:24&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.26it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.24it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.19it/s] 30%|███       | 61/200 [00:19&lt;00:44,  3.10it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.17it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.20it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.30it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.14it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:35,  3.31it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.30it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.34it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.23it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.27it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:38,  3.11it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.18it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.19it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:57&lt;00:06,  3.01it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto_unet\n\n[192.86672002618963,\n 191.87428977272728,\n 198.26652665571734,\n 199.35923628373578,\n 7.954025506973267,\n 3.18301400271329,\n 3.4703480113636362,\n 192.4814786044034,\n 191.06564331054688,\n 5.301476088437167,\n 191.10903930664062,\n 191.70547346635297,\n 192.72732405229047,\n 194.32039434259588]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 3.18301400271329)\n('V10_MpS', 3.4703480113636362)\n('SOLM_M3pM3', 5.301476088437167)\n('CLOUD_OD', 7.954025506973267)\n('SWSFC_WpM2', 191.06564331054688)\n('CLDTOP_KM', 191.10903930664062)\n('CAPE', 191.70547346635297)\n('SNOWEW_M', 191.87428977272728)\n('T2_K', 192.4814786044034)\n('PBL_WRF_M', 192.72732405229047)\n('TSURF_K', 192.86672002618963)\n('PBL_YSU_M', 194.32039434259588)\n('SNOWAGE_HR', 198.26652665571734)\n('PRATE_MMpH', 199.35923628373578)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1, 80, 80))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                  [-1, 512]       1,638,912\n           Linear-13                 [-1, 3200]       1,641,600\n        Unflatten-14           [-1, 32, 10, 10]               0\n  ConvTranspose2d-15           [-1, 16, 20, 20]           4,624\n             GELU-16           [-1, 16, 20, 20]               0\n           Conv2d-17           [-1, 16, 20, 20]           2,320\n             GELU-18           [-1, 16, 20, 20]               0\n  ConvTranspose2d-19            [-1, 8, 40, 40]           1,160\n             GELU-20            [-1, 8, 40, 40]               0\n           Conv2d-21            [-1, 8, 40, 40]             584\n             GELU-22            [-1, 8, 40, 40]               0\n  ConvTranspose2d-23            [-1, 1, 80, 80]              73\n================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.35\nParams size (MB): 12.58\nEstimated Total Size (MB): 13.95\n----------------------------------------------------------------\n\n\n\nsummary(model, input_size=(1, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                 [-1, 1024]       3,277,824\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 3200]       3,280,000\n        Unflatten-15           [-1, 32, 10, 10]               0\n  ConvTranspose2d-16           [-1, 16, 20, 20]           4,624\n             GELU-17           [-1, 16, 20, 20]               0\n           Conv2d-18           [-1, 16, 20, 20]           2,320\n             GELU-19           [-1, 16, 20, 20]               0\n  ConvTranspose2d-20            [-1, 8, 40, 40]           1,160\n             GELU-21            [-1, 8, 40, 40]               0\n           Conv2d-22            [-1, 8, 40, 40]             584\n             GELU-23            [-1, 8, 40, 40]               0\n  ConvTranspose2d-24            [-1, 1, 80, 80]              73\n          Decoder-25            [-1, 1, 80, 80]               0\n================================================================\nTotal params: 6,575,377\nTrainable params: 6,575,377\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.41\nParams size (MB): 25.08\nEstimated Total Size (MB): 26.51\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "href": "publications_and_projects/data_folder/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Single channel input single channel output(P10) UNET",
    "text": "Single channel input single channel output(P10) UNET\n\ntest_loss_list_P_10_conv_auto_unet = [] \ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 9347.3935\nEpoch [21/200] Loss: 445.2209\nEpoch [41/200] Loss: 421.8352\nEpoch [61/200] Loss: 411.4913\nEpoch [81/200] Loss: 406.5355\nEpoch [101/200] Loss: 410.0266\nEpoch [121/200] Loss: 403.3963\nEpoch [141/200] Loss: 402.2838\nEpoch [161/200] Loss: 404.5083\nEpoch [181/200] Loss: 408.9846\nAverage Test Loss: 393.9739\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1062.0485\nEpoch [21/200] Loss: 425.7831\nEpoch [41/200] Loss: 414.1087\nEpoch [61/200] Loss: 407.9692\nEpoch [81/200] Loss: 409.0262\nEpoch [101/200] Loss: 413.5665\nEpoch [121/200] Loss: 406.8342\nEpoch [141/200] Loss: 401.0203\nEpoch [161/200] Loss: 403.7887\nEpoch [181/200] Loss: 412.0851\nAverage Test Loss: 384.9228\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1334.1772\nEpoch [21/200] Loss: 432.3582\nEpoch [41/200] Loss: 416.2745\nEpoch [61/200] Loss: 412.7553\nEpoch [81/200] Loss: 411.6587\nEpoch [101/200] Loss: 409.5164\nEpoch [121/200] Loss: 409.2917\nEpoch [141/200] Loss: 414.7216\nEpoch [161/200] Loss: 405.0483\nEpoch [181/200] Loss: 404.8374\nAverage Test Loss: 366.9726\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1228.4596\nEpoch [21/200] Loss: 418.4353\nEpoch [41/200] Loss: 411.4351\nEpoch [61/200] Loss: 411.3802\nEpoch [81/200] Loss: 408.2070\nEpoch [101/200] Loss: 402.7285\nEpoch [121/200] Loss: 405.0596\nEpoch [141/200] Loss: 401.6986\nEpoch [161/200] Loss: 401.9078\nEpoch [181/200] Loss: 405.1741\nAverage Test Loss: 397.5330\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1086.5345\nEpoch [21/200] Loss: 115.0784\nEpoch [41/200] Loss: 55.1418\nEpoch [61/200] Loss: 35.0410\nEpoch [81/200] Loss: 25.9328\nEpoch [101/200] Loss: 22.8977\nEpoch [121/200] Loss: 22.6624\nEpoch [141/200] Loss: 18.5878\nEpoch [161/200] Loss: 16.7638\nEpoch [181/200] Loss: 18.7935\nAverage Test Loss: 16.7694\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1370.2549\nEpoch [21/200] Loss: 253.0794\nEpoch [41/200] Loss: 73.3127\nEpoch [61/200] Loss: 34.2972\nEpoch [81/200] Loss: 23.2712\nEpoch [101/200] Loss: 18.8157\nEpoch [121/200] Loss: 17.0560\nEpoch [141/200] Loss: 13.7983\nEpoch [161/200] Loss: 12.2436\nEpoch [181/200] Loss: 11.2890\nAverage Test Loss: 16.9951\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1202.3667\nEpoch [21/200] Loss: 284.9583\nEpoch [41/200] Loss: 49.5132\nEpoch [61/200] Loss: 23.7217\nEpoch [81/200] Loss: 16.5870\nEpoch [101/200] Loss: 13.4451\nEpoch [121/200] Loss: 11.5799\nEpoch [141/200] Loss: 10.4765\nEpoch [161/200] Loss: 13.5249\nEpoch [181/200] Loss: 9.1645\nAverage Test Loss: 8.1653\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1344.0557\nEpoch [21/200] Loss: 410.2120\nEpoch [41/200] Loss: 409.9936\nEpoch [61/200] Loss: 403.0425\nEpoch [81/200] Loss: 413.2495\nEpoch [101/200] Loss: 410.5454\nEpoch [121/200] Loss: 408.9813\nEpoch [141/200] Loss: 404.3268\nEpoch [161/200] Loss: 402.1354\nEpoch [181/200] Loss: 403.1094\nAverage Test Loss: 370.9240\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1138.2640\nEpoch [21/200] Loss: 428.3663\nEpoch [41/200] Loss: 428.3100\nEpoch [61/200] Loss: 411.9817\nEpoch [81/200] Loss: 406.6649\nEpoch [101/200] Loss: 406.5036\nEpoch [121/200] Loss: 404.4717\nEpoch [141/200] Loss: 413.3260\nEpoch [161/200] Loss: 404.8024\nEpoch [181/200] Loss: 408.2683\nAverage Test Loss: 375.5593\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1171.9761\nEpoch [21/200] Loss: 311.1736\nEpoch [41/200] Loss: 213.8882\nEpoch [61/200] Loss: 86.1118\nEpoch [81/200] Loss: 43.9884\nEpoch [101/200] Loss: 33.4366\nEpoch [121/200] Loss: 26.7442\nEpoch [141/200] Loss: 25.1023\nEpoch [161/200] Loss: 19.1235\nEpoch [181/200] Loss: 16.5218\nAverage Test Loss: 17.8878\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1209.7468\nEpoch [21/200] Loss: 418.6483\nEpoch [41/200] Loss: 411.9460\nEpoch [61/200] Loss: 416.4541\nEpoch [81/200] Loss: 410.3450\nEpoch [101/200] Loss: 404.6752\nEpoch [121/200] Loss: 411.7702\nEpoch [141/200] Loss: 404.7753\nEpoch [161/200] Loss: 410.6092\nEpoch [181/200] Loss: 406.9762\nAverage Test Loss: 364.9331\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1286.6324\nEpoch [21/200] Loss: 416.3279\nEpoch [41/200] Loss: 412.8959\nEpoch [61/200] Loss: 432.9754\nEpoch [81/200] Loss: 423.2503\nEpoch [101/200] Loss: 407.6195\nEpoch [121/200] Loss: 405.0848\nEpoch [141/200] Loss: 418.5362\nEpoch [161/200] Loss: 398.3066\nEpoch [181/200] Loss: 399.7189\nAverage Test Loss: 366.6939\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1160.2099\nEpoch [21/200] Loss: 410.9463\nEpoch [41/200] Loss: 407.0937\nEpoch [61/200] Loss: 406.6331\nEpoch [81/200] Loss: 405.4273\nEpoch [101/200] Loss: 406.6442\nEpoch [121/200] Loss: 405.2662\nEpoch [141/200] Loss: 400.4130\nEpoch [161/200] Loss: 403.3931\nEpoch [181/200] Loss: 409.0273\nAverage Test Loss: 364.0550\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.6661\nEpoch [21/200] Loss: 410.8530\nEpoch [41/200] Loss: 408.6113\nEpoch [61/200] Loss: 400.7242\nEpoch [81/200] Loss: 407.2048\nEpoch [101/200] Loss: 407.0074\nEpoch [121/200] Loss: 405.1491\nEpoch [141/200] Loss: 428.9850\nEpoch [161/200] Loss: 411.7955\nEpoch [181/200] Loss: 412.6084\nAverage Test Loss: 375.2351\n\n\n  0%|          | 1/200 [00:00&lt;01:04,  3.10it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.24it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.20it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.14it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.23it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.16it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.13it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.19it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.09it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.29it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.11it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:07,  2.95it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.19it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.30it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.27it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.26it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.27it/s] 60%|██████    | 121/200 [00:37&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.15it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.25it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.31it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.27it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:44,  3.12it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.21it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.14it/s] 70%|███████   | 141/200 [00:43&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.26it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.29it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:39&lt;00:33,  2.34it/s] 70%|███████   | 141/200 [00:47&lt;00:25,  2.32it/s] 80%|████████  | 161/200 [00:55&lt;00:15,  2.44it/s] 90%|█████████ | 181/200 [01:03&lt;00:06,  3.06it/s]100%|██████████| 200/200 [01:10&lt;00:00,  2.85it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.25it/s] 10%|█         | 21/200 [00:08&lt;01:31,  1.95it/s] 20%|██        | 41/200 [00:15&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:21&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:27&lt;00:34,  3.47it/s] 50%|█████     | 101/200 [00:33&lt;00:32,  3.06it/s] 60%|██████    | 121/200 [00:41&lt;00:22,  3.51it/s] 70%|███████   | 141/200 [00:46&lt;00:16,  3.63it/s] 80%|████████  | 161/200 [00:52&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:58&lt;00:05,  3.47it/s]100%|██████████| 200/200 [01:03&lt;00:00,  3.14it/s]\n  0%|          | 1/200 [00:00&lt;01:13,  2.70it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.49it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.49it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.49it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.47it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.52it/s] 80%|████████  | 161/200 [00:46&lt;00:10,  3.61it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:34&lt;00:50,  1.95it/s] 60%|██████    | 121/200 [00:48&lt;00:21,  3.66it/s] 70%|███████   | 141/200 [00:54&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:59&lt;00:12,  3.17it/s] 90%|█████████ | 181/200 [01:05&lt;00:05,  3.38it/s]100%|██████████| 200/200 [01:12&lt;00:00,  2.74it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.31it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.48it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.49it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.50it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.49it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.43it/s]\n  0%|          | 1/200 [00:00&lt;01:05,  3.05it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.56it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.46it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.53it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:26,  3.68it/s] 60%|██████    | 121/200 [00:34&lt;00:22,  3.56it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.55it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.49it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.57it/s] 50%|█████     | 101/200 [00:29&lt;00:27,  3.55it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.47it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.64it/s] 10%|█         | 21/200 [00:06&lt;00:49,  3.63it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:36&lt;00:25,  3.12it/s] 70%|███████   | 141/200 [00:42&lt;00:18,  3.27it/s] 80%|████████  | 161/200 [00:48&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:54&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:00&lt;00:00,  3.29it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto_unet\n\n[393.97390192205256,\n 384.9228349165483,\n 366.9725619229403,\n 397.5330338911577,\n 16.76939444108443,\n 16.99508944424716,\n 8.165278738195246,\n 370.9240167791193,\n 375.55933172052556,\n 17.88784339211204,\n 364.93308327414775,\n 366.69390869140625,\n 364.0550065474077,\n 375.23513239080256]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 8.165278738195246)\n('CLOUD_OD', 16.76939444108443)\n('U10_MpS', 16.99508944424716)\n('SOLM_M3pM3', 17.88784339211204)\n('PBL_WRF_M', 364.0550065474077)\n('CLDTOP_KM', 364.93308327414775)\n('CAPE', 366.69390869140625)\n('SNOWAGE_HR', 366.9725619229403)\n('T2_K', 370.9240167791193)\n('PBL_YSU_M', 375.23513239080256)\n('SWSFC_WpM2', 375.55933172052556)\n('SNOWEW_M', 384.9228349165483)\n('TSURF_K', 393.97390192205256)\n('PRATE_MMpH', 397.5330338911577)\nLowest 3 names: ['V10_MpS', 'CLOUD_OD', 'U10_MpS']"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#insights-2",
    "href": "publications_and_projects/data_folder/Autoencoder.html#insights-2",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto_unet, test_loss_list_P_25_conv_auto_unet)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto_unet, 'P25': test_loss_list_P_25_conv_auto_unet, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto_unet.csv', index=False)\n\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n8.2\n3.5\n5.8\n\n\n5\nU10_MpS\n17.0\n3.2\n10.1\n\n\n9\nSOLM_M3pM3\n17.9\n5.3\n11.6\n\n\n4\nCLOUD_OD\n16.8\n8.0\n12.4\n\n\n10\nCLDTOP_KM\n364.9\n191.1\n278.0\n\n\n12\nPBL_WRF_M\n364.1\n192.7\n278.4\n\n\n11\nCAPE\n366.7\n191.7\n279.2\n\n\n7\nT2_K\n370.9\n192.5\n281.7\n\n\n2\nSNOWAGE_HR\n367.0\n198.3\n282.6\n\n\n8\nSWSFC_WpM2\n375.6\n191.1\n283.3\n\n\n13\nPBL_YSU_M\n375.2\n194.3\n284.8\n\n\n1\nSNOWEW_M\n384.9\n191.9\n288.4\n\n\n0\nTSURF_K\n394.0\n192.9\n293.4\n\n\n3\nPRATE_MMpH\n397.5\n199.4\n298.4"
  },
  {
    "objectID": "publications_and_projects/data_folder/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "href": "publications_and_projects/data_folder/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate and Weather Modelling",
    "section": "Training on top 4 channel and predicting on all channel UNET",
    "text": "Training on top 4 channel and predicting on all channel UNET\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 100\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/100] Loss: 519.4141\nEpoch [21/100] Loss: 10.5276\nEpoch [41/100] Loss: 2.6655\nEpoch [61/100] Loss: 5.3309\nEpoch [81/100] Loss: 2.3606\nAverage Test Loss: 0.9041\n\n\n  1%|          | 1/100 [00:01&lt;03:17,  1.99s/it] 21%|██        | 21/100 [00:36&lt;02:15,  1.72s/it] 41%|████      | 41/100 [01:10&lt;01:41,  1.72s/it] 61%|██████    | 61/100 [01:44&lt;01:06,  1.72s/it] 81%|████████  | 81/100 [02:19&lt;00:32,  1.72s/it]100%|██████████| 100/100 [02:52&lt;00:00,  1.72s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in4_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data_folder/research.html",
    "href": "publications_and_projects/data_folder/research.html",
    "title": "Towards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning",
    "section": "",
    "text": "Paper accepted and in nomination for best paper award in NeurIPS 2023 Workshop on ReALML (Active learning and Machine learning in real world)\nLive Streamlit Demo App: link\n\n\nKeywords:\nActive Learning, Satellite Imagery, Transfer Learning ### Abstract: - Air pollution is a leading cause of death globally, especially in south-east Asia. Brick production contributes significantly to air pollution. However, unlike other sources such as power plants, brick production is unregulated and thus hard to monitor. Traditional survey-based methods for kiln identification are time and resource-intensive. - Similarly, it is time-consuming for air quality experts to annotate satellite imagery manually. Recently, computer vision machine learning models have helped reduce labeling costs, but they need sufficiently large labeled imagery. In this paper, we propose scalable methods using active learning to accurately detect brick kilns with minimal manual labeling effort. Through this work, we have identified more than 700 new brick kilns across the Indo-Gangetic region: a highly populous and polluted region spanning 0.4 million square kilometers in India. - In addition, we have deployed our model as a web application for automatically identifying brick kilns given a specific area by the user.\nTo know more check: Paper"
  },
  {
    "objectID": "publications_and_projects/data_folder/research.html#towards-scalable-identification-of-brick-kilns-from-satellite-imagery-with-active-learning",
    "href": "publications_and_projects/data_folder/research.html#towards-scalable-identification-of-brick-kilns-from-satellite-imagery-with-active-learning",
    "title": "Towards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning",
    "section": "",
    "text": "Paper accepted and in nomination for best paper award in NeurIPS 2023 Workshop on ReALML (Active learning and Machine learning in real world)\nLive Streamlit Demo App: link\n\n\nActive Learning, Satellite Imagery, Transfer Learning ### Abstract: - Air pollution is a leading cause of death globally, especially in south-east Asia. Brick production contributes significantly to air pollution. However, unlike other sources such as power plants, brick production is unregulated and thus hard to monitor. Traditional survey-based methods for kiln identification are time and resource-intensive. - Similarly, it is time-consuming for air quality experts to annotate satellite imagery manually. Recently, computer vision machine learning models have helped reduce labeling costs, but they need sufficiently large labeled imagery. In this paper, we propose scalable methods using active learning to accurately detect brick kilns with minimal manual labeling effort. Through this work, we have identified more than 700 new brick kilns across the Indo-Gangetic region: a highly populous and polluted region spanning 0.4 million square kilometers in India. - In addition, we have deployed our model as a web application for automatically identifying brick kilns given a specific area by the user.\nTo know more check: Paper"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html",
    "href": "publications_and_projects/data/Autoencoder.html",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "",
    "text": "We have implemented MLP autoencoder, CNN autoencoder, and UNET autoencoder for image to image prediction on Camx dataset.\nGo through the link to see the presentation of the project to get the flow.\nGit Repo: link\nimport numpy as np\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch \nimport os\nimport torch.utils.data as data\nimport torch.nn as nn\nimport glob \n# from torchsummary import summary\nfrom torchinfo import summary\nfrom tqdm import trange\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = torch.device(\"cuda:2\")\nprint(device)\ncurrent_device = device #torch.cuda.current_device()\ndevice_name = torch.cuda.get_device_name(current_device)\nprint(f\"Current GPU assigned: {current_device}, Name: {device_name}\")\n\ncuda\nCurrent GPU assigned: cuda, Name: Quadro RTX 5000\nConsider it a prediction task, use any channel like pressure and/or wind speed inputs to start with, because they are the indicators of horizontal movement of air pollution. Multiple inputs can go as multiple channels of images similar to RGB. Output can be P25 or P10 in cam 120hr files.\ndef get_latitudes():\n    lat_start= 76.8499984741211\n    lat_step=0.009999999776482582\n\n    latitudes=[]\n\n    for i in range(80):\n        latitudes.append(lat_start+i*lat_step)\n    \n    latitudes.reverse()\n\n    return latitudes\n\ndef get_longitudes():\n    long_start= 28.200000762939453\n    long_step=0.009999999776482582\n\n    longitudes=[]\n\n    for i in range(80):\n        longitudes.append(long_start+i*long_step)\n    \n    # longitudes.reverse()\n\n    return longitudes\n\nlatitudes=get_latitudes()\nlongitudes=get_longitudes()\n\ndef create_plot(data,hour,var_name):\n    # print(data[var_name].shape) #shape (120, 1, 80, 80)\n    p10_hour=data[var_name]['TSTEP'==hour] # shape (1, 80, 80)\n    p10_hour=p10_hour[0,:,:] # shape (80, 80)\n    plt.imshow(p10_hour)\n    plt.title(f'{var_name} at hour '+str(hour))\n    # plt.colorbar()\n\n    # only show every latitude and longitude of end points\n    # round to 2 decimal places\n    top=latitudes[0]\n    top=round(top,2) \n    bottom=latitudes[-1]\n    bottom=round(bottom,2)\n    left=longitudes[0]\n    left=round(left,2)\n    right=longitudes[-1]\n    right=round(right,2)\n\n    plt.xticks([0,79],[left,right])\n    plt.xlabel('Longitude')\n    plt.yticks([0,79],[top,bottom])\n    plt.ylabel('Latitude')\n    # plt.savefig(f'plots/120/{var_name}_{day}.png')\n    # plt.close()"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#visualizing-96-hr-files",
    "href": "publications_and_projects/data/Autoencoder.html#visualizing-96-hr-files",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Visualizing 96 hr files",
    "text": "Visualizing 96 hr files\n\ndata_96 = xr.open_dataset('data/camxmet2d.delhi.20230717.96hours.nc')\ndata_96_df = data_96.to_dataframe().reset_index()\n\n\ndata_96\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:     (TSTEP: 96, VAR: 14, DATE-TIME: 2, LAY: 1, ROW: 80, COL: 80)\nDimensions without coordinates: TSTEP, VAR, DATE-TIME, LAY, ROW, COL\nData variables: (12/15)\n    TFLAG       (TSTEP, VAR, DATE-TIME) int32 2023198 0 ... 2023201 230000\n    TSURF_K     (TSTEP, LAY, ROW, COL) float32 302.3 302.3 302.3 ... 300.6 301.2\n    SNOWEW_M    (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SNOWAGE_HR  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PRATE_MMpH  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CLOUD_OD    (TSTEP, LAY, ROW, COL) float32 62.24 61.67 61.1 ... 37.1 36.78\n    ...          ...\n    SWSFC_WpM2  (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    SOLM_M3pM3  (TSTEP, LAY, ROW, COL) float32 0.3131 0.3114 ... 0.3278 0.3292\n    CLDTOP_KM   (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    CAPE        (TSTEP, LAY, ROW, COL) float32 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n    PBL_WRF_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 94.4 120.0\n    PBL_YSU_M   (TSTEP, LAY, ROW, COL) float32 17.21 17.21 17.21 ... 64.43 94.71\nAttributes: (12/33)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023198\n    CTIME:          73941\n    WDATE:          2023198\n    ...             ...\n    VGLVLS:         [0. 0.]\n    GDNAM:          ????????????????\n    UPNAM:          CAMx2IOAPI      \n    VAR-LIST:       TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMp...\n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        xarray.DatasetDimensions:TSTEP: 96VAR: 14DATE-TIME: 2LAY: 1ROW: 80COL: 80Coordinates: (0)Data variables: (15)TFLAG(TSTEP, VAR, DATE-TIME)int322023198 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023198,       0],\n        [2023198,       0],\n        ...,\n        [2023198,       0],\n        [2023198,       0]],\n\n       [[2023198,   10000],\n        [2023198,   10000],\n        ...,\n        [2023198,   10000],\n        [2023198,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)TSURF_K(TSTEP, LAY, ROW, COL)float32302.3 302.3 302.3 ... 300.6 301.2long_name :TSURF_K         units :ppmV            var_desc :VARIABLE TSURF_K                                                                array([[[[302.31464, ..., 301.9288 ],\n         ...,\n         [301.28806, ..., 301.65118]]],\n\n\n       ...,\n\n\n       [[[299.001  , ..., 299.86826],\n         ...,\n         [299.08127, ..., 301.1575 ]]]], dtype=float32)SNOWEW_M(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWEW_M        units :ppmV            var_desc :VARIABLE SNOWEW_M                                                               array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SNOWAGE_HR(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SNOWAGE_HR      units :ppmV            var_desc :VARIABLE SNOWAGE_HR                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PRATE_MMpH(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :PRATE_MMpH      units :ppmV            var_desc :VARIABLE PRATE_MMpH                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CLOUD_OD(TSTEP, LAY, ROW, COL)float3262.24 61.67 61.1 ... 37.1 36.78long_name :CLOUD_OD        units :ppmV            var_desc :VARIABLE CLOUD_OD                                                               array([[[[62.239326, ..., 30.003962],\n         ...,\n         [54.13987 , ..., 43.78004 ]]],\n\n\n       ...,\n\n\n       [[[29.166183, ..., 58.07322 ],\n         ...,\n         [32.563763, ..., 36.776646]]]], dtype=float32)U10_MpS(TSTEP, LAY, ROW, COL)float321.63 1.585 1.54 ... 0.8064 0.8805long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)V10_MpS(TSTEP, LAY, ROW, COL)float320.1487 0.158 ... -1.202 -1.152long_name :V10_MpS         units :ppmV            var_desc :VARIABLE V10_MpS                                                                array([[[[ 0.148722, ...,  0.630212],\n         ...,\n         [-0.328182, ...,  0.625002]]],\n\n\n       ...,\n\n\n       [[[ 0.040896, ..., -1.492097],\n         ...,\n         [-0.365465, ..., -1.152316]]]], dtype=float32)T2_K(TSTEP, LAY, ROW, COL)float32302.6 302.6 302.6 ... 300.4 300.8long_name :T2_K            units :ppmV            var_desc :VARIABLE T2_K                                                                   array([[[[302.57184, ..., 302.88318],\n         ...,\n         [301.1836 , ..., 302.19128]]],\n\n\n       ...,\n\n\n       [[[299.17767, ..., 300.1615 ],\n         ...,\n         [299.30466, ..., 300.77878]]]], dtype=float32)SWSFC_WpM2(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :SWSFC_WpM2      units :ppmV            var_desc :VARIABLE SWSFC_WpM2                                                             array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)SOLM_M3pM3(TSTEP, LAY, ROW, COL)float320.3131 0.3114 ... 0.3278 0.3292long_name :SOLM_M3pM3      units :ppmV            var_desc :VARIABLE SOLM_M3pM3                                                             array([[[[0.313128, ..., 0.271955],\n         ...,\n         [0.306425, ..., 0.287092]]],\n\n\n       ...,\n\n\n       [[[0.312972, ..., 0.263079],\n         ...,\n         [0.29241 , ..., 0.32918 ]]]], dtype=float32)CLDTOP_KM(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CLDTOP_KM       units :ppmV            var_desc :VARIABLE CLDTOP_KM                                                              array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)CAPE(TSTEP, LAY, ROW, COL)float320.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0long_name :CAPE            units :ppmV            var_desc :VARIABLE CAPE                                                                   array([[[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]],\n\n\n       ...,\n\n\n       [[[0., ..., 0.],\n         ...,\n         [0., ..., 0.]]]], dtype=float32)PBL_WRF_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 94.4 120.0long_name :PBL_WRF_M       units :ppmV            var_desc :VARIABLE PBL_WRF_M                                                              array([[[[ 17.212452, ...,  17.253763],\n         ...,\n         [ 17.153704, ...,  17.189987]]],\n\n\n       ...,\n\n\n       [[[ 33.101303, ...,  30.475212],\n         ...,\n         [ 24.450672, ..., 120.012794]]]], dtype=float32)PBL_YSU_M(TSTEP, LAY, ROW, COL)float3217.21 17.21 17.21 ... 64.43 94.71long_name :PBL_YSU_M       units :ppmV            var_desc :VARIABLE PBL_YSU_M                                                              array([[[[17.212452, ..., 17.253763],\n         ...,\n         [17.153704, ..., 17.189987]]],\n\n\n       ...,\n\n\n       [[[17.060207, ..., 17.152937],\n         ...,\n         [17.07223 , ..., 94.71051 ]]]], dtype=float32)Indexes: (0)Attributes: (33)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023198CTIME :73941WDATE :2023198WTIME :73941SDATE :2023198STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :14GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMx2IOAPI      VAR-LIST :TSURF_K         SNOWEW_M        SNOWAGE_HR      PRATE_MMpH      CLOUD_OD        U10_MpS         V10_MpS         T2_K            SWSFC_WpM2      SOLM_M3pM3      CLDTOP_KM       CAPE            PBL_WRF_M       PBL_YSU_M       FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :\n\n\n\ndata_96_df\n\n\n\n\n\n\n\n\nTSTEP\nVAR\nDATE-TIME\nLAY\nROW\nCOL\nTFLAG\nTSURF_K\nSNOWEW_M\nSNOWAGE_HR\n...\nCLOUD_OD\nU10_MpS\nV10_MpS\nT2_K\nSWSFC_WpM2\nSOLM_M3pM3\nCLDTOP_KM\nCAPE\nPBL_WRF_M\nPBL_YSU_M\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n2023198\n302.314636\n0.0\n0.0\n...\n62.239326\n1.630373\n0.148722\n302.571838\n0.0\n0.313128\n0.0\n0.0\n17.212452\n17.212452\n\n\n1\n0\n0\n0\n0\n0\n1\n2023198\n302.312042\n0.0\n0.0\n...\n61.671093\n1.585281\n0.158011\n302.584351\n0.0\n0.311416\n0.0\n0.0\n17.212652\n17.212652\n\n\n2\n0\n0\n0\n0\n0\n2\n2023198\n302.309479\n0.0\n0.0\n...\n61.102859\n1.540188\n0.167299\n302.596832\n0.0\n0.309703\n0.0\n0.0\n17.212852\n17.212852\n\n\n3\n0\n0\n0\n0\n0\n3\n2023198\n302.306885\n0.0\n0.0\n...\n60.534630\n1.495095\n0.176587\n302.609344\n0.0\n0.307990\n0.0\n0.0\n17.213055\n17.213055\n\n\n4\n0\n0\n0\n0\n0\n4\n2023198\n302.303558\n0.0\n0.0\n...\n59.912636\n1.450304\n0.184773\n302.621948\n0.0\n0.306286\n0.0\n0.0\n17.213356\n17.213356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n17203195\n95\n13\n1\n0\n79\n75\n230000\n298.979095\n0.0\n0.0\n...\n37.972748\n0.600130\n-1.341342\n299.389465\n0.0\n0.324286\n0.0\n0.0\n26.661556\n17.082989\n\n\n17203196\n95\n13\n1\n0\n79\n76\n230000\n299.372223\n0.0\n0.0\n...\n37.747482\n0.658128\n-1.301898\n299.638062\n0.0\n0.324927\n0.0\n0.0\n43.123440\n17.094244\n\n\n17203197\n95\n13\n1\n0\n79\n77\n230000\n299.967468\n0.0\n0.0\n...\n37.423786\n0.732263\n-1.252025\n300.018402\n0.0\n0.326345\n0.0\n0.0\n68.759804\n17.112419\n\n\n17203198\n95\n13\n1\n0\n79\n78\n230000\n300.562744\n0.0\n0.0\n...\n37.100090\n0.806398\n-1.202152\n300.398743\n0.0\n0.327763\n0.0\n0.0\n94.396065\n64.426094\n\n\n17203199\n95\n13\n1\n0\n79\n79\n230000\n301.157501\n0.0\n0.0\n...\n36.776646\n0.880477\n-1.152316\n300.778778\n0.0\n0.329180\n0.0\n0.0\n120.012794\n94.710510\n\n\n\n\n17203200 rows × 21 columns\n\n\n\n\ndata_96_df.columns\n\nIndex(['TSTEP', 'VAR', 'DATE-TIME', 'LAY', 'ROW', 'COL', 'TFLAG', 'TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M'],\n      dtype='object')\n\n\n\ndata_96['U10_MpS'] #shape (96, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'U10_MpS' (TSTEP: 96, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  U10_MpS         \n    units:      ppmV            \n    var_desc:   VARIABLE U10_MpS                                             ...xarray.DataArray'U10_MpS'TSTEP: 96LAY: 1ROW: 80COL: 801.63 1.585 1.54 1.495 1.45 ... 0.6001 0.6581 0.7323 0.8064 0.8805array([[[[ 1.630373, ..., -0.434344],\n         ...,\n         [-0.544169, ..., -2.070899]]],\n\n\n       ...,\n\n\n       [[[ 0.569056, ...,  0.423629],\n         ...,\n         [ 0.774855, ...,  0.880477]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :U10_MpS         units :ppmV            var_desc :VARIABLE U10_MpS                                                                \n\n\n\ndata_96['U10_MpS'].shape , data_96['U10_MpS']['TSTEP'==0].shape, data_96['U10_MpS']['TSTEP'==0][0].shape\n\n((96, 1, 80, 80), (1, 80, 80), (80, 80))\n\n\n\nplt.imshow(data_96['U10_MpS']['TSTEP'==0][0]) #shape (80, 80)\n\n&lt;matplotlib.image.AxesImage at 0x7f76abc7cf70&gt;\n\n\n\n\n\n\ncreate_plot(data = data_96, hour = 1, var_name = 'U10_MpS')"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#visualizing-120-hr-files",
    "href": "publications_and_projects/data/Autoencoder.html#visualizing-120-hr-files",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Visualizing 120 hr files",
    "text": "Visualizing 120 hr files\n\ndata_120 = xr.open_dataset('data/camx120hr_merged_20230717.nc')\ndata_120_df = data_120.to_dataframe().reset_index() \n\n\ndata_120 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:  (TSTEP: 120, LAY: 1, ROW: 80, COL: 80, VAR: 9, DATE-TIME: 2)\nDimensions without coordinates: TSTEP, LAY, ROW, COL, VAR, DATE-TIME\nData variables:\n    P10      (TSTEP, LAY, ROW, COL) float32 23.86 23.86 24.07 ... 10.1 10.1\n    P25      (TSTEP, LAY, ROW, COL) float32 19.24 19.24 19.62 ... 9.63 9.63\n    TFLAG    (TSTEP, VAR, DATE-TIME) int32 2023197 0 2023197 ... 2023201 230000\nAttributes: (12/34)\n    IOAPI_VERSION:  $Id: @(#) ioapi library version 3.0 $                    ...\n    EXEC_ID:        ????????????????                                         ...\n    FTYPE:          1\n    CDATE:          2023197\n    CTIME:          83911\n    WDATE:          2023197\n    ...             ...\n    GDNAM:          ????????????????\n    UPNAM:          CAMXMETOU       \n    VAR-LIST:       P10             P25             \n    FILEDESC:       I/O API formatted CAMx AVRG output                       ...\n    HISTORY:        Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc ca...\n    NCO:            netCDF Operators version 4.9.1 (Homepage = http://nco.sf....xarray.DatasetDimensions:TSTEP: 120LAY: 1ROW: 80COL: 80VAR: 9DATE-TIME: 2Coordinates: (0)Data variables: (3)P10(TSTEP, LAY, ROW, COL)float3223.86 23.86 24.07 ... 10.1 10.1long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)P25(TSTEP, LAY, ROW, COL)float3219.24 19.24 19.62 ... 9.63 9.63long_name :FPRM            units :micrograms/m**3 var_desc :VARIABLE FPRM                                                                   array([[[[19.240587, ..., 26.00578 ],\n         ...,\n         [31.570122, ..., 17.510971]]],\n\n\n       ...,\n\n\n       [[[22.36091 , ..., 16.678066],\n         ...,\n         [18.63693 , ...,  9.629862]]]], dtype=float32)TFLAG(TSTEP, VAR, DATE-TIME)int322023197 0 ... 2023201 230000units :&lt;YYYYDDD,HHMMSS&gt;long_name :TFLAG           var_desc :Timestep-valid flags:  (1) YYYYDDD or (2) HHMMSS                                array([[[2023197,       0],\n        [2023197,       0],\n        ...,\n        [2023197,       0],\n        [2023197,       0]],\n\n       [[2023197,   10000],\n        [2023197,   10000],\n        ...,\n        [2023197,   10000],\n        [2023197,   10000]],\n\n       ...,\n\n       [[2023201,  220000],\n        [2023201,  220000],\n        ...,\n        [2023201,  220000],\n        [2023201,  220000]],\n\n       [[2023201,  230000],\n        [2023201,  230000],\n        ...,\n        [2023201,  230000],\n        [2023201,  230000]]], dtype=int32)Indexes: (0)Attributes: (34)IOAPI_VERSION :$Id: @(#) ioapi library version 3.0 $                                           EXEC_ID :????????????????                                                                FTYPE :1CDATE :2023197CTIME :83911WDATE :2023197WTIME :83911SDATE :2023197STIME :0TSTEP :10000NTHIK :1NCOLS :80NROWS :80NLAYS :1NVARS :2GDTYP :1P_ALP :0.0P_BET :0.0P_GAM :0.0XCENT :0.0YCENT :0.0XORIG :76.8499984741211YORIG :28.200000762939453XCELL :0.009999999776482582YCELL :0.009999999776482582VGTYP :-9999VGTOP :-9.999e+36VGLVLS :[0. 0.]GDNAM :????????????????UPNAM :CAMXMETOU       VAR-LIST :P10             P25             FILEDESC :I/O API formatted CAMx AVRG output                                              HISTORY :Mon Jul 17 08:45:22 2023: ncrcat camxout.2023.07.16.nc camxout.2023.07.17.nc camxout.2023.07.18.nc camxout.2023.07.19.nc camxout.2023.07.20.nc camx120hr.nc\nNCO :netCDF Operators version 4.9.1 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n\n\n\ndata_120_df\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n23.857107\n19.240587\n2023197\n\n\n1\n0\n0\n0\n0\n0\n1\n23.857107\n19.240587\n0\n\n\n2\n0\n0\n0\n0\n1\n0\n23.857107\n19.240587\n2023197\n\n\n3\n0\n0\n0\n0\n1\n1\n23.857107\n19.240587\n0\n\n\n4\n0\n0\n0\n0\n2\n0\n23.857107\n19.240587\n2023197\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13823995\n119\n0\n79\n79\n6\n1\n10.097546\n9.629862\n230000\n\n\n13823996\n119\n0\n79\n79\n7\n0\n10.097546\n9.629862\n2023201\n\n\n13823997\n119\n0\n79\n79\n7\n1\n10.097546\n9.629862\n230000\n\n\n13823998\n119\n0\n79\n79\n8\n0\n10.097546\n9.629862\n2023201\n\n\n13823999\n119\n0\n79\n79\n8\n1\n10.097546\n9.629862\n230000\n\n\n\n\n13824000 rows × 9 columns\n\n\n\n\ndata_120_df.describe()\n\n\n\n\n\n\n\n\nTSTEP\nLAY\nROW\nCOL\nVAR\nDATE-TIME\nP10\nP25\nTFLAG\n\n\n\n\ncount\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n13824000.0\n1.382400e+07\n1.382400e+07\n1.382400e+07\n\n\nmean\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n4.101470e+01\n2.856279e+01\n1.069100e+06\n\n\nstd\n3.463981e+01\n0.0\n2.309221e+01\n2.309221e+01\n2.581989e+00\n0.5\n2.361775e+01\n1.582030e+01\n9.553543e+05\n\n\nmin\n0.000000e+00\n0.0\n0.000000e+00\n0.000000e+00\n0.000000e+00\n0.0\n1.111396e+00\n8.114247e-01\n0.000000e+00\n\n\n25%\n2.975000e+01\n0.0\n1.975000e+01\n1.975000e+01\n2.000000e+00\n0.0\n2.546643e+01\n1.880392e+01\n1.175000e+05\n\n\n50%\n5.950000e+01\n0.0\n3.950000e+01\n3.950000e+01\n4.000000e+00\n0.5\n3.590978e+01\n2.589731e+01\n1.126598e+06\n\n\n75%\n8.925000e+01\n0.0\n5.925000e+01\n5.925000e+01\n6.000000e+00\n1.0\n5.103806e+01\n3.433007e+01\n2.023199e+06\n\n\nmax\n1.190000e+02\n0.0\n7.900000e+01\n7.900000e+01\n8.000000e+00\n1.0\n6.492913e+02\n6.130998e+02\n2.023201e+06\n\n\n\n\n\n\n\n\ndata_120['COL']\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'COL' (COL: 80)&gt;\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])\nDimensions without coordinates: COLxarray.DataArray'COL'COL: 800 1 2 3 4 5 6 7 8 9 10 11 12 ... 68 69 70 71 72 73 74 75 76 77 78 79array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n       72, 73, 74, 75, 76, 77, 78, 79])Coordinates: (0)Indexes: (0)Attributes: (0)\n\n\n\ndata_120['P10'] #shape (120, 1, 80, 80)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (TSTEP: 120, LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)\nDimensions without coordinates: TSTEP, LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'TSTEP: 120LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 9.551 9.664 9.808 10.1 10.1array([[[[23.857107, ..., 28.82367 ],\n         ...,\n         [34.902046, ..., 18.506985]]],\n\n\n       ...,\n\n\n       [[[38.434433, ..., 22.536842],\n         ...,\n         [20.921093, ..., 10.097546]]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1]# shape 1x80x80\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'P10' (LAY: 1, ROW: 80, COL: 80)&gt;\narray([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)\nDimensions without coordinates: LAY, ROW, COL\nAttributes:\n    long_name:  CPRM            \n    units:      micrograms/m**3 \n    var_desc:   VARIABLE CPRM                                                ...xarray.DataArray'P10'LAY: 1ROW: 80COL: 8023.86 23.86 24.07 23.42 22.83 22.56 ... 18.47 18.77 18.69 18.51 18.51array([[[23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        [23.857107, 23.857107, ..., 28.82367 , 28.82367 ],\n        ...,\n        [34.902046, 34.902046, ..., 18.506985, 18.506985],\n        [34.902046, 34.902046, ..., 18.506985, 18.506985]]], dtype=float32)Coordinates: (0)Indexes: (0)Attributes: (3)long_name :CPRM            units :micrograms/m**3 var_desc :VARIABLE CPRM                                                                   \n\n\n\ndata_120['P10']['TSTEP'==1][0] # shape 80x80\nplt.imshow(data_120['P25']['TSTEP'==1][0])#, vmin=0, vmax=100)\n\n&lt;matplotlib.image.AxesImage at 0x7f190d573e80&gt;\n\n\n\n\n\n\ncreate_plot(data_120,1,'P25')"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#model-defination",
    "href": "publications_and_projects/data/Autoencoder.html#model-defination",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Model defination",
    "text": "Model defination\n\n\nclass Autoencoder_MLP(nn.Module):\n    def __init__(self):\n        super(Autoencoder_MLP, self).__init__()\n        \n        # Define the layers\n        self.flatten = nn.Flatten()  # Flatten the 2D input matrix\n        self.fc1 = nn.Linear(80*80, 1024)  # Fully connected layer 1\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 1024)  # Fully connected layer 2\n        self.fc4 = nn.Linear(1024, 80*80)  # Fully connected layer 2\n        self.relu = nn.ReLU()  # Activation function\n\n    def forward(self, x):\n        # Flatten the input\n        x = self.flatten(x)\n        \n        # Forward pass through the fully connected layers\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        x = self.relu(x)\n        x = self.fc4(x)\n\n        # Reshape the output to match the 2D matrix size\n        x = x.view(-1, 80, 80)\n        \n        return x\n\n\nsummary(Autoencoder_MLP(), input_size=(1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_MLP                          [1, 80, 80]               --\n├─Flatten: 1-1                           [1, 6400]                 --\n├─Linear: 1-2                            [1, 1024]                 6,554,624\n├─ReLU: 1-3                              [1, 1024]                 --\n├─Linear: 1-4                            [1, 512]                  524,800\n├─ReLU: 1-5                              [1, 512]                  --\n├─Linear: 1-6                            [1, 1024]                 525,312\n├─ReLU: 1-7                              [1, 1024]                 --\n├─Linear: 1-8                            [1, 6400]                 6,560,000\n==========================================================================================\nTotal params: 14,164,736\nTrainable params: 14,164,736\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 14.16\n==========================================================================================\nInput size (MB): 0.03\nForward/backward pass size (MB): 0.07\nParams size (MB): 56.66\nEstimated Total Size (MB): 56.76\n=========================================================================================="
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-single-channel-input-and-outputp25",
    "href": "publications_and_projects/data/Autoencoder.html#training-single-channel-input-and-outputp25",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\nX.shape, y.shape\n\n((1656, 14, 80, 80), (1656, 2, 80, 80))\n\n\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25 = []\ny_channel = 0 \n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 405.4315\nEpoch [21/200] Loss: 214.5978\nEpoch [41/200] Loss: 215.0236\nEpoch [61/200] Loss: 207.2482\nEpoch [81/200] Loss: 206.7534\nEpoch [101/200] Loss: 213.1840\nEpoch [121/200] Loss: 207.5252\nEpoch [141/200] Loss: 208.2888\nEpoch [161/200] Loss: 209.6494\nEpoch [181/200] Loss: 209.7600\nAverage Test Loss: 190.0971\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 464.9160\nEpoch [21/200] Loss: 206.1055\nEpoch [41/200] Loss: 210.0196\nEpoch [61/200] Loss: 209.2211\nEpoch [81/200] Loss: 211.8091\nEpoch [101/200] Loss: 205.4592\nEpoch [121/200] Loss: 203.9590\nEpoch [141/200] Loss: 208.5030\nEpoch [161/200] Loss: 207.1320\nEpoch [181/200] Loss: 204.3017\nAverage Test Loss: 189.7174\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 460.0334\nEpoch [21/200] Loss: 207.1043\nEpoch [41/200] Loss: 206.5882\nEpoch [61/200] Loss: 208.3192\nEpoch [81/200] Loss: 205.4035\nEpoch [101/200] Loss: 209.7349\nEpoch [121/200] Loss: 205.5515\nEpoch [141/200] Loss: 206.1994\nEpoch [161/200] Loss: 205.4001\nEpoch [181/200] Loss: 206.3801\nAverage Test Loss: 190.0263\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 476.9623\nEpoch [21/200] Loss: 207.3108\nEpoch [41/200] Loss: 208.4855\nEpoch [61/200] Loss: 206.4735\nEpoch [81/200] Loss: 205.5895\nEpoch [101/200] Loss: 205.6536\nEpoch [121/200] Loss: 205.9680\nEpoch [141/200] Loss: 205.7129\nEpoch [161/200] Loss: 208.6799\nEpoch [181/200] Loss: 205.6486\nAverage Test Loss: 189.6717\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 599.1686\nEpoch [21/200] Loss: 454.3787\nEpoch [41/200] Loss: 149.9270\nEpoch [61/200] Loss: 23.7072\nEpoch [81/200] Loss: 7.4556\nEpoch [101/200] Loss: 18.4050\nEpoch [121/200] Loss: 3.0840\nEpoch [141/200] Loss: 100.6344\nEpoch [161/200] Loss: 25.9631\nEpoch [181/200] Loss: 38.8335\nAverage Test Loss: 9.8902\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 373.7542\nEpoch [21/200] Loss: 51.2008\nEpoch [41/200] Loss: 37.7057\nEpoch [61/200] Loss: 25.3702\nEpoch [81/200] Loss: 0.2791\nEpoch [101/200] Loss: 20.1649\nEpoch [121/200] Loss: 0.2297\nEpoch [141/200] Loss: 84.8078\nEpoch [161/200] Loss: 2.4612\nEpoch [181/200] Loss: 1.4404\nAverage Test Loss: 0.7598\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 399.2307\nEpoch [21/200] Loss: 1.1263\nEpoch [41/200] Loss: 0.0137\nEpoch [61/200] Loss: 0.0820\nEpoch [81/200] Loss: 2.3733\nEpoch [101/200] Loss: 0.0002\nEpoch [121/200] Loss: 0.0113\nEpoch [141/200] Loss: 0.0393\nEpoch [161/200] Loss: 11.0771\nEpoch [181/200] Loss: 0.0003\nAverage Test Loss: 0.0507\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 412.2657\nEpoch [21/200] Loss: 210.9129\nEpoch [41/200] Loss: 209.2686\nEpoch [61/200] Loss: 206.7048\nEpoch [81/200] Loss: 206.0187\nEpoch [101/200] Loss: 209.1361\nEpoch [121/200] Loss: 212.1444\nEpoch [141/200] Loss: 206.8286\nEpoch [161/200] Loss: 205.8083\nEpoch [181/200] Loss: 208.7421\nAverage Test Loss: 190.9819\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 485.4561\nEpoch [21/200] Loss: 210.1752\nEpoch [41/200] Loss: 207.2874\nEpoch [61/200] Loss: 207.9026\nEpoch [81/200] Loss: 206.2404\nEpoch [101/200] Loss: 205.9271\nEpoch [121/200] Loss: 205.6481\nEpoch [141/200] Loss: 208.2400\nEpoch [161/200] Loss: 205.4081\nEpoch [181/200] Loss: 205.1859\nAverage Test Loss: 199.1508\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 305.6402\nEpoch [21/200] Loss: 176.6024\nEpoch [41/200] Loss: 161.6851\nEpoch [61/200] Loss: 157.7673\nEpoch [81/200] Loss: 138.7664\nEpoch [101/200] Loss: 132.1217\nEpoch [121/200] Loss: 109.4900\nEpoch [141/200] Loss: 104.2382\nEpoch [161/200] Loss: 69.5860\nEpoch [181/200] Loss: 50.0248\nAverage Test Loss: 42.1182\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 461.0668\nEpoch [21/200] Loss: 205.2268\nEpoch [41/200] Loss: 213.0740\nEpoch [61/200] Loss: 206.9702\nEpoch [81/200] Loss: 205.7769\nEpoch [101/200] Loss: 210.3533\nEpoch [121/200] Loss: 206.2578\nEpoch [141/200] Loss: 207.2727\nEpoch [161/200] Loss: 205.3765\nEpoch [181/200] Loss: 206.2520\nAverage Test Loss: 192.2221\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 468.8439\nEpoch [21/200] Loss: 207.2851\nEpoch [41/200] Loss: 211.7429\nEpoch [61/200] Loss: 205.8536\nEpoch [81/200] Loss: 205.3124\nEpoch [101/200] Loss: 207.8155\nEpoch [121/200] Loss: 208.3488\nEpoch [141/200] Loss: 206.0120\nEpoch [161/200] Loss: 205.9442\nEpoch [181/200] Loss: 204.1215\nAverage Test Loss: 190.8858\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 287.4021\nEpoch [21/200] Loss: 209.5118\nEpoch [41/200] Loss: 209.2537\nEpoch [61/200] Loss: 206.0375\nEpoch [81/200] Loss: 207.1711\nEpoch [101/200] Loss: 205.2734\nEpoch [121/200] Loss: 210.4286\nEpoch [141/200] Loss: 207.5243\nEpoch [161/200] Loss: 208.9856\nEpoch [181/200] Loss: 204.6973\nAverage Test Loss: 191.9669\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 323.2034\nEpoch [21/200] Loss: 207.3925\nEpoch [41/200] Loss: 210.0555\nEpoch [61/200] Loss: 206.0877\nEpoch [81/200] Loss: 207.9315\nEpoch [101/200] Loss: 207.4798\nEpoch [121/200] Loss: 206.1072\nEpoch [141/200] Loss: 207.4259\nEpoch [161/200] Loss: 209.3318\nEpoch [181/200] Loss: 207.3006\nAverage Test Loss: 194.3565\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25\n\n[190.09705144708806,\n 189.7173545143821,\n 190.02632834694603,\n 189.67173489657316,\n 9.890190774744207,\n 0.7598354017192667,\n 0.050707115029746834,\n 190.98193498091265,\n 199.15079567649147,\n 42.118152445012875,\n 192.22213883833453,\n 190.88581431995738,\n 191.9668634588068,\n 194.3564910888672]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 0.050707115029746834)\n('U10_MpS', 0.7598354017192667)\n('CLOUD_OD', 9.890190774744207)\n('SOLM_M3pM3', 42.118152445012875)\n('PRATE_MMpH', 189.67173489657316)\n('SNOWEW_M', 189.7173545143821)\n('SNOWAGE_HR', 190.02632834694603)\n('TSURF_K', 190.09705144708806)\n('CAPE', 190.88581431995738)\n('T2_K', 190.98193498091265)\n('PBL_WRF_M', 191.9668634588068)\n('CLDTOP_KM', 192.22213883833453)\n('PBL_YSU_M', 194.3564910888672)\n('SWSFC_WpM2', 199.15079567649147)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'CLOUD_OD']"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "href": "publications_and_projects/data/Autoencoder.html#training-single-channel-input-and-outputp25-1",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Training single channel input and output(P25)",
    "text": "Training single channel input and output(P25)\n\ntest_loss_list_P_10 = []\ny_channel = 1\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel, :,:]\n    X_test = X_test_all[:, x_channel, :,:]\n    y_train = y_train_all[:, y_channel, :,:]\n    y_test = y_test_all[:, y_channel, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_MLP()\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in range(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(inputs)\n\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    \n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 689.7490\nEpoch [21/200] Loss: 418.6676\nEpoch [41/200] Loss: 410.9685\nEpoch [61/200] Loss: 404.8066\nEpoch [81/200] Loss: 400.5351\nEpoch [101/200] Loss: 401.0250\nEpoch [121/200] Loss: 401.0659\nEpoch [141/200] Loss: 403.6432\nEpoch [161/200] Loss: 402.2360\nEpoch [181/200] Loss: 407.2921\nAverage Test Loss: 401.8882\nX Channel name :  SNOWEW_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1031.9999\nEpoch [21/200] Loss: 405.7656\nEpoch [41/200] Loss: 402.2811\nEpoch [61/200] Loss: 403.8337\nEpoch [81/200] Loss: 398.2591\nEpoch [101/200] Loss: 409.4181\nEpoch [121/200] Loss: 396.3737\nEpoch [141/200] Loss: 395.1861\nEpoch [161/200] Loss: 403.0066\nEpoch [181/200] Loss: 397.6094\nAverage Test Loss: 361.2384\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1050.1397\nEpoch [21/200] Loss: 397.1798\nEpoch [41/200] Loss: 401.0453\nEpoch [61/200] Loss: 401.7171\nEpoch [81/200] Loss: 400.2689\nEpoch [101/200] Loss: 396.7101\nEpoch [121/200] Loss: 404.5816\nEpoch [141/200] Loss: 397.7231\nEpoch [161/200] Loss: 394.6489\nEpoch [181/200] Loss: 399.8899\nAverage Test Loss: 362.0641\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1034.3143\nEpoch [21/200] Loss: 399.8861\nEpoch [41/200] Loss: 399.7876\nEpoch [61/200] Loss: 395.9112\nEpoch [81/200] Loss: 400.4803\nEpoch [101/200] Loss: 396.5552\nEpoch [121/200] Loss: 401.0467\nEpoch [141/200] Loss: 406.4987\nEpoch [161/200] Loss: 401.3759\nEpoch [181/200] Loss: 397.4671\nAverage Test Loss: 361.1263\nX Channel name :  CLOUD_OD\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1214.4916\nEpoch [21/200] Loss: 725.7131\nEpoch [41/200] Loss: 220.5272\nEpoch [61/200] Loss: 23.5934\nEpoch [81/200] Loss: 8.8063\nEpoch [101/200] Loss: 274.7881\nEpoch [121/200] Loss: 218.0335\nEpoch [141/200] Loss: 232.4360\nEpoch [161/200] Loss: 212.6520\nEpoch [181/200] Loss: 195.4785\nAverage Test Loss: 180.2198\nX Channel name :  U10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 822.4214\nEpoch [21/200] Loss: 182.0370\nEpoch [41/200] Loss: 50.0221\nEpoch [61/200] Loss: 60.5883\nEpoch [81/200] Loss: 37.8001\nEpoch [101/200] Loss: 2.6386\nEpoch [121/200] Loss: 0.0629\nEpoch [141/200] Loss: 76.5106\nEpoch [161/200] Loss: 41.2225\nEpoch [181/200] Loss: 4.6194\nAverage Test Loss: 0.6711\nX Channel name :  V10_MpS\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1160.5402\nEpoch [21/200] Loss: 8.7026\nEpoch [41/200] Loss: 0.3045\nEpoch [61/200] Loss: 0.1493\nEpoch [81/200] Loss: 0.1235\nEpoch [101/200] Loss: 5.4404\nEpoch [121/200] Loss: 0.0000\nEpoch [141/200] Loss: 10.2387\nEpoch [161/200] Loss: 0.2644\nEpoch [181/200] Loss: 0.0284\nAverage Test Loss: 0.6849\nX Channel name :  T2_K\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 646.6076\nEpoch [21/200] Loss: 412.2161\nEpoch [41/200] Loss: 407.6458\nEpoch [61/200] Loss: 403.7684\nEpoch [81/200] Loss: 399.9570\nEpoch [101/200] Loss: 397.7386\nEpoch [121/200] Loss: 402.0990\nEpoch [141/200] Loss: 397.6674\nEpoch [161/200] Loss: 398.1612\nEpoch [181/200] Loss: 403.3343\nAverage Test Loss: 360.5232\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1027.1539\nEpoch [21/200] Loss: 403.9408\nEpoch [41/200] Loss: 401.9444\nEpoch [61/200] Loss: 399.7967\nEpoch [81/200] Loss: 401.7070\nEpoch [101/200] Loss: 404.1586\nEpoch [121/200] Loss: 398.2909\nEpoch [141/200] Loss: 396.0758\nEpoch [161/200] Loss: 396.4020\nEpoch [181/200] Loss: 402.5728\nAverage Test Loss: 362.6420\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 696.3634\nEpoch [21/200] Loss: 331.1351\nEpoch [41/200] Loss: 296.5541\nEpoch [61/200] Loss: 301.9112\nEpoch [81/200] Loss: 272.0759\nEpoch [101/200] Loss: 251.3878\nEpoch [121/200] Loss: 232.4279\nEpoch [141/200] Loss: 170.0628\nEpoch [161/200] Loss: 138.8842\nEpoch [181/200] Loss: 108.4864\nAverage Test Loss: 110.6638\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1017.6373\nEpoch [21/200] Loss: 400.4651\nEpoch [41/200] Loss: 405.2223\nEpoch [61/200] Loss: 402.9269\nEpoch [81/200] Loss: 400.7290\nEpoch [101/200] Loss: 400.5154\nEpoch [121/200] Loss: 404.1065\nEpoch [141/200] Loss: 403.8573\nEpoch [161/200] Loss: 397.0463\nEpoch [181/200] Loss: 398.1819\nAverage Test Loss: 361.3055\nX Channel name :  CAPE\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 1029.8392\nEpoch [21/200] Loss: 401.8889\nEpoch [41/200] Loss: 399.1993\nEpoch [61/200] Loss: 400.2419\nEpoch [81/200] Loss: 396.8849\nEpoch [101/200] Loss: 395.3639\nEpoch [121/200] Loss: 401.5739\nEpoch [141/200] Loss: 403.1417\nEpoch [161/200] Loss: 402.3704\nEpoch [181/200] Loss: 400.0658\nAverage Test Loss: 361.0597\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 536.0374\nEpoch [21/200] Loss: 401.9832\nEpoch [41/200] Loss: 402.4697\nEpoch [61/200] Loss: 404.1890\nEpoch [81/200] Loss: 400.4949\nEpoch [101/200] Loss: 399.0872\nEpoch [121/200] Loss: 398.4192\nEpoch [141/200] Loss: 400.4667\nEpoch [161/200] Loss: 398.1382\nEpoch [181/200] Loss: 408.3430\nAverage Test Loss: 360.3921\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 80, 80) (332, 80, 80) (1324, 80, 80) (332, 80, 80)\nEpoch [1/200] Loss: 538.3320\nEpoch [21/200] Loss: 405.8272\nEpoch [41/200] Loss: 430.2409\nEpoch [61/200] Loss: 398.9199\nEpoch [81/200] Loss: 404.8572\nEpoch [101/200] Loss: 397.2876\nEpoch [121/200] Loss: 403.3052\nEpoch [141/200] Loss: 401.7307\nEpoch [161/200] Loss: 396.1157\nEpoch [181/200] Loss: 399.0870\nAverage Test Loss: 361.4502\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10\n\n[401.88824185458094,\n 361.23838112571025,\n 362.0641340775923,\n 361.1262997713956,\n 180.2197820490057,\n 0.6710819737477736,\n 0.6848962740464644,\n 360.5232141668146,\n 362.6420177112926,\n 110.6638252951882,\n 361.3054504394531,\n 361.05967018821025,\n 360.39211342551494,\n 361.45016063343394]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 0.6710819737477736)\n('V10_MpS', 0.6848962740464644)\n('SOLM_M3pM3', 110.6638252951882)\n('CLOUD_OD', 180.2197820490057)\n('PBL_WRF_M', 360.39211342551494)\n('T2_K', 360.5232141668146)\n('CAPE', 361.05967018821025)\n('PRATE_MMpH', 361.1262997713956)\n('SNOWEW_M', 361.23838112571025)\n('CLDTOP_KM', 361.3054504394531)\n('PBL_YSU_M', 361.45016063343394)\n('SNOWAGE_HR', 362.0641340775923)\n('SWSFC_WpM2', 362.6420177112926)\n('TSURF_K', 401.88824185458094)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#insights",
    "href": "publications_and_projects/data/Autoencoder.html#insights",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10, test_loss_list_P_25)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10, 'P25': test_loss_list_P_25, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_MLP_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n0.7\n0.1\n0.4\n\n\n5\nU10_MpS\n0.7\n0.8\n0.7\n\n\n9\nSOLM_M3pM3\n110.7\n42.1\n76.4\n\n\n4\nCLOUD_OD\n180.2\n9.9\n95.1\n\n\n3\nPRATE_MMpH\n361.1\n189.7\n275.4\n\n\n1\nSNOWEW_M\n361.2\n189.7\n275.5\n\n\n7\nT2_K\n360.5\n191.0\n275.8\n\n\n11\nCAPE\n361.1\n190.9\n276.0\n\n\n2\nSNOWAGE_HR\n362.1\n190.0\n276.0\n\n\n12\nPBL_WRF_M\n360.4\n192.0\n276.2\n\n\n10\nCLDTOP_KM\n361.3\n192.2\n276.8\n\n\n13\nPBL_YSU_M\n361.5\n194.4\n277.9\n\n\n8\nSWSFC_WpM2\n362.6\n199.2\n280.9\n\n\n0\nTSURF_K\n401.9\n190.1\n296.0"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#model-defination-1",
    "href": "publications_and_projects/data/Autoencoder.html#model-defination-1",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Model defination",
    "text": "Model defination\n\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__() \n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid \n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential( \n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        ) \n        \n    def forward(self, x):\n        return self.net(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n\n        )\n    def forward(self, x):\n        return self.decoder(x)\n\nclass Autoencoder_CNN(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_CNN, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim \n        self.activation = activation    \n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        \n    \n    def forward(self, x):\n        x = self.encoder(x)\n        # print(x.shape)\n        x = self.decoder(x)\n        return x\n\n\nmodel_auto = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_CNN(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 1024]              --\n│    └─Sequential: 2-1                   [1656, 1024]              --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 6400]       6,560,000\n        Unflatten-15           [-1, 64, 10, 10]               0\n  ConvTranspose2d-16           [-1, 32, 20, 20]          18,464\n             GELU-17           [-1, 32, 20, 20]               0\n           Conv2d-18           [-1, 32, 20, 20]           9,248\n             GELU-19           [-1, 32, 20, 20]               0\n  ConvTranspose2d-20           [-1, 16, 40, 40]           4,624\n             GELU-21           [-1, 16, 40, 40]               0\n           Conv2d-22           [-1, 16, 40, 40]           2,320\n             GELU-23           [-1, 16, 40, 40]               0\n  ConvTranspose2d-24            [-1, 2, 80, 80]             290\n          Decoder-25            [-1, 2, 80, 80]               0\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.80\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.44\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "href": "publications_and_projects/data/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "training on all channel and predicting on all channel",
    "text": "training on all channel and predicting on all channel\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 50 #200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 10 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/50] Loss: 1620.3361\nAverage Test Loss: 377.9475\nEpoch [11/50] Loss: 249.2443\nAverage Test Loss: 228.7919\nEpoch [21/50] Loss: 168.6332\nAverage Test Loss: 172.4825\nEpoch [31/50] Loss: 102.0034\nAverage Test Loss: 111.1758\nEpoch [41/50] Loss: 65.1520\nAverage Test Loss: 60.7862\n\n\n  2%|▏         | 1/50 [00:04&lt;04:03,  4.96s/it] 22%|██▏       | 11/50 [00:58&lt;03:15,  5.00s/it] 42%|████▏     | 21/50 [01:33&lt;01:40,  3.45s/it] 62%|██████▏   | 31/50 [02:21&lt;01:14,  3.90s/it] 80%|████████  | 40/50 [02:49&lt;00:32,  3.22s/it] 82%|████████▏ | 41/50 [02:53&lt;00:30,  3.39s/it]100%|██████████| 50/50 [03:30&lt;00:00,  4.22s/it]\n\n\n\n\n\n\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nAverage Test Loss: 20.4597\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in14_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "href": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-output-p25",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Single channel input single channel output (P25)",
    "text": "Single channel input single channel output (P25)\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto = []\ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 462.4725\nEpoch [21/200] Loss: 212.0043\nEpoch [41/200] Loss: 213.3173\nEpoch [61/200] Loss: 234.0997\nEpoch [81/200] Loss: 211.7991\nEpoch [101/200] Loss: 216.9750\nEpoch [121/200] Loss: 211.1435\nEpoch [141/200] Loss: 214.2518\nEpoch [161/200] Loss: 209.3667\nEpoch [181/200] Loss: 208.2786\nAverage Test Loss: 196.8076\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 540.4606\nEpoch [21/200] Loss: 216.1673\nEpoch [41/200] Loss: 211.7699\nEpoch [61/200] Loss: 211.9133\nEpoch [81/200] Loss: 214.4790\nEpoch [101/200] Loss: 212.5737\nEpoch [121/200] Loss: 212.8330\nEpoch [141/200] Loss: 209.0824\nEpoch [161/200] Loss: 207.7827\nEpoch [181/200] Loss: 208.2993\nAverage Test Loss: 191.6095\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 590.1837\nEpoch [21/200] Loss: 211.4096\nEpoch [41/200] Loss: 213.1281\nEpoch [61/200] Loss: 208.5330\nEpoch [81/200] Loss: 209.0068\nEpoch [101/200] Loss: 211.9243\nEpoch [121/200] Loss: 212.2428\nEpoch [141/200] Loss: 210.3188\nEpoch [161/200] Loss: 211.6509\nEpoch [181/200] Loss: 212.8352\nAverage Test Loss: 191.7548\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 750.4891\nEpoch [21/200] Loss: 213.9414\nEpoch [41/200] Loss: 214.3703\nEpoch [61/200] Loss: 209.9882\nEpoch [81/200] Loss: 210.4570\nEpoch [101/200] Loss: 211.3801\nEpoch [121/200] Loss: 214.6792\nEpoch [141/200] Loss: 210.8055\nEpoch [161/200] Loss: 214.0414\nEpoch [181/200] Loss: 206.5140\nAverage Test Loss: 194.9402\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 563.3871\nEpoch [21/200] Loss: 63.2729\nEpoch [41/200] Loss: 23.8580\nEpoch [61/200] Loss: 15.2393\nEpoch [81/200] Loss: 11.0129\nEpoch [101/200] Loss: 9.3018\nEpoch [121/200] Loss: 8.1090\nEpoch [141/200] Loss: 7.4336\nEpoch [161/200] Loss: 8.8601\nEpoch [181/200] Loss: 6.1420\nAverage Test Loss: 7.4988\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 684.8967\nEpoch [21/200] Loss: 111.6166\nEpoch [41/200] Loss: 22.6558\nEpoch [61/200] Loss: 10.9635\nEpoch [81/200] Loss: 12.9050\nEpoch [101/200] Loss: 6.2923\nEpoch [121/200] Loss: 9.0327\nEpoch [141/200] Loss: 4.1764\nEpoch [161/200] Loss: 3.8160\nEpoch [181/200] Loss: 3.3957\nAverage Test Loss: 3.1739\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 609.8626\nEpoch [21/200] Loss: 58.6357\nEpoch [41/200] Loss: 14.3107\nEpoch [61/200] Loss: 10.6218\nEpoch [81/200] Loss: 6.9871\nEpoch [101/200] Loss: 5.3444\nEpoch [121/200] Loss: 4.6127\nEpoch [141/200] Loss: 4.0642\nEpoch [161/200] Loss: 3.5636\nEpoch [181/200] Loss: 3.2190\nAverage Test Loss: 2.9050\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 458.4550\nEpoch [21/200] Loss: 213.3441\nEpoch [41/200] Loss: 211.7652\nEpoch [61/200] Loss: 212.4970\nEpoch [81/200] Loss: 210.7104\nEpoch [101/200] Loss: 208.7177\nEpoch [121/200] Loss: 212.2414\nEpoch [141/200] Loss: 207.2175\nEpoch [161/200] Loss: 211.3121\nEpoch [181/200] Loss: 208.7926\nAverage Test Loss: 191.9081\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 760.0155\nEpoch [21/200] Loss: 224.7744\nEpoch [41/200] Loss: 215.4670\nEpoch [61/200] Loss: 214.6593\nEpoch [81/200] Loss: 211.3864\nEpoch [101/200] Loss: 214.5846\nEpoch [121/200] Loss: 215.2816\nEpoch [141/200] Loss: 210.3355\nEpoch [161/200] Loss: 213.1056\nEpoch [181/200] Loss: 209.3009\nAverage Test Loss: 193.2200\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 518.8583\nEpoch [21/200] Loss: 183.2842\nEpoch [41/200] Loss: 106.5493\nEpoch [61/200] Loss: 47.8360\nEpoch [81/200] Loss: 17.4837\nEpoch [101/200] Loss: 12.6859\nEpoch [121/200] Loss: 9.7521\nEpoch [141/200] Loss: 8.4768\nEpoch [161/200] Loss: 7.1218\nEpoch [181/200] Loss: 7.4880\nAverage Test Loss: 5.5514\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 603.6223\nEpoch [21/200] Loss: 216.0129\nEpoch [41/200] Loss: 212.5011\nEpoch [61/200] Loss: 215.2624\nEpoch [81/200] Loss: 210.4688\nEpoch [101/200] Loss: 211.2793\nEpoch [121/200] Loss: 208.5935\nEpoch [141/200] Loss: 209.1620\nEpoch [161/200] Loss: 208.9249\nEpoch [181/200] Loss: 207.8753\nAverage Test Loss: 205.4745\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 480.7086\nEpoch [21/200] Loss: 215.4105\nEpoch [41/200] Loss: 211.5271\nEpoch [61/200] Loss: 211.8129\nEpoch [81/200] Loss: 208.5153\nEpoch [101/200] Loss: 208.6479\nEpoch [121/200] Loss: 215.5420\nEpoch [141/200] Loss: 207.9001\nEpoch [161/200] Loss: 206.7139\nEpoch [181/200] Loss: 208.8671\nAverage Test Loss: 191.5815\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 487.6684\nEpoch [21/200] Loss: 210.6665\nEpoch [41/200] Loss: 211.8588\nEpoch [61/200] Loss: 209.3883\nEpoch [81/200] Loss: 210.8873\nEpoch [101/200] Loss: 209.3165\nEpoch [121/200] Loss: 208.2383\nEpoch [141/200] Loss: 208.4831\nEpoch [161/200] Loss: 209.2067\nEpoch [181/200] Loss: 207.0104\nAverage Test Loss: 190.8408\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 538.0652\nEpoch [21/200] Loss: 213.2395\nEpoch [41/200] Loss: 209.5626\nEpoch [61/200] Loss: 218.8836\nEpoch [81/200] Loss: 213.1157\nEpoch [101/200] Loss: 211.7407\nEpoch [121/200] Loss: 214.4044\nEpoch [141/200] Loss: 209.6131\nEpoch [161/200] Loss: 209.1906\nEpoch [181/200] Loss: 207.2547\nAverage Test Loss: 196.5285\n\n\n  0%|          | 1/200 [00:00&lt;02:14,  1.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:24&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.58it/s] 50%|█████     | 101/200 [00:28&lt;00:27,  3.61it/s] 60%|██████    | 121/200 [00:34&lt;00:21,  3.60it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.48it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.43it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.47it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.44it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.37it/s] 60%|██████    | 121/200 [00:35&lt;00:21,  3.76it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.42it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.61it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.63it/s] 10%|█         | 21/200 [00:05&lt;00:46,  3.82it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:37,  3.14it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.40it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.49it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.52it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.57it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.46it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.34it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.37it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.45it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:11&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.52it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.52it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.32it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.49it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.42it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.10it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.39it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:36&lt;00:23,  3.31it/s] 70%|███████   | 141/200 [00:42&lt;00:17,  3.30it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.46it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.41it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.18it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.38it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.42it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.41it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.33it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.36it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.39it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.35it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.37it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.43it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.44it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto\n\n[196.80755892666903,\n 191.6094970703125,\n 191.75477183948863,\n 194.94017861106178,\n 7.4987756772474805,\n 3.1738592061129483,\n 2.9050224044106225,\n 191.90811157226562,\n 193.21996238014916,\n 5.551396109841087,\n 205.47452198375356,\n 191.58150828968394,\n 190.84084944291547,\n 196.5285311612216]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 2.9050224044106225)\n('U10_MpS', 3.1738592061129483)\n('SOLM_M3pM3', 5.551396109841087)\n('CLOUD_OD', 7.4987756772474805)\n('PBL_WRF_M', 190.84084944291547)\n('CAPE', 191.58150828968394)\n('SNOWEW_M', 191.6094970703125)\n('SNOWAGE_HR', 191.75477183948863)\n('T2_K', 191.90811157226562)\n('SWSFC_WpM2', 193.21996238014916)\n('PRATE_MMpH', 194.94017861106178)\n('PBL_YSU_M', 196.5285311612216)\n('TSURF_K', 196.80755892666903)\n('CLDTOP_KM', 205.47452198375356)\nLowest 3 names: ['V10_MpS', 'U10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1000, 1, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1000, 1, 80, 80]         --\n├─Encoder: 1-1                           [1000, 512]               --\n│    └─Sequential: 2-1                   [1000, 512]               --\n│    │    └─Conv2d: 3-1                  [1000, 8, 40, 40]         80\n│    │    └─GELU: 3-2                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-3                  [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-4                    [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-5                  [1000, 16, 20, 20]        1,168\n│    │    └─GELU: 3-6                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-8                    [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1000, 32, 10, 10]        4,640\n│    │    └─GELU: 3-10                   [1000, 32, 10, 10]        --\n│    │    └─Flatten: 3-11                [1000, 3200]              --\n│    │    └─Linear: 3-12                 [1000, 512]               1,638,912\n├─Decoder: 1-2                           [1000, 1, 80, 80]         --\n│    └─Sequential: 2-2                   [1000, 1, 80, 80]         --\n│    │    └─Linear: 3-13                 [1000, 3200]              1,641,600\n│    │    └─Unflatten: 3-14              [1000, 32, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1000, 16, 20, 20]        4,624\n│    │    └─GELU: 3-16                   [1000, 16, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1000, 16, 20, 20]        2,320\n│    │    └─GELU: 3-18                   [1000, 16, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1000, 8, 40, 40]         1,160\n│    │    └─GELU: 3-20                   [1000, 8, 40, 40]         --\n│    │    └─Conv2d: 3-21                 [1000, 8, 40, 40]         584\n│    │    └─GELU: 3-22                   [1000, 8, 40, 40]         --\n│    │    └─ConvTranspose2d: 3-23        [1000, 1, 80, 80]         73\n==========================================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 12.24\n==========================================================================================\nInput size (MB): 25.60\nForward/backward pass size (MB): 720.90\nParams size (MB): 13.19\nEstimated Total Size (MB): 759.69\n=========================================================================================="
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "href": "publications_and_projects/data/Autoencoder.html#single-channel-input-multiple-channel-output-p10",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Single channel input multiple channel output (P10)",
    "text": "Single channel input multiple channel output (P10)\n\ntest_loss_list_P_10_conv_auto = []\ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_CNN(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 831.9463\nEpoch [21/200] Loss: 413.6695\nEpoch [41/200] Loss: 416.1795\nEpoch [61/200] Loss: 414.0692\nEpoch [81/200] Loss: 408.8671\nEpoch [101/200] Loss: 408.6212\nEpoch [121/200] Loss: 408.1124\nEpoch [141/200] Loss: 407.2696\nEpoch [161/200] Loss: 419.0648\nEpoch [181/200] Loss: 411.9767\nAverage Test Loss: 367.9784\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1193.0016\nEpoch [21/200] Loss: 413.8656\nEpoch [41/200] Loss: 416.9739\nEpoch [61/200] Loss: 405.7197\nEpoch [81/200] Loss: 414.4462\nEpoch [101/200] Loss: 403.9555\nEpoch [121/200] Loss: 406.6566\nEpoch [141/200] Loss: 404.8177\nEpoch [161/200] Loss: 414.5372\nEpoch [181/200] Loss: 418.9334\nAverage Test Loss: 373.6601\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.4868\nEpoch [21/200] Loss: 410.2950\nEpoch [41/200] Loss: 414.8993\nEpoch [61/200] Loss: 405.8627\nEpoch [81/200] Loss: 416.4907\nEpoch [101/200] Loss: 413.8539\nEpoch [121/200] Loss: 406.4541\nEpoch [141/200] Loss: 404.2276\nEpoch [161/200] Loss: 407.0577\nEpoch [181/200] Loss: 402.6227\nAverage Test Loss: 393.0328\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1028.3382\nEpoch [21/200] Loss: 434.1988\nEpoch [41/200] Loss: 412.9816\nEpoch [61/200] Loss: 405.9959\nEpoch [81/200] Loss: 404.1220\nEpoch [101/200] Loss: 415.6019\nEpoch [121/200] Loss: 403.8031\nEpoch [141/200] Loss: 407.2635\nEpoch [161/200] Loss: 404.3506\nEpoch [181/200] Loss: 409.8351\nAverage Test Loss: 367.3246\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1599.0141\nEpoch [21/200] Loss: 192.7198\nEpoch [41/200] Loss: 68.0225\nEpoch [61/200] Loss: 44.4028\nEpoch [81/200] Loss: 32.4964\nEpoch [101/200] Loss: 25.7723\nEpoch [121/200] Loss: 23.6108\nEpoch [141/200] Loss: 19.3957\nEpoch [161/200] Loss: 19.0788\nEpoch [181/200] Loss: 21.9598\nAverage Test Loss: 18.7717\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1023.2784\nEpoch [21/200] Loss: 280.1561\nEpoch [41/200] Loss: 60.3793\nEpoch [61/200] Loss: 29.8831\nEpoch [81/200] Loss: 19.7393\nEpoch [101/200] Loss: 16.8795\nEpoch [121/200] Loss: 12.9342\nEpoch [141/200] Loss: 11.2801\nEpoch [161/200] Loss: 10.1249\nEpoch [181/200] Loss: 11.3411\nAverage Test Loss: 8.8163\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1439.9822\nEpoch [21/200] Loss: 142.6731\nEpoch [41/200] Loss: 39.2041\nEpoch [61/200] Loss: 20.6722\nEpoch [81/200] Loss: 16.5665\nEpoch [101/200] Loss: 12.4099\nEpoch [121/200] Loss: 10.8300\nEpoch [141/200] Loss: 9.6913\nEpoch [161/200] Loss: 8.9197\nEpoch [181/200] Loss: 10.7642\nAverage Test Loss: 23.4499\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1037.4250\nEpoch [21/200] Loss: 413.2349\nEpoch [41/200] Loss: 411.9822\nEpoch [61/200] Loss: 406.0331\nEpoch [81/200] Loss: 408.5417\nEpoch [101/200] Loss: 403.6246\nEpoch [121/200] Loss: 409.9414\nEpoch [141/200] Loss: 402.6003\nEpoch [161/200] Loss: 401.2209\nEpoch [181/200] Loss: 403.0725\nAverage Test Loss: 366.0120\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1358.9732\nEpoch [21/200] Loss: 419.2162\nEpoch [41/200] Loss: 413.3607\nEpoch [61/200] Loss: 409.3754\nEpoch [81/200] Loss: 411.0923\nEpoch [101/200] Loss: 404.1099\nEpoch [121/200] Loss: 409.2167\nEpoch [141/200] Loss: 408.5509\nEpoch [161/200] Loss: 410.1073\nEpoch [181/200] Loss: 407.5864\nAverage Test Loss: 364.5863\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1237.3417\nEpoch [21/200] Loss: 337.7638\nEpoch [41/200] Loss: 223.9836\nEpoch [61/200] Loss: 98.1059\nEpoch [81/200] Loss: 46.6351\nEpoch [101/200] Loss: 31.9773\nEpoch [121/200] Loss: 26.2758\nEpoch [141/200] Loss: 19.9056\nEpoch [161/200] Loss: 17.7226\nEpoch [181/200] Loss: 15.4444\nAverage Test Loss: 14.0181\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1227.4362\nEpoch [21/200] Loss: 430.1239\nEpoch [41/200] Loss: 414.0475\nEpoch [61/200] Loss: 414.5093\nEpoch [81/200] Loss: 412.3910\nEpoch [101/200] Loss: 413.5266\nEpoch [121/200] Loss: 408.5565\nEpoch [141/200] Loss: 404.2547\nEpoch [161/200] Loss: 417.1860\nEpoch [181/200] Loss: 408.9205\nAverage Test Loss: 367.3491\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1362.6006\nEpoch [21/200] Loss: 417.5348\nEpoch [41/200] Loss: 409.7356\nEpoch [61/200] Loss: 411.1969\nEpoch [81/200] Loss: 413.3693\nEpoch [101/200] Loss: 414.5004\nEpoch [121/200] Loss: 405.8445\nEpoch [141/200] Loss: 404.8460\nEpoch [161/200] Loss: 415.0352\nEpoch [181/200] Loss: 414.2297\nAverage Test Loss: 366.4497\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1069.1285\nEpoch [21/200] Loss: 408.2721\nEpoch [41/200] Loss: 408.3345\nEpoch [61/200] Loss: 411.1250\nEpoch [81/200] Loss: 406.6349\nEpoch [101/200] Loss: 409.1261\nEpoch [121/200] Loss: 403.4630\nEpoch [141/200] Loss: 405.4774\nEpoch [161/200] Loss: 403.7881\nEpoch [181/200] Loss: 408.1804\nAverage Test Loss: 367.3865\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1065.3006\nEpoch [21/200] Loss: 443.1136\nEpoch [41/200] Loss: 407.0351\nEpoch [61/200] Loss: 409.3611\nEpoch [81/200] Loss: 403.8964\nEpoch [101/200] Loss: 402.2005\nEpoch [121/200] Loss: 402.1708\nEpoch [141/200] Loss: 403.3740\nEpoch [161/200] Loss: 398.9786\nEpoch [181/200] Loss: 408.3548\nAverage Test Loss: 380.6446\n\n\n  0%|          | 1/200 [00:00&lt;01:02,  3.18it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.39it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.39it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.44it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.40it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.38it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.40it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.38it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.40it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.35it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.38it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.36it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.19it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.36it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.48it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.38it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.43it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.41it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.49it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.38it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.36it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:30&lt;00:29,  3.38it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.39it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.37it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.40it/s]\n  0%|          | 1/200 [00:00&lt;00:53,  3.73it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.44it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.35it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.35it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.37it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.39it/s] 80%|████████  | 161/200 [00:47&lt;00:10,  3.56it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.44it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.53it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.50it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:38,  3.66it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.51it/s] 50%|█████     | 101/200 [00:28&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:34&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.41it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.54it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.37it/s] 30%|███       | 61/200 [00:17&lt;00:41,  3.39it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.41it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.42it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.41it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.42it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.41it/s]\n  0%|          | 1/200 [00:00&lt;00:52,  3.82it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.43it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.45it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.45it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.46it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.36it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:56,  3.51it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.38it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.38it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.45it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.42it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.34it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.38it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.35it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.39it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.37it/s] 20%|██        | 41/200 [00:12&lt;00:46,  3.42it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.40it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.34it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.42it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.36it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.46it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.38it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.30it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.45it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.36it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.44it/s] 80%|████████  | 161/200 [00:47&lt;00:11,  3.45it/s] 90%|█████████ | 181/200 [00:53&lt;00:05,  3.41it/s]100%|██████████| 200/200 [00:59&lt;00:00,  3.39it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:05&lt;00:52,  3.42it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.51it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.50it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.43it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.40it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.43it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.43it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.39it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.37it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.42it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.55it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.52it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.46it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.44it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.45it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.40it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.37it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.45it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.44it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto\n\n[367.97836858575994,\n 373.660117409446,\n 393.0328174937855,\n 367.32460992986506,\n 18.771740219809793,\n 8.816288991407914,\n 23.44990175420588,\n 366.0119906338778,\n 364.58628151633525,\n 14.018102992664684,\n 367.34911554509944,\n 366.4497375488281,\n 367.38653841885656,\n 380.64462835138494]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 8.816288991407914)\n('SOLM_M3pM3', 14.018102992664684)\n('CLOUD_OD', 18.771740219809793)\n('V10_MpS', 23.44990175420588)\n('SWSFC_WpM2', 364.58628151633525)\n('T2_K', 366.0119906338778)\n('CAPE', 366.4497375488281)\n('PRATE_MMpH', 367.32460992986506)\n('CLDTOP_KM', 367.34911554509944)\n('PBL_WRF_M', 367.38653841885656)\n('TSURF_K', 367.97836858575994)\n('SNOWEW_M', 373.660117409446)\n('PBL_YSU_M', 380.64462835138494)\n('SNOWAGE_HR', 393.0328174937855)\nLowest 3 names: ['U10_MpS', 'SOLM_M3pM3', 'CLOUD_OD']"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#insights-1",
    "href": "publications_and_projects/data/Autoencoder.html#insights-1",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto, test_loss_list_P_25_conv_auto)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto, 'P25': test_loss_list_P_25_conv_auto, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto.csv', index=False)\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n5\nU10_MpS\n8.8\n3.2\n6.0\n\n\n9\nSOLM_M3pM3\n14.0\n5.6\n9.8\n\n\n4\nCLOUD_OD\n18.8\n7.5\n13.1\n\n\n6\nV10_MpS\n23.4\n2.9\n13.2\n\n\n8\nSWSFC_WpM2\n364.6\n193.2\n278.9\n\n\n7\nT2_K\n366.0\n191.9\n279.0\n\n\n11\nCAPE\n366.4\n191.6\n279.0\n\n\n12\nPBL_WRF_M\n367.4\n190.8\n279.1\n\n\n3\nPRATE_MMpH\n367.3\n194.9\n281.1\n\n\n0\nTSURF_K\n368.0\n196.8\n282.4\n\n\n1\nSNOWEW_M\n373.7\n191.6\n282.6\n\n\n10\nCLDTOP_KM\n367.3\n205.5\n286.4\n\n\n13\nPBL_YSU_M\n380.6\n196.5\n288.6\n\n\n2\nSNOWAGE_HR\n393.0\n191.8\n292.4"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "href": "publications_and_projects/data/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-conv",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Training on top 4 channel and predicting on all channel CONV",
    "text": "Training on top 4 channel and predicting on all channel CONV\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 80\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\n        model.eval()  # Set the model to evaluation mode\n        test_loss = 0.0\n\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs = inputs.to(device)\n                targets = targets.to(device)\n\n                # Forward pass\n                outputs = model(inputs)\n\n                # Calculate the loss\n                loss = criterion(outputs, targets)\n\n                test_loss += loss.item()\n\n        # Print the average test loss\n        average_test_loss = test_loss / len(test_loader)\n        # test_loss_list_P_25.append(average_test_loss)\n        print(f\"Average Test Loss: {average_test_loss:.4f}\")\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_CNN                          [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           [1656, 2048]              --\n│    └─Sequential: 2-1                   [1656, 2048]              --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           [1656, 2, 80, 80]         --\n│    └─Sequential: 2-2                   [1656, 2, 80, 80]         --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/80] Loss: 1343.9761\nAverage Test Loss: 313.2759\nEpoch [21/80] Loss: 51.4848\nAverage Test Loss: 41.3688\nEpoch [41/80] Loss: 19.3960\nAverage Test Loss: 15.7720\nEpoch [61/80] Loss: 10.1627\nAverage Test Loss: 6.6679\nAverage Test Loss: 3.3631\n\n\n  1%|▏         | 1/80 [00:02&lt;02:40,  2.03s/it] 26%|██▋       | 21/80 [00:35&lt;01:40,  1.71s/it] 51%|█████▏    | 41/80 [01:09&lt;01:07,  1.72s/it] 76%|███████▋  | 61/80 [01:43&lt;00:32,  1.71s/it]100%|██████████| 80/80 [02:15&lt;00:00,  1.70s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_in4_out2.pt')\nmodel = Autoencoder_CNN(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#model-defination-2",
    "href": "publications_and_projects/data/Autoencoder.html#model-defination-2",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Model defination",
    "text": "Model defination\n\n\nclass Encoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1, c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Encoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels=self.num_input_channels, out_channels=self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=2 * self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels=2 * self.c_hid, out_channels=4*self.c_hid, kernel_size=3, stride=2, padding=1),\n            self.activation(),\n            nn.Flatten(),\n            nn.Linear((4 * self.c_hid) * self.image_size//8 * self.image_size//8, self.latent_dim)\n        )\n    def forward(self, x):\n        return self.net(x)\n    \nclass Decoder(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU):\n        super(Decoder, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.decoder = nn.Sequential(\n            nn.Linear(self.latent_dim, (4 * self.c_hid) * self.image_size//8 * self.image_size//8),\n            nn.Unflatten(1, (4*self.c_hid, self.image_size//8, self.image_size//8)),\n            nn.ConvTranspose2d(in_channels=4*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = 2*self.c_hid, out_channels= 2*self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = 2*self.c_hid, out_channels = self.c_hid, kernel_size=3, stride=2, padding=1, output_padding=1),\n            self.activation(),\n            nn.Conv2d(in_channels = self.c_hid,out_channels= self.c_hid, kernel_size=3, padding=1),\n            self.activation(),\n            nn.ConvTranspose2d(in_channels = self.c_hid, out_channels= self.num_output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n        )\n    def forward(self, x):\n        return self.decoder(x)\n    \nclass Autoencoder_UNET(nn.Module):\n    def __init__(self, image_size, num_input_channels, num_output_channels = 1,c_hid = 16, latent_dim = 1024, activation= nn.GELU, encoder = Encoder, decoder = Decoder):\n        super(Autoencoder_UNET, self).__init__()\n        self.image_size = image_size\n        self.num_input_channels = num_input_channels\n        self.num_output_channels = num_output_channels\n        self.c_hid = c_hid\n        self.latent_dim = latent_dim\n        self.activation = activation\n        self.encoder = encoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n        self.decoder = decoder(self.image_size, self.num_input_channels, self.num_output_channels, self.c_hid, self.latent_dim, self.activation)\n    # def forward(self, x):\n    #     x = self.encoder(x)\n    #     # print(x.shape)\n    #     x = self.decoder(x)\n    #     return x\n    def forward(self, x):\n        conv1 = self.encoder.net[0]\n        activation1 = self.encoder.net[1]\n        conv2 = self.encoder.net[2]\n        activation2 = self.encoder.net[3]\n        conv3 = self.encoder.net[4]\n        activation3 = self.encoder.net[5]\n        conv4 = self.encoder.net[6]\n        activation4 = self.encoder.net[7]\n        conv5 = self.encoder.net[8]\n        activation5 = self.encoder.net[9]\n        flatten = self.encoder.net[10]\n        linear = self.encoder.net[11]\n        lineart = self.decoder.decoder[0]\n        unflattent = self.decoder.decoder[1]\n        convt2 = self.decoder.decoder[2]\n        activation7 = self.decoder.decoder[3]\n        convt3 = self.decoder.decoder[4]\n        activation8 = self.decoder.decoder[5]\n        convt4 = self.decoder.decoder[6]\n        activation9 = self.decoder.decoder[7]\n        convt5 = self.decoder.decoder[8]\n        activation10 = self.decoder.decoder[9]\n        convt6 = self.decoder.decoder[10]\n        x1 = activation1(conv1(x))\n        x2 = activation2(conv2(x1))\n        x3 = activation3(conv3(x2))\n        x4 = activation4(conv4(x3))\n        x5 = activation5(conv5(x4))\n        x6 = flatten(x5)\n        x7 = linear(x6)\n        x8 = lineart(x7)\n        x9 = unflattent(x8)\n        x10 = activation7(convt2(x9+x5))\n        x11 = activation8(convt3(x10+x4))\n        x12 = activation9(convt4(x11+x3))\n        x13 = activation10(convt5(x12+x2))\n        x13 = convt6(x13+x1)\n        return x13\n\n\nmodel_auto = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 16, latent_dim = 1024, activation= nn.GELU)\nmodel_auto\ndummy_input = torch.randn(11, 14, 80, 80)  # Assuming input size of (batch_size, num_num_input_channels, height, width)\nout = model_auto(dummy_input)\nprint(out.shape)    \nprint(model_auto)\n\ntorch.Size([11, 2, 80, 80])\nAutoencoder_UNET(\n  (encoder): Encoder(\n    (net): Sequential(\n      (0): Conv2d(14, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (1): GELU(approximate='none')\n      (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): Flatten(start_dim=1, end_dim=-1)\n      (11): Linear(in_features=6400, out_features=1024, bias=True)\n    )\n  )\n  (decoder): Decoder(\n    (decoder): Sequential(\n      (0): Linear(in_features=1024, out_features=6400, bias=True)\n      (1): Unflatten(dim=1, unflattened_size=(64, 10, 10))\n      (2): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (3): GELU(approximate='none')\n      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (5): GELU(approximate='none')\n      (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n      (7): GELU(approximate='none')\n      (8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (9): GELU(approximate='none')\n      (10): ConvTranspose2d(16, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    )\n  )\n)\n\n\n\nfrom torchinfo import summary\nsummary(model_auto, input_size=(1656, 14, 80, 80))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 16, 40, 40]        2,032\n│    │    └─GELU: 3-2                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-4                    [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 32, 20, 20]        4,640\n│    │    └─GELU: 3-6                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-7                  [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-8                    [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-9                  [1656, 64, 10, 10]        18,496\n│    │    └─GELU: 3-10                   [1656, 64, 10, 10]        --\n│    │    └─Flatten: 3-11                [1656, 6400]              --\n│    │    └─Linear: 3-12                 [1656, 1024]              6,554,624\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 6400]              6,560,000\n│    │    └─Unflatten: 3-14              [1656, 64, 10, 10]        --\n│    │    └─ConvTranspose2d: 3-15        [1656, 32, 20, 20]        18,464\n│    │    └─GELU: 3-16                   [1656, 32, 20, 20]        --\n│    │    └─Conv2d: 3-17                 [1656, 32, 20, 20]        9,248\n│    │    └─GELU: 3-18                   [1656, 32, 20, 20]        --\n│    │    └─ConvTranspose2d: 3-19        [1656, 16, 40, 40]        4,624\n│    │    └─GELU: 3-20                   [1656, 16, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 16, 40, 40]        2,320\n│    │    └─GELU: 3-22                   [1656, 16, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         290\n==========================================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 85.34\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 2387.61\nParams size (MB): 52.75\nEstimated Total Size (MB): 3033.86\n==========================================================================================\n\n\n\nfrom torchsummary import summary\nsummary(model_auto, input_size=(14, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 40, 40]           2,032\n              GELU-2           [-1, 16, 40, 40]               0\n            Conv2d-3           [-1, 16, 40, 40]           2,320\n              GELU-4           [-1, 16, 40, 40]               0\n            Conv2d-5           [-1, 32, 20, 20]           4,640\n              GELU-6           [-1, 32, 20, 20]               0\n            Conv2d-7           [-1, 32, 20, 20]           9,248\n              GELU-8           [-1, 32, 20, 20]               0\n            Conv2d-9           [-1, 64, 10, 10]          18,496\n             GELU-10           [-1, 64, 10, 10]               0\n          Flatten-11                 [-1, 6400]               0\n           Linear-12                 [-1, 1024]       6,554,624\n           Linear-13                 [-1, 6400]       6,560,000\n        Unflatten-14           [-1, 64, 10, 10]               0\n  ConvTranspose2d-15           [-1, 32, 20, 20]          18,464\n             GELU-16           [-1, 32, 20, 20]               0\n           Conv2d-17           [-1, 32, 20, 20]           9,248\n             GELU-18           [-1, 32, 20, 20]               0\n  ConvTranspose2d-19           [-1, 16, 40, 40]           4,624\n             GELU-20           [-1, 16, 40, 40]               0\n           Conv2d-21           [-1, 16, 40, 40]           2,320\n             GELU-22           [-1, 16, 40, 40]               0\n  ConvTranspose2d-23            [-1, 2, 80, 80]             290\n================================================================\nTotal params: 13,186,306\nTrainable params: 13,186,306\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.34\nForward/backward pass size (MB): 2.69\nParams size (MB): 50.30\nEstimated Total Size (MB): 53.34\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "href": "publications_and_projects/data/Autoencoder.html#training-on-all-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Training on all channel and predicting on all channel UNET",
    "text": "Training on all channel and predicting on all channel UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 14, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 200\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 14, 80, 80) (332, 14, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        8,128\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 106,001,410\nTrainable params: 106,001,410\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.09\n==========================================================================================\nInput size (MB): 593.51\nForward/backward pass size (MB): 9014.58\nParams size (MB): 424.01\nEstimated Total Size (MB): 10032.09\n==========================================================================================\nEpoch [1/200] Loss: 3185.9086\nEpoch [21/200] Loss: 238.7220\nEpoch [41/200] Loss: 90.1479\nEpoch [61/200] Loss: 19.3867\nEpoch [81/200] Loss: 7.6679\nEpoch [101/200] Loss: 5.9376\nEpoch [121/200] Loss: 7.9944\nEpoch [141/200] Loss: 5.3282\nEpoch [161/200] Loss: 4.2981\nEpoch [181/200] Loss: 4.3096\nAverage Test Loss: 3.9575\n\n\n  0%|          | 1/200 [00:03&lt;11:37,  3.50s/it] 10%|█         | 21/200 [01:08&lt;10:15,  3.44s/it] 20%|██        | 41/200 [02:19&lt;09:16,  3.50s/it] 30%|███       | 61/200 [03:44&lt;09:57,  4.30s/it] 40%|████      | 81/200 [05:06&lt;08:00,  4.04s/it] 50%|█████     | 101/200 [06:30&lt;07:00,  4.24s/it] 60%|██████    | 121/200 [07:57&lt;05:25,  4.12s/it] 70%|███████   | 141/200 [09:21&lt;03:59,  4.06s/it] 80%|████████  | 161/200 [10:38&lt;02:34,  3.97s/it] 90%|█████████ | 181/200 [11:58&lt;01:13,  3.87s/it]100%|██████████| 200/200 [13:15&lt;00:00,  3.98s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in14_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 14, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in14_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "href": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-outputp25-unet",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Single channel input single channel output(P25) UNET",
    "text": "Single channel input single channel output(P25) UNET\n\ntarget_var_96_list =['TSURF_K',\n       'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n       'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n       'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 14, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 14, 80, 80), (332, 14, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\ntest_loss_list_P_25_conv_auto_unet = [] \ny_channel = 0 # selecting P25 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_25_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 2015.0104\nEpoch [21/200] Loss: 224.4622\nEpoch [41/200] Loss: 217.3994\nEpoch [61/200] Loss: 213.6307\nEpoch [81/200] Loss: 211.9555\nEpoch [101/200] Loss: 212.0086\nEpoch [121/200] Loss: 210.0710\nEpoch [141/200] Loss: 216.9347\nEpoch [161/200] Loss: 214.3995\nEpoch [181/200] Loss: 209.9813\nAverage Test Loss: 192.8667\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 720.3883\nEpoch [21/200] Loss: 215.2669\nEpoch [41/200] Loss: 212.9412\nEpoch [61/200] Loss: 210.6425\nEpoch [81/200] Loss: 209.0133\nEpoch [101/200] Loss: 216.0626\nEpoch [121/200] Loss: 209.8225\nEpoch [141/200] Loss: 209.2404\nEpoch [161/200] Loss: 212.0243\nEpoch [181/200] Loss: 208.9202\nAverage Test Loss: 191.8743\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 656.6864\nEpoch [21/200] Loss: 215.9682\nEpoch [41/200] Loss: 211.3083\nEpoch [61/200] Loss: 213.7716\nEpoch [81/200] Loss: 210.4161\nEpoch [101/200] Loss: 208.3461\nEpoch [121/200] Loss: 210.5009\nEpoch [141/200] Loss: 210.7708\nEpoch [161/200] Loss: 208.3222\nEpoch [181/200] Loss: 208.7196\nAverage Test Loss: 198.2665\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 598.2960\nEpoch [21/200] Loss: 227.3699\nEpoch [41/200] Loss: 219.5535\nEpoch [61/200] Loss: 210.1886\nEpoch [81/200] Loss: 210.2708\nEpoch [101/200] Loss: 209.0613\nEpoch [121/200] Loss: 208.2215\nEpoch [141/200] Loss: 207.7972\nEpoch [161/200] Loss: 213.5013\nEpoch [181/200] Loss: 208.5548\nAverage Test Loss: 199.3592\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 586.9568\nEpoch [21/200] Loss: 62.2030\nEpoch [41/200] Loss: 21.8947\nEpoch [61/200] Loss: 13.4883\nEpoch [81/200] Loss: 13.7934\nEpoch [101/200] Loss: 8.8402\nEpoch [121/200] Loss: 8.3218\nEpoch [141/200] Loss: 8.3719\nEpoch [161/200] Loss: 7.2083\nEpoch [181/200] Loss: 6.5652\nAverage Test Loss: 7.9540\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 473.9317\nEpoch [21/200] Loss: 69.0522\nEpoch [41/200] Loss: 18.4941\nEpoch [61/200] Loss: 10.7839\nEpoch [81/200] Loss: 7.7059\nEpoch [101/200] Loss: 6.4899\nEpoch [121/200] Loss: 5.4837\nEpoch [141/200] Loss: 7.5561\nEpoch [161/200] Loss: 3.8554\nEpoch [181/200] Loss: 4.0530\nAverage Test Loss: 3.1830\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 561.4590\nEpoch [21/200] Loss: 71.1724\nEpoch [41/200] Loss: 19.3733\nEpoch [61/200] Loss: 9.0035\nEpoch [81/200] Loss: 7.7880\nEpoch [101/200] Loss: 5.6728\nEpoch [121/200] Loss: 8.2339\nEpoch [141/200] Loss: 3.8740\nEpoch [161/200] Loss: 4.9030\nEpoch [181/200] Loss: 4.0399\nAverage Test Loss: 3.4703\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 4816.9447\nEpoch [21/200] Loss: 234.2117\nEpoch [41/200] Loss: 222.6848\nEpoch [61/200] Loss: 215.6618\nEpoch [81/200] Loss: 213.7606\nEpoch [101/200] Loss: 213.5892\nEpoch [121/200] Loss: 211.2940\nEpoch [141/200] Loss: 213.9061\nEpoch [161/200] Loss: 214.1695\nEpoch [181/200] Loss: 208.9214\nAverage Test Loss: 192.4815\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 512.7773\nEpoch [21/200] Loss: 214.1751\nEpoch [41/200] Loss: 212.7058\nEpoch [61/200] Loss: 214.4303\nEpoch [81/200] Loss: 212.5624\nEpoch [101/200] Loss: 208.8632\nEpoch [121/200] Loss: 210.4221\nEpoch [141/200] Loss: 209.8110\nEpoch [161/200] Loss: 207.3880\nEpoch [181/200] Loss: 207.7816\nAverage Test Loss: 191.0656\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 574.8718\nEpoch [21/200] Loss: 181.5824\nEpoch [41/200] Loss: 115.5675\nEpoch [61/200] Loss: 39.5182\nEpoch [81/200] Loss: 16.1863\nEpoch [101/200] Loss: 11.7081\nEpoch [121/200] Loss: 8.9398\nEpoch [141/200] Loss: 8.2249\nEpoch [161/200] Loss: 6.6518\nEpoch [181/200] Loss: 5.9480\nAverage Test Loss: 5.3015\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 647.1522\nEpoch [21/200] Loss: 215.3608\nEpoch [41/200] Loss: 211.6647\nEpoch [61/200] Loss: 209.6907\nEpoch [81/200] Loss: 211.7445\nEpoch [101/200] Loss: 207.4063\nEpoch [121/200] Loss: 207.9890\nEpoch [141/200] Loss: 207.9899\nEpoch [161/200] Loss: 209.2992\nEpoch [181/200] Loss: 213.8687\nAverage Test Loss: 191.1090\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 589.8784\nEpoch [21/200] Loss: 213.6897\nEpoch [41/200] Loss: 215.2983\nEpoch [61/200] Loss: 216.7662\nEpoch [81/200] Loss: 213.0695\nEpoch [101/200] Loss: 214.7085\nEpoch [121/200] Loss: 213.2036\nEpoch [141/200] Loss: 212.2645\nEpoch [161/200] Loss: 207.4999\nEpoch [181/200] Loss: 213.0907\nAverage Test Loss: 191.7055\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 613.1753\nEpoch [21/200] Loss: 215.8198\nEpoch [41/200] Loss: 211.0306\nEpoch [61/200] Loss: 210.1862\nEpoch [81/200] Loss: 208.5808\nEpoch [101/200] Loss: 208.4701\nEpoch [121/200] Loss: 209.8240\nEpoch [141/200] Loss: 208.0958\nEpoch [161/200] Loss: 209.3367\nEpoch [181/200] Loss: 207.0369\nAverage Test Loss: 192.7273\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 419.9077\nEpoch [21/200] Loss: 210.7698\nEpoch [41/200] Loss: 208.6584\nEpoch [61/200] Loss: 210.8284\nEpoch [81/200] Loss: 208.8175\nEpoch [101/200] Loss: 206.5321\nEpoch [121/200] Loss: 207.8562\nEpoch [141/200] Loss: 207.3433\nEpoch [161/200] Loss: 206.2741\nEpoch [181/200] Loss: 207.5728\nAverage Test Loss: 194.3204\n\n\n  0%|          | 0/200 [00:00&lt;?, ?it/s]  0%|          | 1/200 [00:00&lt;01:04,  3.09it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.11it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.20it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.18it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.19it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.26it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.15it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.26it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.22it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.17it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:11,  3.26it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.17it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.17it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.18it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.14it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.17it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.14it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.18it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.25it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.16it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.16it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.25it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.33it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.30it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.21it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.27it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.29it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.23it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.23it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.22it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.22it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.37it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.19it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.22it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.15it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.23it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.26it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.21it/s]\n  0%|          | 1/200 [00:00&lt;00:59,  3.34it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.28it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.24it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.22it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.26it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.23it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.25it/s] 40%|████      | 81/200 [00:24&lt;00:36,  3.22it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.26it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.24it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.39it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.19it/s] 30%|███       | 61/200 [00:19&lt;00:44,  3.10it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.17it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.20it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.25it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.21it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.20it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.20it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.30it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.15it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.14it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.17it/s] 40%|████      | 81/200 [00:25&lt;00:35,  3.31it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.30it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.22it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.34it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.23it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.27it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.21it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:38,  3.11it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.18it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.19it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.24it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.20it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:18&lt;00:43,  3.20it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.19it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.18it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:57&lt;00:06,  3.01it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_25_conv_auto_unet\n\n[192.86672002618963,\n 191.87428977272728,\n 198.26652665571734,\n 199.35923628373578,\n 7.954025506973267,\n 3.18301400271329,\n 3.4703480113636362,\n 192.4814786044034,\n 191.06564331054688,\n 5.301476088437167,\n 191.10903930664062,\n 191.70547346635297,\n 192.72732405229047,\n 194.32039434259588]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_25_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('U10_MpS', 3.18301400271329)\n('V10_MpS', 3.4703480113636362)\n('SOLM_M3pM3', 5.301476088437167)\n('CLOUD_OD', 7.954025506973267)\n('SWSFC_WpM2', 191.06564331054688)\n('CLDTOP_KM', 191.10903930664062)\n('CAPE', 191.70547346635297)\n('SNOWEW_M', 191.87428977272728)\n('T2_K', 192.4814786044034)\n('PBL_WRF_M', 192.72732405229047)\n('TSURF_K', 192.86672002618963)\n('PBL_YSU_M', 194.32039434259588)\n('SNOWAGE_HR', 198.26652665571734)\n('PRATE_MMpH', 199.35923628373578)\nLowest 3 names: ['U10_MpS', 'V10_MpS', 'SOLM_M3pM3']\n\n\n\nsummary(model, input_size=(1, 80, 80))\n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                  [-1, 512]       1,638,912\n           Linear-13                 [-1, 3200]       1,641,600\n        Unflatten-14           [-1, 32, 10, 10]               0\n  ConvTranspose2d-15           [-1, 16, 20, 20]           4,624\n             GELU-16           [-1, 16, 20, 20]               0\n           Conv2d-17           [-1, 16, 20, 20]           2,320\n             GELU-18           [-1, 16, 20, 20]               0\n  ConvTranspose2d-19            [-1, 8, 40, 40]           1,160\n             GELU-20            [-1, 8, 40, 40]               0\n           Conv2d-21            [-1, 8, 40, 40]             584\n             GELU-22            [-1, 8, 40, 40]               0\n  ConvTranspose2d-23            [-1, 1, 80, 80]              73\n================================================================\nTotal params: 3,298,065\nTrainable params: 3,298,065\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.35\nParams size (MB): 12.58\nEstimated Total Size (MB): 13.95\n----------------------------------------------------------------\n\n\n\nsummary(model, input_size=(1, 80, 80)) \n\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 40, 40]              80\n              GELU-2            [-1, 8, 40, 40]               0\n            Conv2d-3            [-1, 8, 40, 40]             584\n              GELU-4            [-1, 8, 40, 40]               0\n            Conv2d-5           [-1, 16, 20, 20]           1,168\n              GELU-6           [-1, 16, 20, 20]               0\n            Conv2d-7           [-1, 16, 20, 20]           2,320\n              GELU-8           [-1, 16, 20, 20]               0\n            Conv2d-9           [-1, 32, 10, 10]           4,640\n             GELU-10           [-1, 32, 10, 10]               0\n          Flatten-11                 [-1, 3200]               0\n           Linear-12                 [-1, 1024]       3,277,824\n          Encoder-13                 [-1, 1024]               0\n           Linear-14                 [-1, 3200]       3,280,000\n        Unflatten-15           [-1, 32, 10, 10]               0\n  ConvTranspose2d-16           [-1, 16, 20, 20]           4,624\n             GELU-17           [-1, 16, 20, 20]               0\n           Conv2d-18           [-1, 16, 20, 20]           2,320\n             GELU-19           [-1, 16, 20, 20]               0\n  ConvTranspose2d-20            [-1, 8, 40, 40]           1,160\n             GELU-21            [-1, 8, 40, 40]               0\n           Conv2d-22            [-1, 8, 40, 40]             584\n             GELU-23            [-1, 8, 40, 40]               0\n  ConvTranspose2d-24            [-1, 1, 80, 80]              73\n          Decoder-25            [-1, 1, 80, 80]               0\n================================================================\nTotal params: 6,575,377\nTrainable params: 6,575,377\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.02\nForward/backward pass size (MB): 1.41\nParams size (MB): 25.08\nEstimated Total Size (MB): 26.51\n----------------------------------------------------------------"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "href": "publications_and_projects/data/Autoencoder.html#single-channel-input-single-channel-outputp10-unet",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Single channel input single channel output(P10) UNET",
    "text": "Single channel input single channel output(P10) UNET\n\ntest_loss_list_P_10_conv_auto_unet = [] \ny_channel = 1 # selecting P10 as output\n# x_channel = 0\nfor x_channel in range(X.shape[1]):\n    ####################### Selecting the channel #######################\n    print('X Channel name : ', target_var_96_list[x_channel])\n    X_train = X_train_all[:, x_channel:x_channel+1, :,:]\n    X_test = X_test_all[:, x_channel:x_channel+1, :,:]\n    y_train = y_train_all[:, y_channel:y_channel+1, :,:]\n    y_test = y_test_all[:, y_channel:y_channel+1, :,:]\n    print('Shapes: ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\n    ####################### Creating the dataset loader #######################\n    train_custom_dataset = CustomDataset(X_train, y_train)\n    # print(len(train_custom_dataset))\n    batch_size = 32\n    train_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n    # print(len(train_loader))\n    \n    test_custom_dataset = CustomDataset(X_test, y_test)\n    # print(len(test_custom_dataset))\n    batch_size = 32\n    test_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n    # print(len(test_loader))\n    \n\n    #################### Training the model ####################\n    model = Autoencoder_UNET(image_size = 80, num_input_channels = 1, num_output_channels=1, c_hid = 8, latent_dim = 512, activation= nn.GELU)\n\n    model.to(device)\n    # Define the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    losses = []\n    # Training loop\n    num_epochs = 200\n    for epoch in trange(num_epochs):\n        model.train()  # Set the model to training mode\n        total_loss = 0.0\n        \n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Zero the gradients\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            \n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            \n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n\n        # Print the average loss for this epoch\n        average_loss = total_loss / len(train_loader)\n        losses.append(average_loss)\n        if epoch % 20 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n    \n    ############################# testing the model #############################\n    model.eval()  # Set the model to evaluation mode\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            # Forward pass\n            outputs = model(inputs)\n            # Calculate the loss\n            loss = criterion(outputs, targets)\n            test_loss += loss.item()\n\n    # Print the average test loss\n    average_test_loss = test_loss / len(test_loader)\n    test_loss_list_P_10_conv_auto_unet.append(average_test_loss)\n    print(f\"Average Test Loss: {average_test_loss:.4f}\")\n    plt.plot(range(1, num_epochs + 1), losses)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\n    plt.grid(True) \n    plt.show() \n\nX Channel name :  TSURF_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 9347.3935\nEpoch [21/200] Loss: 445.2209\nEpoch [41/200] Loss: 421.8352\nEpoch [61/200] Loss: 411.4913\nEpoch [81/200] Loss: 406.5355\nEpoch [101/200] Loss: 410.0266\nEpoch [121/200] Loss: 403.3963\nEpoch [141/200] Loss: 402.2838\nEpoch [161/200] Loss: 404.5083\nEpoch [181/200] Loss: 408.9846\nAverage Test Loss: 393.9739\nX Channel name :  SNOWEW_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1062.0485\nEpoch [21/200] Loss: 425.7831\nEpoch [41/200] Loss: 414.1087\nEpoch [61/200] Loss: 407.9692\nEpoch [81/200] Loss: 409.0262\nEpoch [101/200] Loss: 413.5665\nEpoch [121/200] Loss: 406.8342\nEpoch [141/200] Loss: 401.0203\nEpoch [161/200] Loss: 403.7887\nEpoch [181/200] Loss: 412.0851\nAverage Test Loss: 384.9228\nX Channel name :  SNOWAGE_HR\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1334.1772\nEpoch [21/200] Loss: 432.3582\nEpoch [41/200] Loss: 416.2745\nEpoch [61/200] Loss: 412.7553\nEpoch [81/200] Loss: 411.6587\nEpoch [101/200] Loss: 409.5164\nEpoch [121/200] Loss: 409.2917\nEpoch [141/200] Loss: 414.7216\nEpoch [161/200] Loss: 405.0483\nEpoch [181/200] Loss: 404.8374\nAverage Test Loss: 366.9726\nX Channel name :  PRATE_MMpH\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1228.4596\nEpoch [21/200] Loss: 418.4353\nEpoch [41/200] Loss: 411.4351\nEpoch [61/200] Loss: 411.3802\nEpoch [81/200] Loss: 408.2070\nEpoch [101/200] Loss: 402.7285\nEpoch [121/200] Loss: 405.0596\nEpoch [141/200] Loss: 401.6986\nEpoch [161/200] Loss: 401.9078\nEpoch [181/200] Loss: 405.1741\nAverage Test Loss: 397.5330\nX Channel name :  CLOUD_OD\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1086.5345\nEpoch [21/200] Loss: 115.0784\nEpoch [41/200] Loss: 55.1418\nEpoch [61/200] Loss: 35.0410\nEpoch [81/200] Loss: 25.9328\nEpoch [101/200] Loss: 22.8977\nEpoch [121/200] Loss: 22.6624\nEpoch [141/200] Loss: 18.5878\nEpoch [161/200] Loss: 16.7638\nEpoch [181/200] Loss: 18.7935\nAverage Test Loss: 16.7694\nX Channel name :  U10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1370.2549\nEpoch [21/200] Loss: 253.0794\nEpoch [41/200] Loss: 73.3127\nEpoch [61/200] Loss: 34.2972\nEpoch [81/200] Loss: 23.2712\nEpoch [101/200] Loss: 18.8157\nEpoch [121/200] Loss: 17.0560\nEpoch [141/200] Loss: 13.7983\nEpoch [161/200] Loss: 12.2436\nEpoch [181/200] Loss: 11.2890\nAverage Test Loss: 16.9951\nX Channel name :  V10_MpS\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1202.3667\nEpoch [21/200] Loss: 284.9583\nEpoch [41/200] Loss: 49.5132\nEpoch [61/200] Loss: 23.7217\nEpoch [81/200] Loss: 16.5870\nEpoch [101/200] Loss: 13.4451\nEpoch [121/200] Loss: 11.5799\nEpoch [141/200] Loss: 10.4765\nEpoch [161/200] Loss: 13.5249\nEpoch [181/200] Loss: 9.1645\nAverage Test Loss: 8.1653\nX Channel name :  T2_K\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1344.0557\nEpoch [21/200] Loss: 410.2120\nEpoch [41/200] Loss: 409.9936\nEpoch [61/200] Loss: 403.0425\nEpoch [81/200] Loss: 413.2495\nEpoch [101/200] Loss: 410.5454\nEpoch [121/200] Loss: 408.9813\nEpoch [141/200] Loss: 404.3268\nEpoch [161/200] Loss: 402.1354\nEpoch [181/200] Loss: 403.1094\nAverage Test Loss: 370.9240\nX Channel name :  SWSFC_WpM2\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1138.2640\nEpoch [21/200] Loss: 428.3663\nEpoch [41/200] Loss: 428.3100\nEpoch [61/200] Loss: 411.9817\nEpoch [81/200] Loss: 406.6649\nEpoch [101/200] Loss: 406.5036\nEpoch [121/200] Loss: 404.4717\nEpoch [141/200] Loss: 413.3260\nEpoch [161/200] Loss: 404.8024\nEpoch [181/200] Loss: 408.2683\nAverage Test Loss: 375.5593\nX Channel name :  SOLM_M3pM3\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1171.9761\nEpoch [21/200] Loss: 311.1736\nEpoch [41/200] Loss: 213.8882\nEpoch [61/200] Loss: 86.1118\nEpoch [81/200] Loss: 43.9884\nEpoch [101/200] Loss: 33.4366\nEpoch [121/200] Loss: 26.7442\nEpoch [141/200] Loss: 25.1023\nEpoch [161/200] Loss: 19.1235\nEpoch [181/200] Loss: 16.5218\nAverage Test Loss: 17.8878\nX Channel name :  CLDTOP_KM\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1209.7468\nEpoch [21/200] Loss: 418.6483\nEpoch [41/200] Loss: 411.9460\nEpoch [61/200] Loss: 416.4541\nEpoch [81/200] Loss: 410.3450\nEpoch [101/200] Loss: 404.6752\nEpoch [121/200] Loss: 411.7702\nEpoch [141/200] Loss: 404.7753\nEpoch [161/200] Loss: 410.6092\nEpoch [181/200] Loss: 406.9762\nAverage Test Loss: 364.9331\nX Channel name :  CAPE\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1286.6324\nEpoch [21/200] Loss: 416.3279\nEpoch [41/200] Loss: 412.8959\nEpoch [61/200] Loss: 432.9754\nEpoch [81/200] Loss: 423.2503\nEpoch [101/200] Loss: 407.6195\nEpoch [121/200] Loss: 405.0848\nEpoch [141/200] Loss: 418.5362\nEpoch [161/200] Loss: 398.3066\nEpoch [181/200] Loss: 399.7189\nAverage Test Loss: 366.6939\nX Channel name :  PBL_WRF_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1160.2099\nEpoch [21/200] Loss: 410.9463\nEpoch [41/200] Loss: 407.0937\nEpoch [61/200] Loss: 406.6331\nEpoch [81/200] Loss: 405.4273\nEpoch [101/200] Loss: 406.6442\nEpoch [121/200] Loss: 405.2662\nEpoch [141/200] Loss: 400.4130\nEpoch [161/200] Loss: 403.3931\nEpoch [181/200] Loss: 409.0273\nAverage Test Loss: 364.0550\nX Channel name :  PBL_YSU_M\nShapes:  (1324, 1, 80, 80) (332, 1, 80, 80) (1324, 1, 80, 80) (332, 1, 80, 80)\nEpoch [1/200] Loss: 1176.6661\nEpoch [21/200] Loss: 410.8530\nEpoch [41/200] Loss: 408.6113\nEpoch [61/200] Loss: 400.7242\nEpoch [81/200] Loss: 407.2048\nEpoch [101/200] Loss: 407.0074\nEpoch [121/200] Loss: 405.1491\nEpoch [141/200] Loss: 428.9850\nEpoch [161/200] Loss: 411.7955\nEpoch [181/200] Loss: 412.6084\nAverage Test Loss: 375.2351\n\n\n  0%|          | 1/200 [00:00&lt;01:04,  3.10it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.21it/s] 20%|██        | 41/200 [00:12&lt;00:49,  3.24it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.20it/s] 50%|█████     | 101/200 [00:31&lt;00:31,  3.19it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.21it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.24it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.14it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.23it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.23it/s] 10%|█         | 21/200 [00:06&lt;00:57,  3.13it/s] 20%|██        | 41/200 [00:12&lt;00:50,  3.16it/s] 30%|███       | 61/200 [00:19&lt;00:43,  3.21it/s] 40%|████      | 81/200 [00:25&lt;00:37,  3.13it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.32it/s] 60%|██████    | 121/200 [00:37&lt;00:24,  3.19it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.20it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.16it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.18it/s]\n  0%|          | 1/200 [00:00&lt;01:02,  3.19it/s] 10%|█         | 21/200 [00:06&lt;00:55,  3.24it/s] 20%|██        | 41/200 [00:12&lt;00:51,  3.09it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.29it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.28it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.11it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.17it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:56&lt;00:05,  3.18it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:07,  2.95it/s] 10%|█         | 21/200 [00:06&lt;00:56,  3.19it/s] 20%|██        | 41/200 [00:13&lt;00:49,  3.20it/s] 30%|███       | 61/200 [00:19&lt;00:42,  3.30it/s] 40%|████      | 81/200 [00:25&lt;00:36,  3.27it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.29it/s] 60%|██████    | 121/200 [00:38&lt;00:24,  3.26it/s] 70%|███████   | 141/200 [00:44&lt;00:18,  3.19it/s] 80%|████████  | 161/200 [00:50&lt;00:12,  3.22it/s] 90%|█████████ | 181/200 [00:56&lt;00:06,  3.16it/s]100%|██████████| 200/200 [01:02&lt;00:00,  3.19it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:41,  3.35it/s] 40%|████      | 81/200 [00:24&lt;00:35,  3.32it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.27it/s] 60%|██████    | 121/200 [00:37&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:43&lt;00:18,  3.15it/s] 80%|████████  | 161/200 [00:49&lt;00:11,  3.25it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.31it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.27it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:53,  3.35it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.29it/s] 30%|███       | 61/200 [00:18&lt;00:44,  3.12it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.16it/s] 50%|█████     | 101/200 [00:31&lt;00:30,  3.21it/s] 60%|██████    | 121/200 [00:37&lt;00:25,  3.14it/s] 70%|███████   | 141/200 [00:43&lt;00:17,  3.46it/s] 80%|████████  | 161/200 [00:49&lt;00:12,  3.23it/s] 90%|█████████ | 181/200 [00:55&lt;00:05,  3.21it/s]100%|██████████| 200/200 [01:01&lt;00:00,  3.26it/s]\n  0%|          | 1/200 [00:00&lt;01:00,  3.28it/s] 10%|█         | 21/200 [00:06&lt;00:54,  3.29it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.26it/s] 30%|███       | 61/200 [00:18&lt;00:42,  3.29it/s] 40%|████      | 81/200 [00:24&lt;00:37,  3.18it/s] 50%|█████     | 101/200 [00:31&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:39&lt;00:33,  2.34it/s] 70%|███████   | 141/200 [00:47&lt;00:25,  2.32it/s] 80%|████████  | 161/200 [00:55&lt;00:15,  2.44it/s] 90%|█████████ | 181/200 [01:03&lt;00:06,  3.06it/s]100%|██████████| 200/200 [01:10&lt;00:00,  2.85it/s]\n  0%|          | 1/200 [00:00&lt;01:01,  3.25it/s] 10%|█         | 21/200 [00:08&lt;01:31,  1.95it/s] 20%|██        | 41/200 [00:15&lt;00:48,  3.25it/s] 30%|███       | 61/200 [00:21&lt;00:42,  3.27it/s] 40%|████      | 81/200 [00:27&lt;00:34,  3.47it/s] 50%|█████     | 101/200 [00:33&lt;00:32,  3.06it/s] 60%|██████    | 121/200 [00:41&lt;00:22,  3.51it/s] 70%|███████   | 141/200 [00:46&lt;00:16,  3.63it/s] 80%|████████  | 161/200 [00:52&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:58&lt;00:05,  3.47it/s]100%|██████████| 200/200 [01:03&lt;00:00,  3.14it/s]\n  0%|          | 1/200 [00:00&lt;01:13,  2.70it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.49it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.49it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.49it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.50it/s] 50%|█████     | 101/200 [00:29&lt;00:29,  3.36it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.47it/s] 70%|███████   | 141/200 [00:41&lt;00:16,  3.52it/s] 80%|████████  | 161/200 [00:46&lt;00:10,  3.61it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.47it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.43it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:34&lt;00:50,  1.95it/s] 60%|██████    | 121/200 [00:48&lt;00:21,  3.66it/s] 70%|███████   | 141/200 [00:54&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:59&lt;00:12,  3.17it/s] 90%|█████████ | 181/200 [01:05&lt;00:05,  3.38it/s]100%|██████████| 200/200 [01:12&lt;00:00,  2.74it/s]\n  0%|          | 1/200 [00:00&lt;00:58,  3.40it/s] 10%|█         | 21/200 [00:06&lt;00:51,  3.46it/s] 20%|██        | 41/200 [00:12&lt;00:48,  3.31it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.44it/s] 40%|████      | 81/200 [00:23&lt;00:34,  3.48it/s] 50%|█████     | 101/200 [00:29&lt;00:28,  3.49it/s] 60%|██████    | 121/200 [00:35&lt;00:22,  3.50it/s] 70%|███████   | 141/200 [00:40&lt;00:17,  3.45it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.49it/s]100%|██████████| 200/200 [00:58&lt;00:00,  3.43it/s]\n  0%|          | 1/200 [00:00&lt;01:05,  3.05it/s] 10%|█         | 21/200 [00:06&lt;00:50,  3.56it/s] 20%|██        | 41/200 [00:11&lt;00:45,  3.46it/s] 30%|███       | 61/200 [00:17&lt;00:39,  3.53it/s] 40%|████      | 81/200 [00:23&lt;00:35,  3.37it/s] 50%|█████     | 101/200 [00:29&lt;00:26,  3.68it/s] 60%|██████    | 121/200 [00:34&lt;00:22,  3.56it/s] 70%|███████   | 141/200 [00:40&lt;00:16,  3.55it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.53it/s] 90%|█████████ | 181/200 [00:51&lt;00:05,  3.50it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.49it/s]\n  0%|          | 1/200 [00:00&lt;00:57,  3.44it/s] 10%|█         | 21/200 [00:06&lt;00:52,  3.40it/s] 20%|██        | 41/200 [00:11&lt;00:46,  3.43it/s] 30%|███       | 61/200 [00:18&lt;00:40,  3.42it/s] 40%|████      | 81/200 [00:23&lt;00:33,  3.57it/s] 50%|█████     | 101/200 [00:29&lt;00:27,  3.55it/s] 60%|██████    | 121/200 [00:35&lt;00:23,  3.32it/s] 70%|███████   | 141/200 [00:41&lt;00:17,  3.47it/s] 80%|████████  | 161/200 [00:46&lt;00:11,  3.43it/s] 90%|█████████ | 181/200 [00:52&lt;00:05,  3.47it/s]100%|██████████| 200/200 [00:57&lt;00:00,  3.45it/s]\n  0%|          | 1/200 [00:00&lt;00:54,  3.64it/s] 10%|█         | 21/200 [00:06&lt;00:49,  3.63it/s] 20%|██        | 41/200 [00:12&lt;00:47,  3.35it/s] 30%|███       | 61/200 [00:17&lt;00:40,  3.41it/s] 40%|████      | 81/200 [00:23&lt;00:36,  3.23it/s] 50%|█████     | 101/200 [00:30&lt;00:30,  3.24it/s] 60%|██████    | 121/200 [00:36&lt;00:25,  3.12it/s] 70%|███████   | 141/200 [00:42&lt;00:18,  3.27it/s] 80%|████████  | 161/200 [00:48&lt;00:11,  3.32it/s] 90%|█████████ | 181/200 [00:54&lt;00:05,  3.29it/s]100%|██████████| 200/200 [01:00&lt;00:00,  3.29it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_loss_list_P_10_conv_auto_unet\n\n[393.97390192205256,\n 384.9228349165483,\n 366.9725619229403,\n 397.5330338911577,\n 16.76939444108443,\n 16.99508944424716,\n 8.165278738195246,\n 370.9240167791193,\n 375.55933172052556,\n 17.88784339211204,\n 364.93308327414775,\n 366.69390869140625,\n 364.0550065474077,\n 375.23513239080256]\n\n\n\nname_loss_pairs = list(zip(target_var_96_list, test_loss_list_P_10_conv_auto_unet))\n\n# Sort based on loss values\nsorted_name_loss_pairs = sorted(name_loss_pairs, key=lambda x: x[1])\nfor pair in sorted_name_loss_pairs:\n    print(pair)\n# print(sorted_name_loss_pairs)\n# Select the lowest 3 names\nlowest_3_names = [pair[0] for pair in sorted_name_loss_pairs[:3]]\n\nprint(\"Lowest 3 names:\", lowest_3_names)\n\n('V10_MpS', 8.165278738195246)\n('CLOUD_OD', 16.76939444108443)\n('U10_MpS', 16.99508944424716)\n('SOLM_M3pM3', 17.88784339211204)\n('PBL_WRF_M', 364.0550065474077)\n('CLDTOP_KM', 364.93308327414775)\n('CAPE', 366.69390869140625)\n('SNOWAGE_HR', 366.9725619229403)\n('T2_K', 370.9240167791193)\n('PBL_YSU_M', 375.23513239080256)\n('SWSFC_WpM2', 375.55933172052556)\n('SNOWEW_M', 384.9228349165483)\n('TSURF_K', 393.97390192205256)\n('PRATE_MMpH', 397.5330338911577)\nLowest 3 names: ['V10_MpS', 'CLOUD_OD', 'U10_MpS']"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#insights-2",
    "href": "publications_and_projects/data/Autoencoder.html#insights-2",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Insights",
    "text": "Insights\n\n# Adding values at corresponding indices\nresult = [(x + y)/2 for x, y in zip(test_loss_list_P_10_conv_auto_unet, test_loss_list_P_25_conv_auto_unet)]\n\n# Creating a DataFrame\ndata_frame = {'Input channel': target_var_96_list, 'P10': test_loss_list_P_10_conv_auto_unet, 'P25': test_loss_list_P_25_conv_auto_unet, 'P10+P25 avg': result}\ndf = pd.DataFrame(data_frame)\n\n# Sorting the DataFrame based on \"List1 + List2\"\ndf_sorted = df.sort_values(by='P10+P25 avg')\n\n# Displaying the sorted DataFrame\n\ndf_rounded = df_sorted.round(1)\ndf_rounded.to_csv('/home/rishabh.mondal/climax_alternative/Climax_2/results/test_loss_list_P_10_P25_conv_auto_unet.csv', index=False)\n\ndf_rounded\n\n\n\n\n\n\n\n\nInput channel\nP10\nP25\nP10+P25 avg\n\n\n\n\n6\nV10_MpS\n8.2\n3.5\n5.8\n\n\n5\nU10_MpS\n17.0\n3.2\n10.1\n\n\n9\nSOLM_M3pM3\n17.9\n5.3\n11.6\n\n\n4\nCLOUD_OD\n16.8\n8.0\n12.4\n\n\n10\nCLDTOP_KM\n364.9\n191.1\n278.0\n\n\n12\nPBL_WRF_M\n364.1\n192.7\n278.4\n\n\n11\nCAPE\n366.7\n191.7\n279.2\n\n\n7\nT2_K\n370.9\n192.5\n281.7\n\n\n2\nSNOWAGE_HR\n367.0\n198.3\n282.6\n\n\n8\nSWSFC_WpM2\n375.6\n191.1\n283.3\n\n\n13\nPBL_YSU_M\n375.2\n194.3\n284.8\n\n\n1\nSNOWEW_M\n384.9\n191.9\n288.4\n\n\n0\nTSURF_K\n394.0\n192.9\n293.4\n\n\n3\nPRATE_MMpH\n397.5\n199.4\n298.4"
  },
  {
    "objectID": "publications_and_projects/data/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "href": "publications_and_projects/data/Autoencoder.html#training-on-top-4-channel-and-predicting-on-all-channel-unet",
    "title": "Image-to-Image for Climate Modelling using Auto-Encoders",
    "section": "Training on top 4 channel and predicting on all channel UNET",
    "text": "Training on top 4 channel and predicting on all channel UNET\n\n# target_var_96_list =['TSURF_K',\n#        'SNOWEW_M', 'SNOWAGE_HR', 'PRATE_MMpH', 'CLOUD_OD', 'U10_MpS',\n#        'V10_MpS', 'T2_K', 'SWSFC_WpM2', 'SOLM_M3pM3', 'CLDTOP_KM', 'CAPE',\n#        'PBL_WRF_M', 'PBL_YSU_M']  # ['U10_MpS', 'T2_K', 'V10_MpS']\ntarget_var_96_list= ['V10_MpS','U10_MpS','SOLM_M3pM3', 'CLOUD_OD']\ntarget_var_120_list = ['P25','P10'] \nX,y  = get_data(target_var_96_list, target_var_120_list)\n\nfrom sklearn.model_selection import train_test_split\nX_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape\n\nX shape  (1656, 4, 80, 80)\ny shape (1656, 2, 80, 80)\n\n\n((1324, 4, 80, 80), (332, 4, 80, 80), (1324, 2, 80, 80), (332, 2, 80, 80))\n\n\n\nprint(X_train_all.shape, X_test_all.shape, y_train_all.shape, y_test_all.shape)\ntrain_custom_dataset = CustomDataset(X_train_all, y_train_all)\n# print(len(train_custom_dataset))\nbatch_size = 32\ntrain_loader = data.DataLoader(train_custom_dataset, batch_size=batch_size, shuffle=True)\n# print(len(train_loader))\n\ntest_custom_dataset = CustomDataset(X_test_all, y_test_all)\n# print(len(test_custom_dataset))\nbatch_size = 32\ntest_loader = data.DataLoader(test_custom_dataset, batch_size=batch_size, shuffle=False)\n# print(len(test_loader))\n\n\n#################### Training the model ####################\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nfrom torchinfo import summary\nprint(summary(model, input_size=(1656, 4, 80, 80)))\n# Define the loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nlosses = [] \n# Training loop\nnum_epochs = 100\nfor epoch in trange(num_epochs):\n    model.train()  # Set the model to training mode\n    total_loss = 0.0\n    \n    for inputs, targets in train_loader:\n        optimizer.zero_grad()  # Zero the gradients\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        # Forward pass\n        outputs = model(inputs)\n        \n        # Calculate the loss\n        loss = criterion(outputs, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n\n    # Print the average loss for this epoch\n    average_loss = total_loss / len(train_loader)\n    losses.append(average_loss)\n    if epoch % 20 == 0: \n        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {average_loss:.4f}\")\n\n############################# testing the model #############################\nmodel.eval()  # Set the model to evaluation mode\ntest_loss = 0.0\n\nwith torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n\n        # Calculate the loss\n        loss = criterion(outputs, targets)\n\n        test_loss += loss.item()\n\n# Print the average test loss\naverage_test_loss = test_loss / len(test_loader)\n# test_loss_list_P_25.append(average_test_loss)\nprint(f\"Average Test Loss: {average_test_loss:.4f}\")\n\nplt.plot(range(1, num_epochs + 1), losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n# plt.title('Train Loss vs. Epoch for channel '+target_var_96_list[x_channel])\nplt.grid(True) \nplt.show() \n\n(1324, 4, 80, 80) (332, 4, 80, 80) (1324, 2, 80, 80) (332, 2, 80, 80)\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAutoencoder_UNET                         [1656, 2, 80, 80]         --\n├─Encoder: 1-1                           --                        --\n│    └─Sequential: 2-1                   --                        --\n│    │    └─Conv2d: 3-1                  [1656, 64, 40, 40]        2,368\n│    │    └─GELU: 3-2                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-3                  [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-4                    [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-5                  [1656, 128, 20, 20]       73,856\n│    │    └─GELU: 3-6                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-7                  [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-8                    [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-9                  [1656, 256, 10, 10]       295,168\n│    │    └─GELU: 3-10                   [1656, 256, 10, 10]       --\n│    │    └─Flatten: 3-11                [1656, 25600]             --\n│    │    └─Linear: 3-12                 [1656, 2048]              52,430,848\n├─Decoder: 1-2                           --                        --\n│    └─Sequential: 2-2                   --                        --\n│    │    └─Linear: 3-13                 [1656, 25600]             52,454,400\n│    │    └─Unflatten: 3-14              [1656, 256, 10, 10]       --\n│    │    └─ConvTranspose2d: 3-15        [1656, 128, 20, 20]       295,040\n│    │    └─GELU: 3-16                   [1656, 128, 20, 20]       --\n│    │    └─Conv2d: 3-17                 [1656, 128, 20, 20]       147,584\n│    │    └─GELU: 3-18                   [1656, 128, 20, 20]       --\n│    │    └─ConvTranspose2d: 3-19        [1656, 64, 40, 40]        73,792\n│    │    └─GELU: 3-20                   [1656, 64, 40, 40]        --\n│    │    └─Conv2d: 3-21                 [1656, 64, 40, 40]        36,928\n│    │    └─GELU: 3-22                   [1656, 64, 40, 40]        --\n│    │    └─ConvTranspose2d: 3-23        [1656, 2, 80, 80]         1,154\n==========================================================================================\nTotal params: 105,995,650\nTrainable params: 105,995,650\nNon-trainable params: 0\nTotal mult-adds (Units.TERABYTES): 1.07\n==========================================================================================\nInput size (MB): 169.57\nForward/backward pass size (MB): 9014.58\nParams size (MB): 423.98\nEstimated Total Size (MB): 9608.13\n==========================================================================================\nEpoch [1/100] Loss: 519.4141\nEpoch [21/100] Loss: 10.5276\nEpoch [41/100] Loss: 2.6655\nEpoch [61/100] Loss: 5.3309\nEpoch [81/100] Loss: 2.3606\nAverage Test Loss: 0.9041\n\n\n  1%|          | 1/100 [00:01&lt;03:17,  1.99s/it] 21%|██        | 21/100 [00:36&lt;02:15,  1.72s/it] 41%|████      | 41/100 [01:10&lt;01:41,  1.72s/it] 61%|██████    | 61/100 [01:44&lt;01:06,  1.72s/it] 81%|████████  | 81/100 [02:19&lt;00:32,  1.72s/it]100%|██████████| 100/100 [02:52&lt;00:00,  1.72s/it]\n\n\n\n\n\n\ntorch.save(model.state_dict(), 'model/auto_conv_unet_in4_out2.pt')\nmodel = Autoencoder_UNET(image_size = 80, num_input_channels = 4, num_output_channels=2, c_hid = 64, latent_dim = 2048, activation= nn.GELU)\nmodel.to(device) \nmodel.load_state_dict(torch.load('model/auto_conv_unet_in4_out2.pt'))\n\n&lt;All keys matched successfully&gt;"
  },
  {
    "objectID": "publications_and_projects/data/research.html",
    "href": "publications_and_projects/data/research.html",
    "title": "Towards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning",
    "section": "",
    "text": "Paper accepted and in nomination for best paper award in NeurIPS 2023 Workshop on ReALML (Active learning and Machine learning in real world)\nLive Streamlit Demo App: link\n\n\nKeywords:\nActive Learning, Satellite Imagery, Transfer Learning\n\n\nAbstract:\n\nAir pollution is a leading cause of death globally, especially in south-east Asia. Brick production contributes significantly to air pollution. However, unlike other sources such as power plants, brick production is unregulated and thus hard to monitor. Traditional survey-based methods for kiln identification are time and resource-intensive.\nSimilarly, it is time-consuming for air quality experts to annotate satellite imagery manually. Recently, computer vision machine learning models have helped reduce labeling costs, but they need sufficiently large labeled imagery. In this paper, we propose scalable methods using active learning to accurately detect brick kilns with minimal manual labeling effort. Through this work, we have identified more than 700 new brick kilns across the Indo-Gangetic region: a highly populous and polluted region spanning 0.4 million square kilometers in India.\nIn addition, we have deployed our model as a web application for automatically identifying brick kilns given a specific area by the user.\n\nTo know more check: Paper"
  },
  {
    "objectID": "blogs/blogsData/process_and_screen.html",
    "href": "blogs/blogsData/process_and_screen.html",
    "title": "Process and Screen",
    "section": "",
    "text": "To know the owner of the process\nKill a process\nscreen: &lt;sid.sname&gt;"
  },
  {
    "objectID": "blogs/blogsData/process_and_screen.html#step-by-step-example",
    "href": "blogs/blogsData/process_and_screen.html#step-by-step-example",
    "title": "Process and Screen",
    "section": "Step  by step example:",
    "text": "Step  by step example:\n\nscreen -S sample\nrun some process with stdout output\nctrl + A, Esc (scroll using up and down key)\nctrl + A + D (Detach)\nscreen -ls\n\n(base) dosisiddhesh@lingolexico:/home/dosisiddhesh$ screen -ls\nThere is a screen on:\n220829.sample   (11/17/2023 11:55:27 AM)        (Detached)\n\nscreen -r sample (resume)\nctrl + A + D (Detach)\nscreen -XS sample quit (Screen terminated)\nscreen -ls\n\nNo Sockets found in /run/screen/S-dosisiddhesh.\nPlease visit : Siddhesh Dosi for more intersting content"
  },
  {
    "objectID": "blogs/blogsData/Logistic_regression_pyro.html",
    "href": "blogs/blogsData/Logistic_regression_pyro.html",
    "title": "Logistic Regression using the Pyro",
    "section": "",
    "text": "Here we implement Logistic Regression using the Pyro library.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nsns.set_context(\"notebook\")\ntry:\n    import pyro\nexcept ImportError:\n    %pip install pyro-ppl\n    import pyro\nimport pyro.distributions as dist\nfrom pyro.infer import MCMC, NUTS, Predictive\n\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\nX_train.shape, y_train.shape,X_test.shape, y_test.shape\n\n(torch.Size([80, 2]), torch.Size([80]), torch.Size([20, 2]), torch.Size([20]))\n\n\n\n# Separate data points by class\nclass_0 = X[y == 0]\nclass_1 = X[y == 1]\n\n# Create a scatter plot\nplt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", marker='o')\nplt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", marker='o')\n\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Generated Moons Dataset\")\nplt.legend()\nplt.show()\n\n\n\n\n\ndef logistic_model(X, y):\n    # sample from prior\n    w = pyro.sample(\n      'w', dist.Normal(torch.zeros(X.shape[1]), torch.ones(X.shape[1]))\n    )\n    b = pyro.sample(\n      'b', dist.Normal(torch.zeros(1), torch.ones(1))\n    )\n    with pyro.iarange('data', X.shape[0]):\n        model_logits = torch.matmul(X, w) + b\n        pyro.sample('obs', dist.Bernoulli(logits=model_logits), obs=y)\n\nWe use NUTS MCMC sampling to sample the posterior and take 1000 samples for posterior distribution and use 500 samples as burn/warm up.\n\nnuts_kernel = NUTS(logistic_model, adapt_step_size=True)\nmcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=500)\nmcmc.run(X_train, y_train)\n\nWarmup:   0%|          | 3/1500 [00:00, 25.77it/s, step size=1.20e-01, acc. prob=0.333]Sample: 100%|██████████| 1500/1500 [00:32, 46.05it/s, step size=6.50e-01, acc. prob=0.929] \n\n\n\nposterior_samples = mcmc.get_samples()\n\nFollowing are the trace plots for the parameters of the posterior distribution.\n\nimport arviz as az\n\nidata = az.from_pyro(mcmc)\naz.plot_trace(idata, compact=True);\n\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\arviz\\data\\io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False\n  warnings.warn(\n\n\n\n\n\n\nposterior_samples['w'].mean(0), posterior_samples['b'].mean(0)\n\n(tensor([ 1.1069, -2.0874]), tensor([0.1179]))\n\n\n\nposterior_samples['w'].std(0), posterior_samples['b'].std(0)\n\n(tensor([0.3259, 0.5515]), tensor([0.3369]))\n\n\nPloting the decision boundry for the test data\n\n# Define a function to plot the decision boundary\ndef plot_decision_boundary(X, y, posterior_samples, title=\"Posterior Decision Boundary\"):\n    # Create a meshgrid of points for the entire feature space\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100))\n\n    # Flatten the meshgrid for prediction\n    grid = torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1)\n\n    # Get the number of posterior samples\n    num_samples = len(posterior_samples['w'])\n\n    # Plot the posterior decision boundary for each sample\n    for i in range(num_samples):\n        w = posterior_samples['w'][i]\n        b = posterior_samples['b'][i]\n\n        # Calculate the logits and probabilities\n        logits = torch.matmul(grid, w) + b\n        probs = 1 / (1 + torch.exp(-logits))\n        probs = probs.detach().numpy().reshape(xx.shape)\n\n        # Plot the decision boundary\n        # plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], alpha=0.2, cmap=plt.cm.RdBu)\n        plt.contourf(xx, yy, probs, 10, cmap=plt.cm.RdBu)\n    # Plot the data points\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Class 0\", marker='o', color = 'r')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Class 1\", marker='o', color = 'b')\n\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n# Plot the decision boundary on the test data\nplot_decision_boundary(X_test, y_test, posterior_samples)"
  },
  {
    "objectID": "demo_notebooks/PML_assign4_Suraj_Aditi.html",
    "href": "demo_notebooks/PML_assign4_Suraj_Aditi.html",
    "title": "Assignment 4 PML",
    "section": "",
    "text": "Implement Logistic Regression using the Pyro library referring [1] for guidance. Show both the mean prediction as well as standard deviation in the predictions over the 2d grid. Use NUTS MCMC sampling to sample the posterior. Take 1000 samples for posterior distribution and use 500 samples as burn/warm up. Use the below given dataset.\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport seaborn as sns\nfrom jax import random\nsns.set_context(\"notebook\")\n\n\ntry:\n    import numpyro\nexcept ImportError:\n    %pip install numpyro\n    import numpyro\n\n\ntry:\n    import pyro\nexcept ImportError:\n    %pip install pyro-ppl\n    import pyro\n\n\nimport pyro.distributions as dist\n\n\nfrom pyro.infer import MCMC, NUTS, Predictive\n\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n\n\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\nX_train.shape, y_train.shape,X_test.shape, y_test.shape\n\n(torch.Size([80, 2]), torch.Size([80]), torch.Size([20, 2]), torch.Size([20]))\n\n\n\n# Separate data points by class\nclass_0 = X[y == 0]\nclass_1 = X[y == 1]\n\n# Create a scatter plot\nplt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", marker='o')\nplt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", marker='o')\n\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Generated Moons Dataset\")\nplt.legend()\nplt.show()\n\n\n\n\n\ndef logistic_model(X, y):\n    # sample from prior\n    w = pyro.sample(\n      'w', dist.Normal(torch.zeros(X.shape[1]), torch.ones(X.shape[1]))\n    )\n    b = pyro.sample(\n      'b', dist.Normal(torch.zeros(1), torch.ones(1))\n    )\n    with pyro.iarange('data', X.shape[0]):\n        model_logits = torch.matmul(X, w) + b\n        pyro.sample('obs', dist.Bernoulli(logits=model_logits), obs=y)\n\n\nnuts_kernel = NUTS(logistic_model, adapt_step_size=True)\nmcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=500)\nmcmc.run(X_train, y_train)\n\nWarmup:   0%|          | 3/1500 [00:00, 25.77it/s, step size=1.20e-01, acc. prob=0.333]Sample: 100%|██████████| 1500/1500 [00:32, 46.05it/s, step size=6.50e-01, acc. prob=0.929] \n\n\n\nposterior_samples = mcmc.get_samples()\n\n\nimport arviz as az\n\nidata = az.from_pyro(mcmc)\naz.plot_trace(idata, compact=True);\n\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\arviz\\data\\io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False\n  warnings.warn(\n\n\n\n\n\n\nposterior_samples['w'].mean(0), posterior_samples['b'].mean(0)\n\n(tensor([ 1.1069, -2.0874]), tensor([0.1179]))\n\n\n\nposterior_samples['w'].std(0), posterior_samples['b'].std(0)\n\n(tensor([0.3259, 0.5515]), tensor([0.3369]))\n\n\n\n# Define a function to plot the decision boundary\ndef plot_decision_boundary(X, y, posterior_samples, title=\"Posterior Decision Boundary\"):\n    # Create a meshgrid of points for the entire feature space\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100))\n\n    # Flatten the meshgrid for prediction\n    grid = torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1)\n\n    # Get the number of posterior samples\n    num_samples = len(posterior_samples['w'])\n\n    # Plot the posterior decision boundary for each sample\n    for i in range(num_samples):\n        w = posterior_samples['w'][i]\n        b = posterior_samples['b'][i]\n\n        # Calculate the logits and probabilities\n        logits = torch.matmul(grid, w) + b\n        probs = 1 / (1 + torch.exp(-logits))\n        probs = probs.detach().numpy().reshape(xx.shape)\n\n        # Plot the decision boundary\n        # plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], alpha=0.2, cmap=plt.cm.RdBu)\n        plt.contourf(xx, yy, probs, 10, cmap=plt.cm.RdBu)\n    # Plot the data points\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Class 0\", marker='o', color = 'r')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Class 1\", marker='o', color = 'b')\n\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n# Plot the decision boundary on the test data\nplot_decision_boundary(X_test, y_test, posterior_samples)"
  },
  {
    "objectID": "demo_notebooks/PML_assign4_Suraj_Aditi.html#question-1",
    "href": "demo_notebooks/PML_assign4_Suraj_Aditi.html#question-1",
    "title": "Assignment 4 PML",
    "section": "",
    "text": "Implement Logistic Regression using the Pyro library referring [1] for guidance. Show both the mean prediction as well as standard deviation in the predictions over the 2d grid. Use NUTS MCMC sampling to sample the posterior. Take 1000 samples for posterior distribution and use 500 samples as burn/warm up. Use the below given dataset.\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport seaborn as sns\nfrom jax import random\nsns.set_context(\"notebook\")\n\n\ntry:\n    import numpyro\nexcept ImportError:\n    %pip install numpyro\n    import numpyro\n\n\ntry:\n    import pyro\nexcept ImportError:\n    %pip install pyro-ppl\n    import pyro\n\n\nimport pyro.distributions as dist\n\n\nfrom pyro.infer import MCMC, NUTS, Predictive\n\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=100, noise=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n\n\nX_train = torch.tensor(X_train).float()\ny_train = torch.tensor(y_train).float()\nX_test = torch.tensor(X_test).float()\ny_test = torch.tensor(y_test).float()\nX_train.shape, y_train.shape,X_test.shape, y_test.shape\n\n(torch.Size([80, 2]), torch.Size([80]), torch.Size([20, 2]), torch.Size([20]))\n\n\n\n# Separate data points by class\nclass_0 = X[y == 0]\nclass_1 = X[y == 1]\n\n# Create a scatter plot\nplt.scatter(class_0[:, 0], class_0[:, 1], label=\"Class 0\", marker='o')\nplt.scatter(class_1[:, 0], class_1[:, 1], label=\"Class 1\", marker='o')\n\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Generated Moons Dataset\")\nplt.legend()\nplt.show()\n\n\n\n\n\ndef logistic_model(X, y):\n    # sample from prior\n    w = pyro.sample(\n      'w', dist.Normal(torch.zeros(X.shape[1]), torch.ones(X.shape[1]))\n    )\n    b = pyro.sample(\n      'b', dist.Normal(torch.zeros(1), torch.ones(1))\n    )\n    with pyro.iarange('data', X.shape[0]):\n        model_logits = torch.matmul(X, w) + b\n        pyro.sample('obs', dist.Bernoulli(logits=model_logits), obs=y)\n\n\nnuts_kernel = NUTS(logistic_model, adapt_step_size=True)\nmcmc = MCMC(nuts_kernel, num_samples=1000, warmup_steps=500)\nmcmc.run(X_train, y_train)\n\nWarmup:   0%|          | 3/1500 [00:00, 25.77it/s, step size=1.20e-01, acc. prob=0.333]Sample: 100%|██████████| 1500/1500 [00:32, 46.05it/s, step size=6.50e-01, acc. prob=0.929] \n\n\n\nposterior_samples = mcmc.get_samples()\n\n\nimport arviz as az\n\nidata = az.from_pyro(mcmc)\naz.plot_trace(idata, compact=True);\n\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\arviz\\data\\io_pyro.py:157: UserWarning: Could not get vectorized trace, log_likelihood group will be omitted. Check your model vectorization or set log_likelihood=False\n  warnings.warn(\n\n\n\n\n\n\nposterior_samples['w'].mean(0), posterior_samples['b'].mean(0)\n\n(tensor([ 1.1069, -2.0874]), tensor([0.1179]))\n\n\n\nposterior_samples['w'].std(0), posterior_samples['b'].std(0)\n\n(tensor([0.3259, 0.5515]), tensor([0.3369]))\n\n\n\n# Define a function to plot the decision boundary\ndef plot_decision_boundary(X, y, posterior_samples, title=\"Posterior Decision Boundary\"):\n    # Create a meshgrid of points for the entire feature space\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = torch.meshgrid(torch.linspace(x_min, x_max, 100), torch.linspace(y_min, y_max, 100))\n\n    # Flatten the meshgrid for prediction\n    grid = torch.cat((xx.reshape(-1, 1), yy.reshape(-1, 1)), dim=1)\n\n    # Get the number of posterior samples\n    num_samples = len(posterior_samples['w'])\n\n    # Plot the posterior decision boundary for each sample\n    for i in range(num_samples):\n        w = posterior_samples['w'][i]\n        b = posterior_samples['b'][i]\n\n        # Calculate the logits and probabilities\n        logits = torch.matmul(grid, w) + b\n        probs = 1 / (1 + torch.exp(-logits))\n        probs = probs.detach().numpy().reshape(xx.shape)\n\n        # Plot the decision boundary\n        # plt.contourf(xx, yy, probs, levels=[0, 0.5, 1], alpha=0.2, cmap=plt.cm.RdBu)\n        plt.contourf(xx, yy, probs, 10, cmap=plt.cm.RdBu)\n    # Plot the data points\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Class 0\", marker='o', color = 'r')\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Class 1\", marker='o', color = 'b')\n\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\n# Plot the decision boundary on the test data\nplot_decision_boundary(X_test, y_test, posterior_samples)"
  },
  {
    "objectID": "demo_notebooks/PML_assign4_Suraj_Aditi.html#question-2",
    "href": "demo_notebooks/PML_assign4_Suraj_Aditi.html#question-2",
    "title": "Assignment 4 PML",
    "section": "Question 2:",
    "text": "Question 2:\nConsider the FVC dataset example discussed in the class. Find the notebook link at [2]. We had only used the train dataset. Now, we want to find out the performance of various models on the test dataset. Use the given dataset and deduce which model works best in terms of error (MAE) and coverage? The base model is Linear Regression by Sklearn (from sklearn.linear_model import LinearRegression). Plot the trace diagrams and posterior distribution. Also plot the predictive posterior distribution with 90% confidence interval.\n\nURL = \"https://gist.githubusercontent.com/ucals/\" + \"2cf9d101992cb1b78c2cdd6e3bac6a4b/raw/\"+ \"43034c39052dcf97d4b894d2ec1bc3f90f3623d9/\"+ \"osic_pulmonary_fibrosis.csv\"\n\n\ndf = pd.read_csv(URL)\ndf.head()\n\n\n\n\n\n\n\n\nPatient\nWeeks\nFVC\nPercent\nAge\nSex\nSmokingStatus\n\n\n\n\n0\nID00007637202177411956430\n-4\n2315\n58.253649\n79\nMale\nEx-smoker\n\n\n1\nID00007637202177411956430\n5\n2214\n55.712129\n79\nMale\nEx-smoker\n\n\n2\nID00007637202177411956430\n7\n2061\n51.862104\n79\nMale\nEx-smoker\n\n\n3\nID00007637202177411956430\n9\n2144\n53.950679\n79\nMale\nEx-smoker\n\n\n4\nID00007637202177411956430\n11\n2069\n52.063412\n79\nMale\nEx-smoker\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(df[\"Weeks\"], df['FVC'], train_size = 0.8, random_state = 0)\n\nprint(f\"Training set shape: {x_train.shape}\")\nprint(f\"Testing set shape: {x_test.shape}\")\n\nTraining set shape: (1239,)\nTesting set shape: (310,)\n\n\n\n### Linear regression from scikit-learn\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(x_train.values.reshape(-1,1), y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nall_weeks = np.arange(-12, 134, 1)\n\n\n# Plot the data and the regression line\nplt.scatter(x_test.values.reshape(-1,1), y_test, alpha=0.3)\nplt.plot(x_test, lr.predict(x_test.values.reshape(-1,1)), color=\"black\", lw=2)\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"FVC\")\nplt.title(\"Linear Regression predictions on test data\")\n\nText(0.5, 1.0, 'Linear Regression predictions on test data')\n\n\n\n\n\nPrediction: Vanilla LR\n\npredictions = lr.predict(x_test.values.reshape(-1, 1))\n\nfrom sklearn.metrics import mean_absolute_error\n\nmaes = {}\nmaes[\"LinearRegression\"] = mean_absolute_error(y_test.values, predictions)\nmaes\n\n{'LinearRegression': 626.3184730275215}\n\n\n\nPooled model\n\\(\\alpha \\sim \\text{Normal}(0, 500)\\)\n\\(\\beta \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma \\sim \\text{HalfNormal}(100)\\)\nfor i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha + \\beta \\cdot Week_i, \\sigma)\\)\n\ndef pooled_model(X, y=None):\n    α = numpyro.sample(\"α\", dist.Normal(0., 500.))\n    β = numpyro.sample(\"β\", dist.Normal(0., 500.))\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(50.))\n    with numpyro.plate(\"samples\", len(X)):\n        fvc = numpyro.sample(\"fvc\", dist.Normal(α + β * X, σ), obs=y)\n    return fvc\n\n\nimport numpyro.distributions as dist\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\npatient_encoder = LabelEncoder()\ndf[\"patient_code\"] = patient_encoder.fit_transform(df[\"Patient\"].values)\n\n\nsample_patient_code_train = df[\"patient_code\"].values[x_train.index]\nsample_patient_code_test = df[\"patient_code\"].values[x_test.index]\n\n\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\n\n\nnuts_kernel = NUTS(pooled_model)\n\nmcmc = MCMC(nuts_kernel, num_samples=4000, num_warmup=2000)\nrng_key = random.PRNGKey(0)\n\nNo GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\nx_train.values.reshape(-1,1).shape, y_train.values.shape\n\n((1239, 1), (1239,))\n\n\n\nmcmc.run(rng_key, X=x_train.values, y=y_train.values)\nposterior_samples = mcmc.get_samples()\n\nsample: 100%|██████████| 6000/6000 [00:07&lt;00:00, 780.32it/s, 15 steps of size 4.41e-01. acc. prob=0.93] \n\n\n\nimport arviz as az\n\nidata = az.from_numpyro(mcmc)\naz.plot_trace(idata, compact=True)\n\narray([[&lt;Axes: title={'center': 'α'}&gt;, &lt;Axes: title={'center': 'α'}&gt;],\n       [&lt;Axes: title={'center': 'β'}&gt;, &lt;Axes: title={'center': 'β'}&gt;],\n       [&lt;Axes: title={'center': 'σ'}&gt;, &lt;Axes: title={'center': 'σ'}&gt;]],\n      dtype=object)\n\n\n\n\n\n\n# Summary statistics\naz.summary(idata, round_to=2)\n\narviz - WARNING - Shape validation failed: input_shape: (1, 4000), minimum_shape: (chains=2, draws=4)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nα\n2732.55\n36.72\n2666.72\n2804.27\n0.86\n0.61\n1831.78\n2063.36\nNaN\n\n\nβ\n-1.37\n0.92\n-3.15\n0.25\n0.02\n0.02\n1918.32\n2020.37\nNaN\n\n\nσ\n773.88\n12.85\n749.97\n798.30\n0.25\n0.18\n2569.03\n1991.23\nNaN\n\n\n\n\n\n\n\n\n# Predictive distribution\npredictive = Predictive(pooled_model, mcmc.get_samples())\n\n\npredictions = predictive(rng_key, x_test.values, None)\n\n\npd.DataFrame(predictions[\"fvc\"]).mean().plot()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\nplt.plot(x_test, predictions[\"fvc\"].mean(axis=0))\nplt.scatter(x_test, y_test, alpha=0.1)\n\n&lt;matplotlib.collections.PathCollection at 0x2eaeed4bc10&gt;\n\n\n\n\n\n\n# Get the mean and standard deviation of the predictions\nmu = predictions[\"fvc\"].mean(axis=0)\nsigma = predictions[\"fvc\"].std(axis=0)\n\n# Plot the predictions\nplt.plot(x_test, mu)\nplt.fill_between(x_test, mu - 1.64*sigma, mu + 1.64*sigma, alpha=0.2)\nplt.scatter(x_test, y_test, alpha=0.2)\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"FVC\")\n\nText(0, 0.5, 'FVC')\n\n\n\n\n\n\npreds_pooled  = predictive(rng_key, x_test.values, None)['fvc']\npredictions_train_pooled = preds_pooled.mean(axis=0)\nstd_train_pooled = preds_pooled.std(axis=0)\n\n\n### Computing Mean Absolute Error and Coverage at 95% confidence interval\n\nmaes[\"PooledModel\"] = mean_absolute_error(y_test, predictions_train_pooled)\nmaes\n\n{'LinearRegression': 626.3184730275215, 'PooledModel': 626.4611966040827}\n\n\n\n### Computing the coverage at 95% confidence interval\n\ndef coverage(y_true, y_pred, sigma):\n    lower = y_pred - 1.96 * sigma\n    upper = y_pred + 1.96 * sigma\n    return np.mean((y_true &gt;= lower) & (y_true &lt;= upper))\ncoverages = {}\ncoverages[\"pooled\"] = coverage(y_test, predictions_train_pooled, std_train_pooled).item()\ncoverages\n\n{'pooled': 0.9483870967741935}\n\n\n\n\nHierarchical model\n\\(\\sigma \\sim \\text{HalfNormal}(100)\\)\n\n\\(\\mu_{\\alpha} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\alpha} \\sim \\text{HalfNormal}(100)\\)\n\\(\\mu_{\\beta} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\beta} \\sim \\text{HalfNormal}(100)\\)\n\nfor p in range(N_patients):\n\\(\\alpha_p \\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha})\\)\n\\(\\beta_p \\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta})\\)\n\nfor i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha_{p[i]} + \\beta_{p[i]} \\cdot Week_i, \\sigma)\\)\n\n### Hierarchical model\n\ndef partial_pool_same_sigma(sample_weeks, sample_patient_code, sample_fvc=None):\n    μ_α = numpyro.sample(\"μ_α\", dist.Normal(0.0, 500.0))\n    σ_α = numpyro.sample(\"σ_α\", dist.HalfNormal(100.0))\n    μ_β = numpyro.sample(\"μ_β\", dist.Normal(0.0, 3.0))\n    σ_β = numpyro.sample(\"σ_β\", dist.HalfNormal(3.0))\n\n    n_patients = len(np.unique(sample_patient_code))\n\n    with numpyro.plate(\"Participants\", n_patients):\n        α = numpyro.sample(\"α\", dist.Normal(μ_α, σ_α))\n        β = numpyro.sample(\"β\", dist.Normal(μ_β, σ_β))\n\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(100.0))\n    FVC_est = α[sample_patient_code] + β[sample_patient_code] * sample_weeks\n\n    with numpyro.plate(\"data\", len(sample_patient_code)):\n        numpyro.sample(\"fvc\", dist.Normal(FVC_est, σ), obs=sample_fvc)\n\n\nnuts_ppss = NUTS(partial_pool_same_sigma)\n\nmcmc_ppss = MCMC(nuts_ppss, num_samples=4000, num_warmup=2000)\nrng_key = random.PRNGKey(0)\n\n\nmodel_kwargs = {\"sample_weeks\": x_train.values,\n                \"sample_patient_code\": sample_patient_code_train,\n                \"sample_fvc\":y_train.values}\n\n\nmcmc_ppss.run(rng_key, **model_kwargs)\n\nsample: 100%|██████████| 6000/6000 [01:28&lt;00:00, 67.52it/s, 63 steps of size 1.26e-02. acc. prob=0.85]  \n\n\n\npredictive_ppss = Predictive(partial_pool_same_sigma, mcmc_ppss.get_samples())\n\n\naz.plot_trace(az.from_numpyro(mcmc_ppss), compact=True)\n\narray([[&lt;Axes: title={'center': 'α'}&gt;, &lt;Axes: title={'center': 'α'}&gt;],\n       [&lt;Axes: title={'center': 'β'}&gt;, &lt;Axes: title={'center': 'β'}&gt;],\n       [&lt;Axes: title={'center': 'μ_α'}&gt;, &lt;Axes: title={'center': 'μ_α'}&gt;],\n       [&lt;Axes: title={'center': 'μ_β'}&gt;, &lt;Axes: title={'center': 'μ_β'}&gt;],\n       [&lt;Axes: title={'center': 'σ'}&gt;, &lt;Axes: title={'center': 'σ'}&gt;],\n       [&lt;Axes: title={'center': 'σ_α'}&gt;, &lt;Axes: title={'center': 'σ_α'}&gt;],\n       [&lt;Axes: title={'center': 'σ_β'}&gt;, &lt;Axes: title={'center': 'σ_β'}&gt;]],\n      dtype=object)\n\n\n\n\n\n\npredictive_ppss = Predictive(partial_pool_same_sigma, mcmc_ppss.get_samples())\n\n\npredictions_test_ppss = predictive_ppss(rng_key,\n                                                         sample_weeks = x_test.values,\n                                                         sample_patient_code = sample_patient_code_test)['fvc']\n\nmu_predictions_test_h = predictions_test_ppss.mean(axis=0)\nstd_predictions_test_h = predictions_test_ppss.std(axis=0)\n\nmaes[\"PatrialPooled_samesigma\"] = mean_absolute_error(y_test, mu_predictions_test_h)\n\ncoverages[\"PatrialPooled_samesigma\"] = coverage(y_test, mu_predictions_test_h, std_predictions_test_h).item()\n\nprint(maes)\nprint(coverages)\n\n{'LinearRegression': 626.3184730275215, 'PooledModel': 626.4611966040827, 'PatrialPooled_samesigma': 110.46457322643649}\n{'pooled': 0.9483870967741935, 'PatrialPooled_samesigma': 0.9419354838709677}\n\n\n\n# Predict for a given patient\n\n\ndef predict_ppss(patient_code):\n    predictions = predictive_ppss(rng_key, all_weeks, patient_code)\n    mu = predictions[\"fvc\"].mean(axis=0)\n    sigma = predictions[\"fvc\"].std(axis=0)\n    return mu, sigma\n\n# Plot the predictions for a given patient\ndef plot_patient_ppss(patient_code):\n    mu, sigma = predict_ppss(patient_code)\n    plt.scatter(all_weeks, mu)\n    plt.fill_between(all_weeks, mu - 1.64*sigma, mu + 1.64*sigma, alpha=0.1)\n    id_to_patient = patient_encoder.inverse_transform([patient_code])[0]\n\n    patient_weeks = x_test.values[sample_patient_code_test == patient_code]\n    patient_fvc = y_test.values[sample_patient_code_test == patient_code]\n\n    plt.scatter(patient_weeks, patient_fvc, alpha=0.8, label=\"Test Set\", color=\"black\")\n    plt.xlabel(\"Weeks\")\n    plt.ylabel(\"FVC\")\n    plt.legend()\n    plt.title(patient_encoder.inverse_transform([patient_code])[0])\n\ndef plot_total_ppss(patient_id = 0, plot_pooled = False):\n    print(all_weeks.shape)\n    print(mu.shape)\n    plot_patient_ppss(np.array([patient_id]))\n    print(all_weeks.shape)\n    print(mu.shape)\n    if plot_pooled:\n        plt.plot(all_weeks, mu, color='g')\n        plt.fill_between(all_weeks, mu - 1.64*sigma, mu + 1.64*sigma, alpha=0.05, color='g')\n\n\n# plot for a given patient\nplot_patient_ppss(np.array([0]))\n\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n\n\n\n\npredictions_test_ppss = predictive_ppss(rng_key,\n                                                 x_test.values,\n                                                 sample_patient_code_test)['fvc']\npredictions_test_ppss.shape\n\n(4000, 310)\n\n\n\nmu_predictions_test_ppss = predictions_test_ppss.mean(axis=0)\nstd_predictions_test_ppss = predictions_test_ppss.std(axis=0)\n\nmaes[\"PartialPooled_samesigma\"] = mean_absolute_error(y_test, mu_predictions_test_ppss)\nmaes\n\n{'LinearRegression': 626.3184730275215,\n 'PooledModel': 626.4611966040827,\n 'PatrialPooled_samesigma': 110.46457322643649,\n 'PartiallyPooled_samesigma': 110.46457322643649,\n 'PatrialPooled_hypersigma': 111.37391593686996,\n 'PartialPooled_samesigma': 110.46457322643649}\n\n\n\n\nHierarchical model\n\\(\\gamma_{\\sigma} \\sim \\text{HalfNormal}(30)\\)\n\\(\\mu_{\\alpha} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\alpha} \\sim \\text{HalfNormal}(100)\\)\n\\(\\mu_{\\beta} \\sim \\text{Normal}(0, 500)\\)\n\\(\\sigma_{\\beta} \\sim \\text{HalfNormal}(100)\\)\n\nfor p in range(N_patients):\n\\(\\alpha_p \\sim \\text{Normal}(\\mu_{\\alpha}, \\sigma_{\\alpha})\\)\n\\(\\beta_p \\sim \\text{Normal}(\\mu_{\\beta}, \\sigma_{\\beta})\\)\n\\(\\sigma_p \\sim \\text{Exp}(\\gamma_{\\sigma})\\)\n\nfor i in range(N_Weeks):\n\\(FVC_i \\sim \\text{Normal}(\\alpha_{p[i]} + \\beta_{p[i]} \\cdot Week_i, \\sigma_{p[i]})\\)\n\n### Hierarchical model\n\ndef partial_pool_hyper_sigma(sample_weeks, sample_patient_code, sample_fvc=None):\n    μ_α = numpyro.sample(\"μ_α\", dist.Normal(0.0, 500.0))\n    σ_α = numpyro.sample(\"σ_α\", dist.HalfNormal(100.0))\n    μ_β = numpyro.sample(\"μ_β\", dist.Normal(0.0, 3.0))\n    σ_β = numpyro.sample(\"σ_β\", dist.HalfNormal(3.0))\n    gamma_sigma = numpyro.sample(\"gamma_sigma\", dist.HalfNormal(30))\n\n\n    n_patients = len(np.unique(sample_patient_code))\n\n    with numpyro.plate(\"Participants\", n_patients):\n        α = numpyro.sample(\"α\", dist.Normal(μ_α, σ_α))\n        β = numpyro.sample(\"β\", dist.Normal(μ_β, σ_β))\n        σ = numpyro.sample(\"σ\", dist.Exponential(gamma_sigma))\n    FVC_est = α[sample_patient_code] + β[sample_patient_code] * sample_weeks\n\n    with numpyro.plate(\"data\", len(sample_patient_code)):\n        numpyro.sample(\"fvc\", dist.Normal(FVC_est, σ[sample_patient_code]), obs=sample_fvc)\n\n\nnuts_pphs = NUTS(partial_pool_hyper_sigma)\n\nmcmc_pphs = MCMC(nuts_pphs, num_samples=4000, num_warmup=2000)\nrng_key = random.PRNGKey(0)\n\n\ny_train.shape\n\n(1239,)\n\n\n\nmodel_kwargs = {\"sample_weeks\": x_train.values,\n                \"sample_patient_code\": sample_patient_code_train,\n                \"sample_fvc\":y_train.values}\n\n\nmcmc_pphs.run(rng_key, **model_kwargs)\n\nsample: 100%|██████████| 6000/6000 [05:03&lt;00:00, 19.80it/s, 511 steps of size 8.39e-03. acc. prob=0.94] \n\n\n\npredictive_pphs = Predictive(partial_pool_hyper_sigma, mcmc_pphs.get_samples())\n\n\naz.plot_trace(az.from_numpyro(mcmc_pphs), compact=True)\n\narray([[&lt;Axes: title={'center': 'gamma_sigma'}&gt;,\n        &lt;Axes: title={'center': 'gamma_sigma'}&gt;],\n       [&lt;Axes: title={'center': 'α'}&gt;, &lt;Axes: title={'center': 'α'}&gt;],\n       [&lt;Axes: title={'center': 'β'}&gt;, &lt;Axes: title={'center': 'β'}&gt;],\n       [&lt;Axes: title={'center': 'μ_α'}&gt;, &lt;Axes: title={'center': 'μ_α'}&gt;],\n       [&lt;Axes: title={'center': 'μ_β'}&gt;, &lt;Axes: title={'center': 'μ_β'}&gt;],\n       [&lt;Axes: title={'center': 'σ'}&gt;, &lt;Axes: title={'center': 'σ'}&gt;],\n       [&lt;Axes: title={'center': 'σ_α'}&gt;, &lt;Axes: title={'center': 'σ_α'}&gt;],\n       [&lt;Axes: title={'center': 'σ_β'}&gt;, &lt;Axes: title={'center': 'σ_β'}&gt;]],\n      dtype=object)\n\n\n\n\n\n\npredictions_test_pphs = predictive_pphs(rng_key,\n                                                         sample_weeks = x_test.values,\n                                                         sample_patient_code = sample_patient_code_test)['fvc']\n\nmu_predictions_test_h = predictions_test_pphs.mean(axis=0)\nstd_predictions_test_h = predictions_test_pphs.std(axis=0)\n\nmaes[\"PatrialPooled_hypersigma\"] = mean_absolute_error(y_test, mu_predictions_test_h)\n\ncoverages[\"PatrialPooled_hypersigma\"] = coverage(y_test, mu_predictions_test_h, std_predictions_test_h).item()\n\nprint(maes)\nprint(coverages)\n\n{'LinearRegression': 626.3184730275215, 'PooledModel': 626.4611966040827, 'PatrialPooled_samesigma': 110.46457322643649, 'PartiallyPooled_samesigma': 110.46457322643649, 'PatrialPooled_hypersigma': 111.37391593686996}\n{'pooled': 0.9483870967741935, 'PatrialPooled_samesigma': 0.9419354838709677, 'PatrialPooled_hypersigma': 0.9451612903225807}\n\n\n\n# Predict for a given patient\n\n\ndef predict_pphs(patient_code):\n    predictions = predictive_pphs(rng_key, all_weeks, patient_code)\n    mu = predictions[\"fvc\"].mean(axis=0)\n    sigma = predictions[\"fvc\"].std(axis=0)\n    return mu, sigma\n\n# Plot the predictions for a given patient\ndef plot_patient_pphs(patient_code):\n    mu, sigma = predict_pphs(patient_code)\n    plt.scatter(all_weeks, mu)\n    plt.fill_between(all_weeks, mu - 1.64*sigma, mu + 1.64*sigma, alpha=0.1)\n    id_to_patient = patient_encoder.inverse_transform([patient_code])[0]\n    #print(id_to_patient[0], patient_code)\n    #print(patient_code, id_to_patient)\n    patient_weeks = x_test.values[sample_patient_code_test == patient_code]\n    patient_fvc = y_test.values[sample_patient_code_test == patient_code]\n    # patient_weeks = train[train[\"Patient\"] == id_to_patient][\"Weeks\"]\n    # patient_fvc = train[train[\"Patient\"] == id_to_patient][\"FVC\"]\n    plt.scatter(patient_weeks, patient_fvc, alpha=0.8, label=\"Test Set\", color=\"black\")\n    #plt.scatter(sample_weeks[train[\"patient_code\"] == patient_code.item()], fvc[train[\"patient_code\"] == patient_code.item()], alpha=0.5)\n    plt.xlabel(\"Weeks\")\n    plt.ylabel(\"FVC\")\n    plt.legend()\n    plt.title(patient_encoder.inverse_transform([patient_code])[0])\n\ndef plot_total_pphs(patient_id = 0, plot_pooled = False):\n    print(all_weeks.shape)\n    print(mu.shape)\n    plot_patient_pphs(np.array([patient_id]))\n    print(all_weeks.shape)\n    print(mu.shape)\n    if plot_pooled:\n        plt.plot(all_weeks, mu, color='g')\n        plt.fill_between(all_weeks, mu - 1.64*sigma, mu + 1.64*sigma, alpha=0.05, color='g')\n\n\n# plot for a given patient\nplot_patient_pphs(np.array([0]))\n\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\nc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:155: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)"
  },
  {
    "objectID": "demo_notebooks/PML_assign4_Suraj_Aditi.html#hyper-network",
    "href": "demo_notebooks/PML_assign4_Suraj_Aditi.html#hyper-network",
    "title": "Assignment 4 PML",
    "section": "Hyper Network",
    "text": "Hyper Network\n\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn as nn\n# import torch.nn.functional as F\nimport torch.optim as optim\nimport os\nfrom torch.utils.data import Dataset, DataLoader, Subset\n# import torchvision.datasets as datasets\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom tqdm import trange\n\nimport os\nimport shutil\n\nfrom tabulate import tabulate\n\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice = torch.device(\"cuda:3\")\nprint(device)\ncurrent_device = device #torch.cuda.current_device()\ndevice_name = torch.cuda.get_device_name(current_device)\nprint(f\"Current GPU assigned: {current_device}, Name: {device_name}\")\n\ncuda:3\nCurrent GPU assigned: cuda:3, Name: NVIDIA A100-SXM4-80GB\n\n\nCeleba data link\nNote book ref link\n\nprepare data\n\n# import os\n\n# data_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba'\n\n# # Check if the specified directory exists\n# if os.path.exists(data_root):\n#     # List all files in the directory with a specific image extension (e.g., .jpg)\n#     image_files = [f for f in os.listdir(data_root) if f.endswith('.jpg')]\n\n#     # Get the count of image files\n#     num_images = len(image_files)\n\n#     print(f\"Number of images in '{data_root}': {num_images}\")\n# else:\n#     print(f\"The directory '{data_root}' does not exist.\")\n\nNumber of images in '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba': 202599\n\n\n\n# data_folder_size = 200000\n# data_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba'\n# output_dir = f'/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_{data_folder_size}'\n\n# # Check if the specified input directory exists\n# if os.path.exists(data_root):\n#     # Ensure the output directory exists, or create it if necessary\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n\n#     # List all files in the input directory\n#     all_image_files = [f for f in os.listdir(data_root) if f.endswith('.jpg')]\n\n#     # random_images = random.sample(all_image_files, data_folder_size)\n#     images_list = all_image_files[:data_folder_size]\n#     # Copy the randomly selected images to the output directory\n#     for image_file in images_list:\n#         src_path = os.path.join(data_root, image_file)\n#         dst_path = os.path.join(output_dir, image_file)\n#         shutil.copy2(src_path, dst_path)\n\n#     print(f\"Successfully copied {data_folder_size} random images to '{output_dir}'.\")\n# else:\n#     print(f\"The input directory '{data_root}' does not exist.\")\n\nSuccessfully copied 200000 random images to '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_200000'.\n\n\ntest data: last 2599 images\n\n# data_folder_size = 2599\n# data_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba'\n# output_dir = f'/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_{data_folder_size}'\n\n# # Check if the specified input directory exists\n# if os.path.exists(data_root):\n#     # Ensure the output directory exists, or create it if necessary\n#     if not os.path.exists(output_dir):\n#         os.makedirs(output_dir)\n\n#     # List all files in the input directory\n#     all_image_files = [f for f in os.listdir(data_root) if f.endswith('.jpg')]\n\n#     # random_images = random.sample(all_image_files, data_folder_size)\n#     images_list = all_image_files[data_folder_size:]\n#     # Copy the randomly selected images to the output directory\n#     for image_file in images_list:\n#         src_path = os.path.join(data_root, image_file)\n#         dst_path = os.path.join(output_dir, image_file)\n#         shutil.copy2(src_path, dst_path)\n\n#     print(f\"Successfully copied {data_folder_size} random images to '{output_dir}'.\")\n# else:\n#     print(f\"The input directory '{data_root}' does not exist.\")\n\nSuccessfully copied 2599 random images to '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_2599'.\n\n\n\n\nExtras\n\nclass CustomImageDataset(Dataset):\n    def __init__(self, data_root, transform=None):\n        self.data_root = data_root\n        self.transform = transform\n        self.image_files = [f for f in os.listdir(data_root) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.data_root, self.image_files[idx])\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\n\n# Create a coordinate dataset from the image\ndef create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n\n    return: tuple of torch.Tensor of shape (height* width, 2) and torch.tensor containing the (num_channels)\n    \"\"\"\n\n    num_channels, height, width = img.shape\n\n    # Create a 2D grid of (x,y) coordinates\n    x_coords = torch.arange(width).repeat(height, 1)\n    y_coords = torch.arange(height).repeat(width, 1).t()\n    x_coords = x_coords.reshape(-1)\n    y_coords = y_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([x_coords, y_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Create a tensor containing the image pixel values\n    Y = img.reshape(-1, num_channels).float().to(device)\n    return X, Y\n\n\ndef neg_loglikelyhood(y_pred,log_sigma,y_true):\n    cov_matrix = torch.diag_embed(log_sigma.exp())\n    dist = torch.distributions.MultivariateNormal(y_pred,cov_matrix,validate_args=False)\n    return - dist.log_prob(y_true).sum()\n\n\ndef count_params(model):\n    # return torch.sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return torch.sum(torch.tensor([p.numel() for p in model.parameters()]))\n\n\n\nloading and preprocessing\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_10000'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),   # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ndata_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=False)\n\nOriginal image after transformation\n\nplt.imshow(torch.einsum('chw -&gt; hwc', data_loader.dataset[33]))\n\n&lt;matplotlib.image.AxesImage at 0x7ff83c94fdf0&gt;\n\n\n\n\n\nOriginal image\n\nplt.imshow(torch.einsum('chw -&gt; hwc', data_loader.dataset[33]))\n\n&lt;matplotlib.image.AxesImage at 0x7f800aac09d0&gt;\n\n\n\n\n\n\n\nmodel defination\n\ntarget net defination\n\n# Create a MLP with 5 hidden layers with 256 neurons each and ReLU activations.\n# Input is (x, y) and output is (r, g, b) or (g) for grayscale\n# here we output 6 values (3 for RGB mean and 3 for RGB std)\ns = 128 # hidden dim of model\n\nclass TargetNet(nn.Module):\n    def _init_siren(self, activation_scale):\n        self.fc1.weight.data.uniform_(-1/self.fc1.in_features, 1/self.fc1.in_features)\n        for layers in [self.fc2, self.fc3, self.fc4, self.fc5]:\n            layers.weight.data.uniform_(-np.sqrt(6/self.fc2.in_features)/activation_scale,\n                                        np.sqrt(6/self.fc2.in_features)/activation_scale)\n\n    def __init__(self, activation=torch.relu, n_out=1, activation_scale=1.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        self.fc1 = nn.Linear(2, s) # input size is 2 (x, y) location of pixel\n        self.fc2 = nn.Linear(s, s)\n        self.fc3 = nn.Linear(s, s)\n        self.fc4 = nn.Linear(s, s)\n        self.fc5 = nn.Linear(s, n_out) #gray scale image (1) or RGB (3)\n        if self.activation == torch.sin:\n            # init weights and biases for sine activation\n            self._init_siren(activation_scale=self.activation_scale)\n\n    def forward(self, x):\n        x = self.activation(self.activation_scale*self.fc1(x))\n        x = self.activation(self.activation_scale*self.fc2(x))\n        x = self.activation(self.activation_scale*self.fc3(x))\n        x = self.activation(self.activation_scale*self.fc4(x))\n        return self.fc5(x)\n\n\n\nHypernetwork defination\n\nInput: (x, y, R, G, B)\n\n\nOutput: Our Hypernetwork should have the output equal to the number of parameters in the main network.\n\n# pass total params of target network before calling the hypernetwork model\nclass HyperNet(nn.Module):\n    def __init__(self, total_params, num_neurons=128, activation=torch.relu):\n        super().__init__()\n        self.activation = activation\n        self.n_out = total_params\n        self.fc1 = nn.Linear(5, num_neurons)\n        self.fc2 = nn.Linear(num_neurons, num_neurons)\n        self.fc3 = nn.Linear(num_neurons, self.n_out)\n\n    def forward(self, x):\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        return self.fc3(x)\n\n\n\n\n\nInitialize the model and input\n\nInitialize the target network\n\nfrom torchinfo import summary\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\nsummary(targetnet, input_size=(img_size* img_size, 2)) #32*32 =1024 is the image size lentgh, 2 is x,y coordinate\n# outputs 6: 1,2,3 mean of each channel and 4,5,6 are log sigma of each channel\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTargetNet                                [1024, 6]                 --\n├─Linear: 1-1                            [1024, 128]               384\n├─Linear: 1-2                            [1024, 128]               16,512\n├─Linear: 1-3                            [1024, 128]               16,512\n├─Linear: 1-4                            [1024, 128]               16,512\n├─Linear: 1-5                            [1024, 6]                 774\n==========================================================================================\nTotal params: 50,694\nTrainable params: 50,694\nNon-trainable params: 0\nTotal mult-adds (M): 51.91\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 4.24\nParams size (MB): 0.20\nEstimated Total Size (MB): 4.45\n==========================================================================================\n\n\n\ntargetnet\n\nTargetNet(\n  (fc1): Linear(in_features=2, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=128, bias=True)\n  (fc4): Linear(in_features=128, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=6, bias=True)\n)\n\n\n\ncount_params(targetnet)\n\ntensor(50694)\n\n\n\n\ninitialize the hypernetwork model\n\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.sin).to(device)\nprint(hypernet)\n\nHyperNet(\n  (fc1): Linear(in_features=5, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=50694, bias=True)\n)\n\n\n\nsummary(hypernet,input_size=(img_size* img_size,5))  # 32*32 = 1024 is the image size length, 5 is the input(x,y,r,g,b) to hypernet\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nHyperNet                                 [1024, 50694]             --\n├─Linear: 1-1                            [1024, 128]               768\n├─Linear: 1-2                            [1024, 128]               16,512\n├─Linear: 1-3                            [1024, 50694]             6,539,526\n==========================================================================================\nTotal params: 6,556,806\nTrainable params: 6,556,806\nNon-trainable params: 0\nTotal mult-adds (G): 6.71\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 417.38\nParams size (MB): 26.23\nEstimated Total Size (MB): 443.63\n==========================================================================================\n\n\n\ntable_data = []\ntotal_params = 0\nstart = 0\nstart_end_mapping = {}\nfor name, param in targetnet.named_parameters():\n    param_count = torch.prod(torch.tensor(param.shape)).item()\n    total_params += param_count\n    end = total_params\n    table_data.append([name, param.shape, param_count, start, end])\n    start_end_mapping[name] = (start, end)\n    start = end\n\nprint(tabulate(table_data, headers=[\"Layer Name\", \"Shape\", \"Parameter Count\", \"Start Index\", \"End Index\"]))\nprint(f\"Total number of parameters: {total_params}\")\n\nLayer Name    Shape                     Parameter Count    Start Index    End Index\n------------  ----------------------  -----------------  -------------  -----------\nfc1.weight    torch.Size([128, 2])                  256              0          256\nfc1.bias      torch.Size([128])                     128            256          384\nfc2.weight    torch.Size([128, 128])              16384            384        16768\nfc2.bias      torch.Size([128])                     128          16768        16896\nfc3.weight    torch.Size([128, 128])              16384          16896        33280\nfc3.bias      torch.Size([128])                     128          33280        33408\nfc4.weight    torch.Size([128, 128])              16384          33408        49792\nfc4.bias      torch.Size([128])                     128          49792        49920\nfc5.weight    torch.Size([6, 128])                  768          49920        50688\nfc5.bias      torch.Size([6])                         6          50688        50694\nTotal number of parameters: 50694\n\n\n\n\ninitialize the input\n\ncorr, vals = create_coordinate_map(data_loader.dataset[0])\ncorr, vals\n\n(tensor([[ 0.,  0.],\n         [ 1.,  0.],\n         [ 2.,  0.],\n         ...,\n         [29., 31.],\n         [30., 31.],\n         [31., 31.]], device='cuda:3'),\n tensor([[0.4510, 0.4706, 0.4824],\n         [0.4745, 0.4745, 0.4471],\n         [0.4667, 0.4353, 0.5412],\n         ...,\n         [0.0314, 0.0549, 0.0471],\n         [0.0431, 0.0392, 0.0510],\n         [0.0549, 0.0392, 0.0549]], device='cuda:3'))\n\n\n\nscaler_img = preprocessing.MinMaxScaler().fit(corr.cpu())\nxy = torch.tensor(scaler_img.transform(corr.cpu())).float().to(device)\nxy, xy.shape\n\n(tensor([[0.0000, 0.0000],\n         [0.0323, 0.0000],\n         [0.0645, 0.0000],\n         ...,\n         [0.9355, 1.0000],\n         [0.9677, 1.0000],\n         [1.0000, 1.0000]], device='cuda:3'),\n torch.Size([1024, 2]))\n\n\n\n\n\nTraining loop\n\nn_epochs=20\nlr = 0.003\n\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.relu).to(device)\noptimizer = optim.Adam(hypernet.parameters(),lr=lr) # only hypernet is updated\n\nn_context = 100\nprint(\"Context Points=\",n_context)\nfor epoch in trange(n_epochs):\n\n    c_idx = np.array(random.sample(range(1023),n_context))\n\n    print(\"Epoch=\",epoch+1)\n    epoch_loss = 0\n    i=1\n\n    for data in data_loader:\n        # print(data.shape)\n        optimizer.zero_grad()\n\n        pixel_intensity = data.reshape(3,-1).T.to(device).float()\n        input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n        hyper_out = hypernet(input)\n        hyper_out = torch.mean(hyper_out,dim=0)\n\n        target_dict ={}\n        for name,param in targetnet.named_parameters():\n            start,end = start_end_mapping[name]\n            target_dict[name] = hyper_out[start:end].reshape(param.shape)\n\n        img_out = torch.func.functional_call(targetnet, target_dict, xy)\n        # print(img_out.shape, img_out[:,:3].shape, img_out[:,3:].shape, pixel_intensity.shape)\n        # print( img_out[:,:3], img_out[:,3:], pixel_intensity)\n        loss = neg_loglikelyhood(img_out[:,:3],img_out[:,3:],pixel_intensity)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss = epoch_loss + loss.item()\n        i=i+1\n\n    print(\"Epoch Loss=\",epoch_loss/len(data_loader))\n\nContext Points= 100\nEpoch= 1\nEpoch Loss= -395.47315481672285\nEpoch= 2\nEpoch Loss= -915.5049703121185\nEpoch= 3\nEpoch Loss= -1166.0503022047044\nEpoch= 4\nEpoch Loss= -1349.56748127985\nEpoch= 5\nEpoch Loss= -1396.8538594449997\nEpoch= 6\nEpoch Loss= -1479.6613237543106\nEpoch= 7\nEpoch Loss= -1449.8615832103728\nEpoch= 8\nEpoch Loss= -1528.4998937654495\nEpoch= 9\nEpoch Loss= -1538.7266953744888\nEpoch= 10\nEpoch Loss= -1574.4109719749451\nEpoch= 11\nEpoch Loss= -1558.2231241334914\nEpoch= 12\nEpoch Loss= -1585.886608516693\nEpoch= 13\nEpoch Loss= -1586.6056880670546\nEpoch= 14\nEpoch Loss= -1561.2374246302604\nEpoch= 15\nEpoch Loss= -1606.3488553873062\nEpoch= 16\nEpoch Loss= -1637.4123486403466\nEpoch= 17\nEpoch Loss= -1656.406247360611\nEpoch= 18\nEpoch Loss= -1621.5405502536773\nEpoch= 19\nEpoch Loss= -1708.3212175039291\nEpoch= 20\nEpoch Loss= -1700.857941632271\n\n\n  0%|          | 0/20 [00:00&lt;?, ?it/s]  5%|▌         | 1/20 [01:05&lt;20:42, 65.40s/it] 10%|█         | 2/20 [02:10&lt;19:37, 65.41s/it] 15%|█▌        | 3/20 [03:15&lt;18:29, 65.26s/it] 20%|██        | 4/20 [04:21&lt;17:24, 65.28s/it] 25%|██▌       | 5/20 [05:26&lt;16:19, 65.30s/it] 30%|███       | 6/20 [06:31&lt;15:14, 65.31s/it] 35%|███▌      | 7/20 [07:37&lt;14:09, 65.36s/it] 40%|████      | 8/20 [08:42&lt;13:04, 65.38s/it] 45%|████▌     | 9/20 [09:47&lt;11:58, 65.31s/it] 50%|█████     | 10/20 [10:53&lt;10:53, 65.36s/it] 55%|█████▌    | 11/20 [11:58&lt;09:48, 65.35s/it] 60%|██████    | 12/20 [13:03&lt;08:42, 65.32s/it] 65%|██████▌   | 13/20 [14:09&lt;07:36, 65.27s/it] 70%|███████   | 14/20 [15:14&lt;06:31, 65.23s/it] 75%|███████▌  | 15/20 [16:19&lt;05:25, 65.20s/it] 80%|████████  | 16/20 [17:24&lt;04:20, 65.24s/it] 85%|████████▌ | 17/20 [18:30&lt;03:15, 65.29s/it] 90%|█████████ | 18/20 [19:33&lt;02:09, 64.69s/it] 95%|█████████▌| 19/20 [20:38&lt;01:04, 64.88s/it]100%|██████████| 20/20 [21:44&lt;00:00, 65.21s/it]\n\n\n\n\nsaving and loading the model\n\ntorch.save(hypernet.state_dict(), 'hypernet_model_10000.pth')\ntorch.save(targetnet.state_dict(), 'targetnet_model_10000.pth')\n\n\n# Load the hypernet and targetnet models\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.relu).to(device)\nhypernet.load_state_dict(torch.load('hypernet_model_10000.pth'))\nhypernet.eval()  # Set the model to evaluation mode\n\nHyperNet(\n  (fc1): Linear(in_features=5, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=50694, bias=True)\n)\n\n\n\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\ntargetnet.load_state_dict(torch.load('targetnet_model_10000.pth'))\ntargetnet.eval()\n\nTargetNet(\n  (fc1): Linear(in_features=2, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=128, bias=True)\n  (fc4): Linear(in_features=128, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=6, bias=True)\n)\n\n\n\n\nPlots\nloading the test data\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_2599'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),   # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ntest_data_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=False)\n\n\ndef plot_hypernet(data,hypernet,targetnet,c_idx):\n\n    pixel_intensity = data.reshape(3,-1).T.to(device).float()\n    input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n    hyper_out = hypernet(input) # hyper_out is a tensor of shape (n_context, total_params)\n    hyper_out = torch.mean(hyper_out,dim=0) # aggregate across context points\n\n    target_dict ={}\n    start = 0\n    for name,param in targetnet.named_parameters():\n        end = start + param.numel()\n        target_dict[name] = hyper_out[start:end].reshape(param.shape)\n        start = end\n\n    img_out = torch.func.functional_call(targetnet, target_dict, xy)\n    return img_out.cpu().detach()\n\n\nc_1 = np.array(random.sample(range(img_size*img_size),1))\nc_10 = np.array(random.sample(range(img_size*img_size),10))\nc_100 = np.array(random.sample(range(img_size*img_size),100))\nc_1000 = np.array(random.sample(range(img_size*img_size),1000))\n\nimage_any = test_data_loader.dataset[0]\nidx = 0\ndata = image_any\n\n\nplt.figure(figsize=(9,7),constrained_layout=True)\nplt.suptitle(\"HyperNetworks\",fontsize=20)\ndef plot_image(i,j,k, data,hypernet,targetnet, c_idx):\n    plt.subplot(i,j,k)\n    img = data.permute(1,2,0)\n    mask = np.zeros((32,32,3))\n    mask[c_idx//32,c_idx%32,:] = 1\n    plt.imshow(img*mask)\n    plt.title(f\"Context: {len(c_idx)}\")\n    plt.axis('off')\n\n    plt.subplot(i,j,k+4)\n    plot_image = plot_hypernet(data,hypernet,targetnet,c_idx)\n    plt.imshow(plot_image[:,:3].T.reshape(3,32,32).permute(1,2,0))\n    plt.axis('off')\n\n    plt.subplot(i,j,k+8)\n    var =plot_image[:,3:].exp().T.reshape(3,32,32).permute(1,2,0)\n    var = var-var.min()\n    var = var/var.max()\n    plt.imshow(var)\n    plt.axis('off')\n\n&lt;Figure size 900x700 with 0 Axes&gt;\n\n\n\ndata.shape\n\ntorch.Size([3, 32, 32])\n\n\n\nplot_image(3,4,1,data,hypernet,targetnet,c_1)\nplot_image(3,4,2,data,hypernet,targetnet,c_10)\nplot_image(3,4,3,data,hypernet,targetnet,c_100)\nplot_image(3,4,4,data,hypernet,targetnet,c_1000)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n/home/jaiswalsuraj/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/matplotlib/cm.py:478: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)"
  },
  {
    "objectID": "demo_notebooks/PML_assign4_Suraj_Aditi.html#neural-processes",
    "href": "demo_notebooks/PML_assign4_Suraj_Aditi.html#neural-processes",
    "title": "Assignment 4 PML",
    "section": "Neural Processes",
    "text": "Neural Processes\nCeleba data link\nNote book ref link\n\nloading and preprocessing\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_10000'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),        # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ndata_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=True)\n\nOriginal image after transformation\n\nplt.imshow(torch.einsum('chw -&gt; hwc', data_loader.dataset[33]))\n\n&lt;matplotlib.image.AxesImage at 0x7f14ea4afdc0&gt;\n\n\n\n\n\nOriginal image\n\nplt.imshow(torch.einsum('chw -&gt; hwc', data_loader.dataset[33]))\n\n&lt;matplotlib.image.AxesImage at 0x7f800aac09d0&gt;\n\n\n\n\n\n\nEncoder Decoder model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, z_dim,activation=torch.sin,activation_scale=30.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        if activation != torch.sin:\n            self.activation_scale = 1.0\n\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, z_dim)\n\n    def forward(self, x):\n        x = self.activation(self.linear1(x)*self.activation_scale)\n        x = self.activation(self.linear2(x)*self.activation_scale)\n        return self.linear3(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim, output_dim,activation=torch.sin,activation_scale=30.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        if activation != torch.sin:\n            self.activation_scale = 1.0\n        self.linear1 = nn.Linear(z_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear4 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear5 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.activation(self.linear1(x)*self.activation_scale)\n        x = self.activation(self.linear2(x)*self.activation_scale)\n        x = self.activation(self.linear3(x)*self.activation_scale)\n        x = self.activation(self.linear4(x)*self.activation_scale)\n        return self.linear5(x)\n\n\nfrom torchinfo import summary\nencoder = Encoder(5, 256, 128, activation=torch.relu,activation_scale=1)\nsummary(encoder,input_size=(img_size*img_size,5)) # 32*32 = 1024 is the image size length, 5 is the input(x,y,r,g,b) to hypernet\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nEncoder                                  [1024, 128]               --\n├─Linear: 1-1                            [1024, 256]               1,536\n├─Linear: 1-2                            [1024, 256]               65,792\n├─Linear: 1-3                            [1024, 128]               32,896\n==========================================================================================\nTotal params: 100,224\nTrainable params: 100,224\nNon-trainable params: 0\nTotal mult-adds (M): 102.63\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 5.24\nParams size (MB): 0.40\nEstimated Total Size (MB): 5.66\n==========================================================================================\n\n\n\nprint(encoder)\n\nEncoder(\n  (linear1): Linear(in_features=5, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=128, bias=True)\n)\n\n\n\ndecoder = Decoder(130, 256, 6, activation=torch.relu,activation_scale=1)\nsummary(decoder,input_size=(img_size*img_size,130))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDecoder                                  [1024, 6]                 --\n├─Linear: 1-1                            [1024, 256]               33,536\n├─Linear: 1-2                            [1024, 256]               65,792\n├─Linear: 1-3                            [1024, 256]               65,792\n├─Linear: 1-4                            [1024, 256]               65,792\n├─Linear: 1-5                            [1024, 6]                 1,542\n==========================================================================================\nTotal params: 232,454\nTrainable params: 232,454\nNon-trainable params: 0\nTotal mult-adds (M): 238.03\n==========================================================================================\nInput size (MB): 0.53\nForward/backward pass size (MB): 8.44\nParams size (MB): 0.93\nEstimated Total Size (MB): 9.90\n==========================================================================================\n\n\n\nprint(decoder)\n\nDecoder(\n  (linear1): Linear(in_features=130, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=256, bias=True)\n  (linear4): Linear(in_features=256, out_features=256, bias=True)\n  (linear5): Linear(in_features=256, out_features=6, bias=True)\n)\n\n\n\n\ninitialize the input\n\ncorr, vals = create_coordinate_map(data_loader.dataset[0])\ncorr, vals\n\n(tensor([[ 0.,  0.],\n         [ 1.,  0.],\n         [ 2.,  0.],\n         ...,\n         [29., 31.],\n         [30., 31.],\n         [31., 31.]], device='cuda:2'),\n tensor([[0.4510, 0.4706, 0.4824],\n         [0.4745, 0.4745, 0.4471],\n         [0.4667, 0.4353, 0.5412],\n         ...,\n         [0.0314, 0.0549, 0.0471],\n         [0.0431, 0.0392, 0.0510],\n         [0.0549, 0.0392, 0.0549]], device='cuda:2'))\n\n\n\nscaler_img = preprocessing.MinMaxScaler().fit(corr.cpu())\nxy = torch.tensor(scaler_img.transform(corr.cpu())).float().to(device)\nxy, xy.shape\n\n(tensor([[0.0000, 0.0000],\n         [0.0323, 0.0000],\n         [0.0645, 0.0000],\n         ...,\n         [0.9355, 1.0000],\n         [0.9677, 1.0000],\n         [1.0000, 1.0000]], device='cuda:2'),\n torch.Size([1024, 2]))\n\n\n\n\n\nTraining loop\n\nn_epochs=20\nlr = 0.003\nn_context = 200\nprint(\"Context Points=\",n_context)\n\nencoder = Encoder(input_dim=5, hidden_dim=512, z_dim=128,activation=torch.relu,activation_scale=1).to(device)\ndecoder = Decoder(z_dim=130, hidden_dim=512, output_dim=6,activation=torch.relu,activation_scale=1).to(device)\noptimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()),lr=lr)\n\nfor epoch in trange(n_epochs):\n\n    c_idx = np.array(random.sample(range(1023),n_context))\n\n    print(\"Epoch=\",epoch+1)\n    epoch_loss = 0\n    i=1\n    for data in data_loader:\n        # print(data.shape)\n\n        optimizer.zero_grad()\n\n        pixel_intensity = data.reshape(3,-1).T.to(device).float()\n        input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n        encoder_out = encoder(input)\n        encoder_out = torch.mean(encoder_out,dim=0)\n\n        decoder_in = encoder_out.repeat(1024,1)\n        decoder_in = torch.concatenate([xy,decoder_in],axis=1)\n\n        img_out = decoder(decoder_in)\n\n        loss = neg_loglikelyhood(img_out[:,:3],img_out[:,3:],pixel_intensity)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss = epoch_loss + loss.item()\n        i=i+1\n    print(\"Epoch Loss=\",epoch_loss/len(data_loader))\n\nContext Points= 200\nEpoch= 1\nEpoch Loss= 47.26116828255653\nEpoch= 2\nEpoch Loss= -455.4421627301693\nEpoch= 3\nEpoch Loss= -683.1707640041351\nEpoch= 4\nEpoch Loss= -761.2885692318916\nEpoch= 5\nEpoch Loss= -827.9153870079041\nEpoch= 6\nEpoch Loss= -938.4062066322326\nEpoch= 7\nEpoch Loss= -1007.5277942465782\nEpoch= 8\nEpoch Loss= -1048.726986592865\nEpoch= 9\nEpoch Loss= -1057.8311284263611\nEpoch= 10\nEpoch Loss= -1070.1760208235742\nEpoch= 11\nEpoch Loss= -1065.5062245418549\nEpoch= 12\nEpoch Loss= -1078.0465439793586\nEpoch= 13\nEpoch Loss= -1088.4120592634201\nEpoch= 14\nEpoch Loss= -1078.4957633354188\nEpoch= 15\nEpoch Loss= -1084.8360796244622\nEpoch= 16\nEpoch Loss= -1093.4410486424447\nEpoch= 17\nEpoch Loss= -1113.8205106693267\nEpoch= 18\nEpoch Loss= -1110.6333978479386\nEpoch= 19\nEpoch Loss= -1098.3281953195572\nEpoch= 20\nEpoch Loss= -1106.2516599431992\n\n\n  0%|          | 0/20 [00:00&lt;?, ?it/s]  5%|▌         | 1/20 [00:58&lt;18:27, 58.30s/it] 10%|█         | 2/20 [01:50&lt;16:27, 54.89s/it] 15%|█▌        | 3/20 [02:43&lt;15:16, 53.92s/it] 20%|██        | 4/20 [03:35&lt;14:13, 53.32s/it] 25%|██▌       | 5/20 [04:36&lt;13:56, 55.78s/it] 30%|███       | 6/20 [05:35&lt;13:20, 57.17s/it] 35%|███▌      | 7/20 [06:30&lt;12:11, 56.24s/it] 40%|████      | 8/20 [07:30&lt;11:28, 57.39s/it] 45%|████▌     | 9/20 [08:20&lt;10:06, 55.15s/it] 50%|█████     | 10/20 [09:17&lt;09:17, 55.74s/it] 55%|█████▌    | 11/20 [09:56&lt;07:34, 50.50s/it] 60%|██████    | 12/20 [10:35&lt;06:16, 47.06s/it] 65%|██████▌   | 13/20 [11:14&lt;05:13, 44.75s/it] 70%|███████   | 14/20 [11:53&lt;04:17, 42.99s/it] 75%|███████▌  | 15/20 [12:32&lt;03:28, 41.78s/it] 80%|████████  | 16/20 [13:11&lt;02:43, 40.94s/it] 85%|████████▌ | 17/20 [13:50&lt;02:01, 40.44s/it] 90%|█████████ | 18/20 [14:29&lt;01:19, 39.99s/it] 95%|█████████▌| 19/20 [15:25&lt;00:44, 44.66s/it]100%|██████████| 20/20 [16:25&lt;00:00, 49.28s/it]\n\n\n\n\nsaving and loading the model\n\ntorch.save(encoder.state_dict(), 'encoder_model_10000.pth')\ntorch.save(decoder.state_dict(), 'decoder_model_10000.pth')\n\n\n# Load the hypernet and targetnet models\nencoder = Encoder(input_dim=5, hidden_dim=128, z_dim=128,activation=torch.relu,activation_scale=1).to(device)\n\nencoder.load_state_dict(torch.load('encoder_model_10000.pth'))\nencoder.eval()  # Set the model to evaluation mode\n\nEncoder(\n  (linear1): Linear(in_features=5, out_features=128, bias=True)\n  (linear2): Linear(in_features=128, out_features=128, bias=True)\n  (linear3): Linear(in_features=128, out_features=128, bias=True)\n)\n\n\n\ndecoder = Decoder(z_dim=130, hidden_dim=256, output_dim=6,activation=torch.relu,activation_scale=1).to(device)\ndecoder.load_state_dict(torch.load('decoder_model_10000.pth'))\ndecoder.eval()\n\nDecoder(\n  (linear1): Linear(in_features=130, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=256, bias=True)\n  (linear4): Linear(in_features=256, out_features=256, bias=True)\n  (linear5): Linear(in_features=256, out_features=6, bias=True)\n)\n\n\n\n\nPlots\nLoading the test data\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_2599'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),   # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ntest_data_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=False)\n\n\ndef plot_enc_dec(data,encoder,decoder,c_idx):\n\n    pixel_intensity = data.reshape(3,-1).T.to(device).float()\n    input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n    encoder_out = encoder(input)\n    encoder_out = torch.mean(encoder_out,dim=0)\n\n    decoder_in = encoder_out.repeat(1024,1)\n    decoder_in = torch.concatenate([xy,decoder_in],axis=1)\n\n    img_out = decoder(decoder_in)\n    return img_out.cpu().detach()\n\n\nc_1 = np.array(random.sample(range(img_size*img_size),1))\nc_10 = np.array(random.sample(range(img_size*img_size),10))\nc_100 = np.array(random.sample(range(img_size*img_size),100))\nc_1000 = np.array(random.sample(range(img_size*img_size),1000))\n\nidx = 5\nimage_any = test_data_loader.dataset[idx]\ndata = image_any\n\n\nplt.figure(figsize=(9,7),constrained_layout=True)\nplt.suptitle(\"Neural process\",fontsize=20)\ndef plot_image(i,j,k, data,encoder,decoder, c_idx):\n    plt.subplot(i,j,k)\n    img = data.permute(1,2,0)\n    mask = np.zeros((32,32,3))\n    mask[c_idx//32,c_idx%32,:] = 1\n    plt.imshow(img*mask)\n    plt.title(f\"Context: {len(c_idx)}\")\n    plt.axis('off')\n\n    plt.subplot(i,j,k+4)\n    plot_image = plot_enc_dec(data,encoder,decoder,c_idx)\n    plt.imshow(plot_image[:,:3].T.reshape(3,32,32).permute(1,2,0))\n    plt.axis('off')\n\n    plt.subplot(i,j,k+8)\n    var =plot_image[:,3:].exp().T.reshape(3,32,32).permute(1,2,0)\n    var = var-var.min()\n    var = var/var.max()\n    plt.imshow(var)\n    plt.axis('off')\n\n&lt;Figure size 900x700 with 0 Axes&gt;\n\n\n\nplot_image(3,4,1,data,encoder,decoder,c_1)\nplot_image(3,4,2,data,encoder,decoder,c_10)\nplot_image(3,4,3,data,encoder,decoder,c_100)\nplot_image(3,4,4,data,encoder,decoder,c_1000)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)."
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "",
    "text": "Our main goal is to predict the whole image given a few context points of image for each image.\nMotivation: Why are we doing this? Say, for example, you have less bandwidth, but you want to send an image to your friend. What you can do is to compress the image to few context points using NN model and give this compressed image and the model to your friend. Your friend can use this model and these few context points of the original image to reconstruct the whole image.\nSimple ways can be as follows, but we have some issues with these approaches: - For this, if we learn a single neural network for all images, then this will be a general model, but at testing time, we don’t have the whole image, so we won’t be able to use the same model. - And if we learn a neural network for each image, then this will be a task-specific model, but at testing time, we don’t know which image model to pick for a new image.\nSo, we use a Meta learning setup using hypernet and neural processes to learn a task-specific neural network that predicts the whole image given a few context points of an image.\nWhy meta? Because we are learning a model that learns the parameters of another model.\nWe use our version of the following to reproduce Figure 4 from the paper referenced at link conditional neural network paper. - Hypernet - Neural Processes\nimport torch\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom tqdm import trange\n\nfrom PIL import Image\nimport os\nfrom tabulate import tabulate\n# select gpu\ndevice = torch.device(\"cuda:3\")\nprint(device)\ncurrent_device = device #torch.cuda.current_device()\ndevice_name = torch.cuda.get_device_name(current_device)\nprint(f\"Current GPU assigned: {current_device}, Name: {device_name}\")\n\ncuda:3\nCurrent GPU assigned: cuda:3, Name: NVIDIA A100-SXM4-80GB"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#model-defination",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#model-defination",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Model defination",
    "text": "Model defination\n\nTarget net defination\n\n# Create a MLP with 5 hidden layers with 256 neurons each and ReLU activations.\n# Input is (x, y) and output is (r, g, b) or (g) for grayscale\n# here we output 6 values (3 for RGB mean and 3 for RGB std)\ns = 128 # hidden dim of model\n\nclass TargetNet(nn.Module):\n    def _init_siren(self, activation_scale):\n        self.fc1.weight.data.uniform_(-1/self.fc1.in_features, 1/self.fc1.in_features)\n        for layers in [self.fc2, self.fc3, self.fc4, self.fc5]:\n            layers.weight.data.uniform_(-np.sqrt(6/self.fc2.in_features)/activation_scale,\n                                        np.sqrt(6/self.fc2.in_features)/activation_scale)\n\n    def __init__(self, activation=torch.relu, n_out=1, activation_scale=1.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        self.fc1 = nn.Linear(2, s) # input size is 2 (x, y) location of pixel\n        self.fc2 = nn.Linear(s, s)\n        self.fc3 = nn.Linear(s, s)\n        self.fc4 = nn.Linear(s, s)\n        self.fc5 = nn.Linear(s, n_out) #gray scale image (1) or RGB (3)\n        if self.activation == torch.sin:\n            # init weights and biases for sine activation\n            self._init_siren(activation_scale=self.activation_scale)\n\n    def forward(self, x):\n        x = self.activation(self.activation_scale*self.fc1(x))\n        x = self.activation(self.activation_scale*self.fc2(x))\n        x = self.activation(self.activation_scale*self.fc3(x))\n        x = self.activation(self.activation_scale*self.fc4(x))\n        return self.fc5(x)\n\n\n\nHypernetwork defination\nInput: (x, y, R, G, B)\nOutput: Our Hypernetwork should have the output equal to the number of parameters in the main network.\n\n# pass total params of target network before calling the hypernetwork model\nclass HyperNet(nn.Module):\n    def __init__(self, total_params, num_neurons=128, activation=torch.relu):\n        super().__init__()\n        self.activation = activation\n        self.n_out = total_params\n        self.fc1 = nn.Linear(5, num_neurons)\n        self.fc2 = nn.Linear(num_neurons, num_neurons)\n        self.fc3 = nn.Linear(num_neurons, self.n_out)\n\n    def forward(self, x):\n        x = self.activation(self.fc1(x))\n        x = self.activation(self.fc2(x))\n        return self.fc3(x)"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#initialize-the-model-and-input",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#initialize-the-model-and-input",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Initialize the model and input",
    "text": "Initialize the model and input\n\nInitialize the target network\n\nfrom torchinfo import summary\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\nsummary(targetnet, input_size=(img_size* img_size, 2)) #32*32 =1024 is the image size lentgh, 2 is x,y coordinate\n# outputs 6: 1,2,3 mean of each channel and 4,5,6 are log sigma of each channel\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTargetNet                                [1024, 6]                 --\n├─Linear: 1-1                            [1024, 128]               384\n├─Linear: 1-2                            [1024, 128]               16,512\n├─Linear: 1-3                            [1024, 128]               16,512\n├─Linear: 1-4                            [1024, 128]               16,512\n├─Linear: 1-5                            [1024, 6]                 774\n==========================================================================================\nTotal params: 50,694\nTrainable params: 50,694\nNon-trainable params: 0\nTotal mult-adds (M): 51.91\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 4.24\nParams size (MB): 0.20\nEstimated Total Size (MB): 4.45\n==========================================================================================\n\n\n\ntargetnet\n\nTargetNet(\n  (fc1): Linear(in_features=2, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=128, bias=True)\n  (fc4): Linear(in_features=128, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=6, bias=True)\n)\n\n\n\ncount_params(targetnet)\n\ntensor(50694)\n\n\n\n\ninitialize the hypernetwork model\n\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.sin).to(device)\nprint(hypernet)\n\nHyperNet(\n  (fc1): Linear(in_features=5, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=50694, bias=True)\n)\n\n\n\nsummary(hypernet,input_size=(img_size* img_size,5))  # 32*32 = 1024 is the image size length, 5 is the input(x,y,r,g,b) to hypernet\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nHyperNet                                 [1024, 50694]             --\n├─Linear: 1-1                            [1024, 128]               768\n├─Linear: 1-2                            [1024, 128]               16,512\n├─Linear: 1-3                            [1024, 50694]             6,539,526\n==========================================================================================\nTotal params: 6,556,806\nTrainable params: 6,556,806\nNon-trainable params: 0\nTotal mult-adds (G): 6.71\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 417.38\nParams size (MB): 26.23\nEstimated Total Size (MB): 443.63\n==========================================================================================\n\n\n\ntable_data = []\ntotal_params = 0\nstart = 0\nstart_end_mapping = {}\nfor name, param in targetnet.named_parameters():\n    param_count = torch.prod(torch.tensor(param.shape)).item()\n    total_params += param_count\n    end = total_params\n    table_data.append([name, param.shape, param_count, start, end])\n    start_end_mapping[name] = (start, end)\n    start = end\n\nprint(tabulate(table_data, headers=[\"Layer Name\", \"Shape\", \"Parameter Count\", \"Start Index\", \"End Index\"]))\nprint(f\"Total number of parameters: {total_params}\")\n\nLayer Name    Shape                     Parameter Count    Start Index    End Index\n------------  ----------------------  -----------------  -------------  -----------\nfc1.weight    torch.Size([128, 2])                  256              0          256\nfc1.bias      torch.Size([128])                     128            256          384\nfc2.weight    torch.Size([128, 128])              16384            384        16768\nfc2.bias      torch.Size([128])                     128          16768        16896\nfc3.weight    torch.Size([128, 128])              16384          16896        33280\nfc3.bias      torch.Size([128])                     128          33280        33408\nfc4.weight    torch.Size([128, 128])              16384          33408        49792\nfc4.bias      torch.Size([128])                     128          49792        49920\nfc5.weight    torch.Size([6, 128])                  768          49920        50688\nfc5.bias      torch.Size([6])                         6          50688        50694\nTotal number of parameters: 50694\n\n\n\n\nInitialize the input\n\ncorr, vals = create_coordinate_map(data_loader.dataset[0])\ncorr, vals\n\n(tensor([[ 0.,  0.],\n         [ 1.,  0.],\n         [ 2.,  0.],\n         ...,\n         [29., 31.],\n         [30., 31.],\n         [31., 31.]], device='cuda:3'),\n tensor([[0.4510, 0.4706, 0.4824],\n         [0.4745, 0.4745, 0.4471],\n         [0.4667, 0.4353, 0.5412],\n         ...,\n         [0.0314, 0.0549, 0.0471],\n         [0.0431, 0.0392, 0.0510],\n         [0.0549, 0.0392, 0.0549]], device='cuda:3'))\n\n\n\nscaler_img = preprocessing.MinMaxScaler().fit(corr.cpu())\nxy = torch.tensor(scaler_img.transform(corr.cpu())).float().to(device)\nxy, xy.shape\n\n(tensor([[0.0000, 0.0000],\n         [0.0323, 0.0000],\n         [0.0645, 0.0000],\n         ...,\n         [0.9355, 1.0000],\n         [0.9677, 1.0000],\n         [1.0000, 1.0000]], device='cuda:3'),\n torch.Size([1024, 2]))"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#training-loop",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#training-loop",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Training loop",
    "text": "Training loop\n\nn_epochs=20\nlr = 0.003\n\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.relu).to(device)\noptimizer = optim.Adam(hypernet.parameters(),lr=lr) # only hypernet is updated\n\nn_context = 100\nprint(\"Context Points=\",n_context)\nfor epoch in trange(n_epochs):\n\n    c_idx = np.array(random.sample(range(1023),n_context))\n\n    print(\"Epoch=\",epoch+1)\n    epoch_loss = 0\n    i=1\n\n    for data in data_loader:\n        # print(data.shape)\n        optimizer.zero_grad()\n\n        pixel_intensity = data.reshape(3,-1).T.to(device).float()\n        input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n        hyper_out = hypernet(input)\n        hyper_out = torch.mean(hyper_out,dim=0)\n\n        target_dict ={}\n        for name,param in targetnet.named_parameters():\n            start,end = start_end_mapping[name]\n            target_dict[name] = hyper_out[start:end].reshape(param.shape)\n\n        img_out = torch.func.functional_call(targetnet, target_dict, xy)\n        # print(img_out.shape, img_out[:,:3].shape, img_out[:,3:].shape, pixel_intensity.shape)\n        # print( img_out[:,:3], img_out[:,3:], pixel_intensity)\n        loss = neg_loglikelyhood(img_out[:,:3],img_out[:,3:],pixel_intensity)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss = epoch_loss + loss.item()\n        i=i+1\n\n    print(\"Epoch Loss=\",epoch_loss/len(data_loader))\n\nContext Points= 100\nEpoch= 1\nEpoch Loss= -395.47315481672285\nEpoch= 2\nEpoch Loss= -915.5049703121185\nEpoch= 3\nEpoch Loss= -1166.0503022047044\nEpoch= 4\nEpoch Loss= -1349.56748127985\nEpoch= 5\nEpoch Loss= -1396.8538594449997\nEpoch= 6\nEpoch Loss= -1479.6613237543106\nEpoch= 7\nEpoch Loss= -1449.8615832103728\nEpoch= 8\nEpoch Loss= -1528.4998937654495\nEpoch= 9\nEpoch Loss= -1538.7266953744888\nEpoch= 10\nEpoch Loss= -1574.4109719749451\nEpoch= 11\nEpoch Loss= -1558.2231241334914\nEpoch= 12\nEpoch Loss= -1585.886608516693\nEpoch= 13\nEpoch Loss= -1586.6056880670546\nEpoch= 14\nEpoch Loss= -1561.2374246302604\nEpoch= 15\nEpoch Loss= -1606.3488553873062\nEpoch= 16\nEpoch Loss= -1637.4123486403466\nEpoch= 17\nEpoch Loss= -1656.406247360611\nEpoch= 18\nEpoch Loss= -1621.5405502536773\nEpoch= 19\nEpoch Loss= -1708.3212175039291\nEpoch= 20\nEpoch Loss= -1700.857941632271\n\n\n  0%|          | 0/20 [00:00&lt;?, ?it/s]  5%|▌         | 1/20 [01:05&lt;20:42, 65.40s/it] 10%|█         | 2/20 [02:10&lt;19:37, 65.41s/it] 15%|█▌        | 3/20 [03:15&lt;18:29, 65.26s/it] 20%|██        | 4/20 [04:21&lt;17:24, 65.28s/it] 25%|██▌       | 5/20 [05:26&lt;16:19, 65.30s/it] 30%|███       | 6/20 [06:31&lt;15:14, 65.31s/it] 35%|███▌      | 7/20 [07:37&lt;14:09, 65.36s/it] 40%|████      | 8/20 [08:42&lt;13:04, 65.38s/it] 45%|████▌     | 9/20 [09:47&lt;11:58, 65.31s/it] 50%|█████     | 10/20 [10:53&lt;10:53, 65.36s/it] 55%|█████▌    | 11/20 [11:58&lt;09:48, 65.35s/it] 60%|██████    | 12/20 [13:03&lt;08:42, 65.32s/it] 65%|██████▌   | 13/20 [14:09&lt;07:36, 65.27s/it] 70%|███████   | 14/20 [15:14&lt;06:31, 65.23s/it] 75%|███████▌  | 15/20 [16:19&lt;05:25, 65.20s/it] 80%|████████  | 16/20 [17:24&lt;04:20, 65.24s/it] 85%|████████▌ | 17/20 [18:30&lt;03:15, 65.29s/it] 90%|█████████ | 18/20 [19:33&lt;02:09, 64.69s/it] 95%|█████████▌| 19/20 [20:38&lt;01:04, 64.88s/it]100%|██████████| 20/20 [21:44&lt;00:00, 65.21s/it]\n\n\n\nsaving and loading the model\n\ntorch.save(hypernet.state_dict(), 'hypernet_model_10000.pth')\ntorch.save(targetnet.state_dict(), 'targetnet_model_10000.pth')\n\n\n# Load the hypernet and targetnet models\nhypernet = HyperNet(total_params=count_params(targetnet), activation=torch.relu).to(device)\nhypernet.load_state_dict(torch.load('hypernet_model_10000.pth'))\nhypernet.eval()  # Set the model to evaluation mode\n\nHyperNet(\n  (fc1): Linear(in_features=5, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=50694, bias=True)\n)\n\n\n\ntargetnet = TargetNet(activation=torch.relu, n_out=6, activation_scale=1).to(device)\ntargetnet.load_state_dict(torch.load('targetnet_model_10000.pth'))\ntargetnet.eval()\n\nTargetNet(\n  (fc1): Linear(in_features=2, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=128, bias=True)\n  (fc3): Linear(in_features=128, out_features=128, bias=True)\n  (fc4): Linear(in_features=128, out_features=128, bias=True)\n  (fc5): Linear(in_features=128, out_features=6, bias=True)\n)"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#testing",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#testing",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Testing",
    "text": "Testing\nTesting phase architecture\n\nloading the test data\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = '/home/jaiswalsuraj/suraj_work/projects/data/celeba/img_align_celeba_2599'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),   # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ntest_data_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=False)\n\n\nPlotting the results\n\ndef plot_hypernet(data,hypernet,targetnet,c_idx):\n\n    pixel_intensity = data.reshape(3,-1).T.to(device).float()\n    input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n    hyper_out = hypernet(input) # hyper_out is a tensor of shape (n_context, total_params)\n    hyper_out = torch.mean(hyper_out,dim=0) # aggregate across context points\n\n    target_dict ={}\n    start = 0\n    for name,param in targetnet.named_parameters():\n        end = start + param.numel()\n        target_dict[name] = hyper_out[start:end].reshape(param.shape)\n        start = end\n\n    img_out = torch.func.functional_call(targetnet, target_dict, xy)\n    return img_out.cpu().detach()\n\n\nc_1 = np.array(random.sample(range(img_size*img_size),1))\nc_10 = np.array(random.sample(range(img_size*img_size),10))\nc_100 = np.array(random.sample(range(img_size*img_size),100))\nc_1000 = np.array(random.sample(range(img_size*img_size),1000))\n\nimage_any = test_data_loader.dataset[0]\nidx = 0\ndata = image_any\n\n\nplt.figure(figsize=(9,7),constrained_layout=True)\nplt.suptitle(\"HyperNetworks\",fontsize=20)\ndef plot_image(i,j,k, data,hypernet,targetnet, c_idx):\n    plt.subplot(i,j,k)\n    img = data.permute(1,2,0)\n    mask = np.zeros((32,32,3))\n    mask[c_idx//32,c_idx%32,:] = 1\n    plt.imshow(img*mask)\n    plt.title(f\"Context: {len(c_idx)}\")\n    plt.axis('off')\n\n    plt.subplot(i,j,k+4)\n    plot_image = plot_hypernet(data,hypernet,targetnet,c_idx)\n    plt.imshow(plot_image[:,:3].T.reshape(3,32,32).permute(1,2,0))\n    plt.axis('off')\n\n    plt.subplot(i,j,k+8)\n    var =plot_image[:,3:].exp().T.reshape(3,32,32).permute(1,2,0)\n    var = var-var.min()\n    var = var/var.max()\n    plt.imshow(var)\n    plt.axis('off')\n\n&lt;Figure size 900x700 with 0 Axes&gt;\n\n\n\nplot_image(3,4,1,data,hypernet,targetnet,c_1)\nplot_image(3,4,2,data,hypernet,targetnet,c_10)\nplot_image(3,4,3,data,hypernet,targetnet,c_100)\nplot_image(3,4,4,data,hypernet,targetnet,c_1000)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n/home/jaiswalsuraj/miniconda3/envs/tf_gpu/lib/python3.10/site-packages/matplotlib/cm.py:478: RuntimeWarning: invalid value encountered in cast\n  xx = (xx * 255).astype(np.uint8)\n\n\n\n\n\nThe first row shows the test context points, second row shows our model prediction and third row shows the variance of the predicted image."
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#encoder-decoder-model-defination",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#encoder-decoder-model-defination",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Encoder Decoder model defination",
    "text": "Encoder Decoder model defination\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, z_dim,activation=torch.sin,activation_scale=30.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        if activation != torch.sin:\n            self.activation_scale = 1.0\n\n        self.linear1 = nn.Linear(input_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, z_dim)\n\n    def forward(self, x):\n        x = self.activation(self.linear1(x)*self.activation_scale)\n        x = self.activation(self.linear2(x)*self.activation_scale)\n        return self.linear3(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, z_dim, hidden_dim, output_dim,activation=torch.sin,activation_scale=30.0):\n        super().__init__()\n        self.activation = activation\n        self.activation_scale = activation_scale\n        if activation != torch.sin:\n            self.activation_scale = 1.0\n        self.linear1 = nn.Linear(z_dim, hidden_dim)\n        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear3 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear4 = nn.Linear(hidden_dim, hidden_dim)\n        self.linear5 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = self.activation(self.linear1(x)*self.activation_scale)\n        x = self.activation(self.linear2(x)*self.activation_scale)\n        x = self.activation(self.linear3(x)*self.activation_scale)\n        x = self.activation(self.linear4(x)*self.activation_scale)\n        return self.linear5(x)\n\n\nfrom torchinfo import summary\nencoder = Encoder(5, 256, 128, activation=torch.relu,activation_scale=1)\nsummary(encoder,input_size=(img_size*img_size,5)) # 32*32 = 1024 is the image size length, 5 is the input(x,y,r,g,b) to hypernet\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nEncoder                                  [1024, 128]               --\n├─Linear: 1-1                            [1024, 256]               1,536\n├─Linear: 1-2                            [1024, 256]               65,792\n├─Linear: 1-3                            [1024, 128]               32,896\n==========================================================================================\nTotal params: 100,224\nTrainable params: 100,224\nNon-trainable params: 0\nTotal mult-adds (M): 102.63\n==========================================================================================\nInput size (MB): 0.02\nForward/backward pass size (MB): 5.24\nParams size (MB): 0.40\nEstimated Total Size (MB): 5.66\n==========================================================================================\n\n\n\nprint(encoder)\n\nEncoder(\n  (linear1): Linear(in_features=5, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=128, bias=True)\n)\n\n\n\ndecoder = Decoder(130, 256, 6, activation=torch.relu,activation_scale=1)\nsummary(decoder,input_size=(img_size*img_size,130))\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nDecoder                                  [1024, 6]                 --\n├─Linear: 1-1                            [1024, 256]               33,536\n├─Linear: 1-2                            [1024, 256]               65,792\n├─Linear: 1-3                            [1024, 256]               65,792\n├─Linear: 1-4                            [1024, 256]               65,792\n├─Linear: 1-5                            [1024, 6]                 1,542\n==========================================================================================\nTotal params: 232,454\nTrainable params: 232,454\nNon-trainable params: 0\nTotal mult-adds (M): 238.03\n==========================================================================================\nInput size (MB): 0.53\nForward/backward pass size (MB): 8.44\nParams size (MB): 0.93\nEstimated Total Size (MB): 9.90\n==========================================================================================\n\n\n\nprint(decoder)\n\nDecoder(\n  (linear1): Linear(in_features=130, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=256, bias=True)\n  (linear4): Linear(in_features=256, out_features=256, bias=True)\n  (linear5): Linear(in_features=256, out_features=6, bias=True)\n)"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#initialize-the-input-1",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#initialize-the-input-1",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Initialize the input",
    "text": "Initialize the input\n\ncorr, vals = create_coordinate_map(data_loader.dataset[0])\ncorr, vals\n\n(tensor([[ 0.,  0.],\n         [ 1.,  0.],\n         [ 2.,  0.],\n         ...,\n         [29., 31.],\n         [30., 31.],\n         [31., 31.]], device='cuda:2'),\n tensor([[0.4510, 0.4706, 0.4824],\n         [0.4745, 0.4745, 0.4471],\n         [0.4667, 0.4353, 0.5412],\n         ...,\n         [0.0314, 0.0549, 0.0471],\n         [0.0431, 0.0392, 0.0510],\n         [0.0549, 0.0392, 0.0549]], device='cuda:2'))\n\n\n\nscaler_img = preprocessing.MinMaxScaler().fit(corr.cpu())\nxy = torch.tensor(scaler_img.transform(corr.cpu())).float().to(device)\nxy, xy.shape\n\n(tensor([[0.0000, 0.0000],\n         [0.0323, 0.0000],\n         [0.0645, 0.0000],\n         ...,\n         [0.9355, 1.0000],\n         [0.9677, 1.0000],\n         [1.0000, 1.0000]], device='cuda:2'),\n torch.Size([1024, 2]))"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#training-loop-1",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#training-loop-1",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Training loop",
    "text": "Training loop\n\nn_epochs=20\nlr = 0.003\nn_context = 200\nprint(\"Context Points=\",n_context)\n\nencoder = Encoder(input_dim=5, hidden_dim=512, z_dim=128,activation=torch.relu,activation_scale=1).to(device)\ndecoder = Decoder(z_dim=130, hidden_dim=512, output_dim=6,activation=torch.relu,activation_scale=1).to(device)\noptimizer = optim.Adam(list(encoder.parameters())+list(decoder.parameters()),lr=lr)\n\nfor epoch in trange(n_epochs):\n\n    c_idx = np.array(random.sample(range(1023),n_context))\n\n    print(\"Epoch=\",epoch+1)\n    epoch_loss = 0\n    i=1\n    for data in data_loader:\n        # print(data.shape)\n\n        optimizer.zero_grad()\n\n        pixel_intensity = data.reshape(3,-1).T.to(device).float()\n        input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n        encoder_out = encoder(input)\n        encoder_out = torch.mean(encoder_out,dim=0)\n\n        decoder_in = encoder_out.repeat(1024,1)\n        decoder_in = torch.concatenate([xy,decoder_in],axis=1)\n\n        img_out = decoder(decoder_in)\n\n        loss = neg_loglikelyhood(img_out[:,:3],img_out[:,3:],pixel_intensity)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss = epoch_loss + loss.item()\n        i=i+1\n    print(\"Epoch Loss=\",epoch_loss/len(data_loader))\n\nContext Points= 200\nEpoch= 1\nEpoch Loss= 47.26116828255653\nEpoch= 2\nEpoch Loss= -455.4421627301693\nEpoch= 3\nEpoch Loss= -683.1707640041351\nEpoch= 4\nEpoch Loss= -761.2885692318916\nEpoch= 5\nEpoch Loss= -827.9153870079041\nEpoch= 6\nEpoch Loss= -938.4062066322326\nEpoch= 7\nEpoch Loss= -1007.5277942465782\nEpoch= 8\nEpoch Loss= -1048.726986592865\nEpoch= 9\nEpoch Loss= -1057.8311284263611\nEpoch= 10\nEpoch Loss= -1070.1760208235742\nEpoch= 11\nEpoch Loss= -1065.5062245418549\nEpoch= 12\nEpoch Loss= -1078.0465439793586\nEpoch= 13\nEpoch Loss= -1088.4120592634201\nEpoch= 14\nEpoch Loss= -1078.4957633354188\nEpoch= 15\nEpoch Loss= -1084.8360796244622\nEpoch= 16\nEpoch Loss= -1093.4410486424447\nEpoch= 17\nEpoch Loss= -1113.8205106693267\nEpoch= 18\nEpoch Loss= -1110.6333978479386\nEpoch= 19\nEpoch Loss= -1098.3281953195572\nEpoch= 20\nEpoch Loss= -1106.2516599431992\n\n\n  0%|          | 0/20 [00:00&lt;?, ?it/s]  5%|▌         | 1/20 [00:58&lt;18:27, 58.30s/it] 10%|█         | 2/20 [01:50&lt;16:27, 54.89s/it] 15%|█▌        | 3/20 [02:43&lt;15:16, 53.92s/it] 20%|██        | 4/20 [03:35&lt;14:13, 53.32s/it] 25%|██▌       | 5/20 [04:36&lt;13:56, 55.78s/it] 30%|███       | 6/20 [05:35&lt;13:20, 57.17s/it] 35%|███▌      | 7/20 [06:30&lt;12:11, 56.24s/it] 40%|████      | 8/20 [07:30&lt;11:28, 57.39s/it] 45%|████▌     | 9/20 [08:20&lt;10:06, 55.15s/it] 50%|█████     | 10/20 [09:17&lt;09:17, 55.74s/it] 55%|█████▌    | 11/20 [09:56&lt;07:34, 50.50s/it] 60%|██████    | 12/20 [10:35&lt;06:16, 47.06s/it] 65%|██████▌   | 13/20 [11:14&lt;05:13, 44.75s/it] 70%|███████   | 14/20 [11:53&lt;04:17, 42.99s/it] 75%|███████▌  | 15/20 [12:32&lt;03:28, 41.78s/it] 80%|████████  | 16/20 [13:11&lt;02:43, 40.94s/it] 85%|████████▌ | 17/20 [13:50&lt;02:01, 40.44s/it] 90%|█████████ | 18/20 [14:29&lt;01:19, 39.99s/it] 95%|█████████▌| 19/20 [15:25&lt;00:44, 44.66s/it]100%|██████████| 20/20 [16:25&lt;00:00, 49.28s/it]\n\n\n\nsaving and loading the model\n\ntorch.save(encoder.state_dict(), 'encoder_model_10000.pth')\ntorch.save(decoder.state_dict(), 'decoder_model_10000.pth')\n\n\n# Load the hypernet and targetnet models\nencoder = Encoder(input_dim=5, hidden_dim=128, z_dim=128,activation=torch.relu,activation_scale=1).to(device)\n\nencoder.load_state_dict(torch.load('encoder_model_10000.pth'))\nencoder.eval()  # Set the model to evaluation mode\n\nEncoder(\n  (linear1): Linear(in_features=5, out_features=128, bias=True)\n  (linear2): Linear(in_features=128, out_features=128, bias=True)\n  (linear3): Linear(in_features=128, out_features=128, bias=True)\n)\n\n\n\ndecoder = Decoder(z_dim=130, hidden_dim=256, output_dim=6,activation=torch.relu,activation_scale=1).to(device)\ndecoder.load_state_dict(torch.load('decoder_model_10000.pth'))\ndecoder.eval()\n\nDecoder(\n  (linear1): Linear(in_features=130, out_features=256, bias=True)\n  (linear2): Linear(in_features=256, out_features=256, bias=True)\n  (linear3): Linear(in_features=256, out_features=256, bias=True)\n  (linear4): Linear(in_features=256, out_features=256, bias=True)\n  (linear5): Linear(in_features=256, out_features=6, bias=True)\n)"
  },
  {
    "objectID": "publications_and_projects/data/Hypernet_neural_process_cifar.html#testing-1",
    "href": "publications_and_projects/data/Hypernet_neural_process_cifar.html#testing-1",
    "title": "Hypernet and Neural Processes on CelebA",
    "section": "Testing",
    "text": "Testing\nTesting phase architecture\n\nLoading the test data\n\nbatch_size = 1 # keep this to 1\nimg_size = 32 # Change as needed\n\n# Specify the root directory where the dataset is located\ndata_root = 'data/celeba/img_align_celeba_2599'\n\n# Define the data transformations\ntransform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),  # Resize the images to a common size (adjust as needed)\n    transforms.ToTensor(),   # Convert images to tensors\n])\n# default shape is torch.Size([3, 218, 178])\n# Create the custom dataset\nceleba_dataset = CustomImageDataset(data_root, transform=transform)\n\n# Create a data loader\ntest_data_loader = DataLoader(celeba_dataset, batch_size=batch_size, shuffle=False)\n\n\nPlotting the results\n\ndef plot_enc_dec(data,encoder,decoder,c_idx):\n\n    pixel_intensity = data.reshape(3,-1).T.to(device).float()\n    input = torch.concatenate([xy[c_idx],pixel_intensity[c_idx]],axis=1).float()\n\n    encoder_out = encoder(input)\n    encoder_out = torch.mean(encoder_out,dim=0)\n\n    decoder_in = encoder_out.repeat(1024,1)\n    decoder_in = torch.concatenate([xy,decoder_in],axis=1)\n\n    img_out = decoder(decoder_in)\n    return img_out.cpu().detach()\n\n\nc_1 = np.array(random.sample(range(img_size*img_size),1))\nc_10 = np.array(random.sample(range(img_size*img_size),10))\nc_100 = np.array(random.sample(range(img_size*img_size),100))\nc_1000 = np.array(random.sample(range(img_size*img_size),1000))\n\nidx = 5\nimage_any = test_data_loader.dataset[idx]\ndata = image_any\n\n\nplt.figure(figsize=(9,7),constrained_layout=True)\nplt.suptitle(\"Neural process\",fontsize=20)\ndef plot_image(i,j,k, data,encoder,decoder, c_idx):\n    plt.subplot(i,j,k)\n    img = data.permute(1,2,0)\n    mask = np.zeros((32,32,3))\n    mask[c_idx//32,c_idx%32,:] = 1\n    plt.imshow(img*mask)\n    plt.title(f\"Context: {len(c_idx)}\")\n    plt.axis('off')\n\n    plt.subplot(i,j,k+4)\n    plot_image = plot_enc_dec(data,encoder,decoder,c_idx)\n    plt.imshow(plot_image[:,:3].T.reshape(3,32,32).permute(1,2,0))\n    plt.axis('off')\n\n    plt.subplot(i,j,k+8)\n    var =plot_image[:,3:].exp().T.reshape(3,32,32).permute(1,2,0)\n    var = var-var.min()\n    var = var/var.max()\n    plt.imshow(var)\n    plt.axis('off')\n\n&lt;Figure size 900x700 with 0 Axes&gt;\n\n\n\nplot_image(3,4,1,data,encoder,decoder,c_1)\nplot_image(3,4,2,data,encoder,decoder,c_10)\nplot_image(3,4,3,data,encoder,decoder,c_100)\nplot_image(3,4,4,data,encoder,decoder,c_1000)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nThe first row shows the test context points, second row shows our model prediction and third row shows the variance of the predicted image."
  }
]