[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html",
    "title": "Baysian Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#known-entities",
    "title": "Baysian Linear Regression",
    "section": "Known entities",
    "text": "Known entities\n\nsigma = 1.0 # standard deviation of the noise\nm0 = 0.0 # mean of the prior\nS0 = 1.0 # covariance of the prior  \np = 6 # order of the polynomial \n\n\nN = 100 # number of data points\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, sigma) # training targets, size Nx1"
  },
  {
    "objectID": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "href": "blogs/blogsData/Bayesian_Lineara_Regressiondemo.html#posterior",
    "title": "Baysian Linear Regression",
    "section": "Posterior",
    "text": "Posterior\nCalculating: \\[\\begin{align}\n&\\text{Parameter posterior: } p(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\mathcal N(\\boldsymbol \\theta \\,|\\, Mn,\\, Sn)\n\\end{align}\\]\n\ndef posterior(X, y, p, m0, S0, sigma):\n    \"\"\"Returns the posterior mean and covariance matrix of the weights given the training data.\"\"\"\n    poly_X = poly_features(X, p)\n\n    SN = scipy.linalg.inv(1.0 * np.eye(p+1) / S0  + 1.0/sigma**2 * poly_X.T @ poly_X)\n    mN = SN @ (m0 / S0 + (1.0/sigma**2) * poly_X.T @ y)    \n    \n    return mN, SN\n\n\nmN , SN = posterior(X, y, p ,m0, S0, sigma)\n\n\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\npoly_X_test = poly_features(Xtest, p)\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\nposterior_pred_mean = poly_X_test @ mN\n\nposterior_pred_uncertainty_para = poly_X_test @ SN @ poly_X_test.T\n\nposterior_pred_var = sigma**2 + posterior_pred_uncertainty_para\n\n\n# print(posterior_pred_mean.shape)\n# print(posterior_pred_var.shape)\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\n# plt.plot(Xtest, m_mle_test)\n# plt.plot(Xtest, m_map_test)\nposterior_pred_mean = posterior_pred_mean.flatten()\nvar_blr = np.diag(posterior_pred_uncertainty_para)\n\nconf_bound1 = np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color=\"k\")\n\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound2, posterior_pred_mean - conf_bound2, alpha = 0.1, color=\"k\")\n\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\nplt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound3, posterior_pred_mean - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.legend([\"Training data\",\"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "by Marc Deisenroth\nThe purpose of this notebook is to practice implementing some linear algebra (equations provided) and to explore some properties of linear regression.\n\nimport numpy as np\nimport scipy.linalg\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nWe consider a linear regression problem of the form \\[\ny = \\boldsymbol x^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n\\] where \\(\\boldsymbol x\\in\\mathbb{R}^D\\) are inputs and \\(y\\in\\mathbb{R}\\) are noisy observations. The parameter vector \\(\\boldsymbol\\theta\\in\\mathbb{R}^D\\) parametrizes the function.\nWe assume we have a training set \\((\\boldsymbol x_n, y_n)\\), \\(n=1,\\ldots, N\\). We summarize the sets of training inputs in \\(\\mathcal X = \\{\\boldsymbol x_1, \\ldots, \\boldsymbol x_N\\}\\) and corresponding training targets \\(\\mathcal Y = \\{y_1, \\ldots, y_N\\}\\), respectively.\nIn this tutorial, we are interested in finding good parameters \\(\\boldsymbol\\theta\\).\n\n# Define training set\nX = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\ny = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n\n# Plot the training set\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nWe will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?\n\n\n\n\n\nLet us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?\n\n\n\n\nWe are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens.\n\n\n\n\n\n\n# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-likelihood",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We will start with maximum likelihood estimation of the parameters \\(\\boldsymbol\\theta\\). In maximum likelihood estimation, we find the parameters \\(\\boldsymbol\\theta^{\\mathrm{ML}}\\) that maximize the likelihood \\[\np(\\mathcal Y | \\mathcal X, \\boldsymbol\\theta) = \\prod_{n=1}^N p(y_n | \\boldsymbol x_n, \\boldsymbol\\theta)\\,.\n\\] From the lecture we know that the maximum likelihood estimator is given by \\[\n\\boldsymbol\\theta^{\\text{ML}} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y\\in\\mathbb{R}^D\\,,\n\\] where \\[\n\\boldsymbol X = [\\boldsymbol x_1, \\ldots, \\boldsymbol x_N]^T\\in\\mathbb{R}^{N\\times D}\\,,\\quad \\boldsymbol y = [y_1, \\ldots, y_N]^T \\in\\mathbb{R}^N\\,.\n\\]\nLet us compute the maximum likelihood estimate for a given training set\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate(X, y):\n    \n    # X: N x D matrix of training inputs\n    # y: N x 1 vector of training targets/observations\n    # returns: maximum likelihood parameters (D x 1)\n    \n    N, D = X.shape\n    theta_ml = np.linalg.solve(X.T @ X, X.T @ y) ## &lt;-- SOLUTION\n    return theta_ml\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X,y)\nprint(theta_ml)\n\n[[0.499]]\n\n\nNow, make a prediction using the maximum likelihood estimate that we just found\n\n## EDIT THIS FUNCTION\ndef predict_with_estimate(Xtest, theta):\n    \n    # Xtest: K x D matrix of test inputs\n    # theta: D x 1 vector of parameters\n    # returns: prediction of f(Xtest); K x 1 vector\n    \n    prediction = Xtest @ theta ## &lt;-- SOLUTION\n    \n    return prediction \n\nNow, let’s see whether we got something useful:\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nDoes the solution above look reasonable?\nPlay around with different values of \\(\\theta\\). How do the corresponding functions change?\nModify the training targets \\(\\mathcal Y\\) and re-run your computation. What changes?\n\nLet us now look at a different training set, where we add 2.0 to every \\(y\\)-value, and compute the maximum likelihood estimate\n\nynew = y + 2.0\n\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get maximum likelihood estimate\ntheta_ml = max_lik_estimate(X, ynew)\nprint(theta_ml)\n\n# define a test set\nXtest = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest, theta_ml)\n\n# plot\nplt.figure()\nplt.plot(X, ynew, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n[[0.499]]\n\n\n\n\n\n\n\n\n\nThis maximum likelihood estimate doesn’t look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\nHow can we fix this problem?\n\nLet us now define a linear regression model that is slightly more flexible: \\[\ny = \\theta_0 + \\boldsymbol x^T \\boldsymbol\\theta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n\\] Here, we added an offset (bias) parameter \\(\\theta_0\\) to our original model.\n\n\n\n\nWhat is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n\nIf we now define the inputs to be the augmented vector \\(\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}\\), we can write the new linear regression model as \\[\ny = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\theta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\theta_{\\text{aug}} = \\begin{bmatrix}\n\\theta_0\\\\\n\\boldsymbol\\theta_1\n\\end{bmatrix}\\,.\n\\]\n\nN, D = X.shape\nX_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (D+1)\ntheta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1\n\nLet us now compute the maximum likelihood estimator for this setting. Hint: If possible, re-use code that you have already written\n\n## EDIT THIS FUNCTION\ndef max_lik_estimate_aug(X_aug, y):\n    \n    theta_aug_ml = max_lik_estimate(X_aug, y) ## &lt;-- SOLUTION\n    \n    return theta_aug_ml\n\n\ntheta_aug_ml = max_lik_estimate_aug(X_aug, y)\n\nNow, we can make predictions again:\n\n# define a test set (we also need to augment the test inputs with ones)\nXtest_aug = np.hstack([np.ones((Xtest.shape[0],1)), Xtest]) # 100 x (D + 1) vector of test inputs\n\n# predict the function values at the test points using the maximum likelihood estimator\nml_prediction = predict_with_estimate(Xtest_aug, theta_aug_ml)\n\n# plot\nplt.figure()\nplt.plot(X, y, '+', markersize=10)\nplt.plot(Xtest, ml_prediction)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nIt seems this has solved our problem! #### Question: 1. Play around with the first parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes. 2. Play around with the second parameter of \\(\\boldsymbol\\theta_{\\text{aug}}\\) and see how the fit of the function changes.\n\n\n\nSo far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs \\(\\boldsymbol x\\), as long as the parameters \\(\\boldsymbol\\theta\\) appear linearly. This means, we can learn functions of the form \\[\nf(\\boldsymbol x, \\boldsymbol\\theta) = \\sum_{k = 1}^K \\theta_k \\phi_k(\\boldsymbol x)\\,,\n\\] where the features \\(\\phi_k(\\boldsymbol x)\\) are (possibly nonlinear) transformations of the inputs \\(\\boldsymbol x\\).\nLet us have a look at an example where the observations clearly do not lie on a straight line:\n\ny = np.array([10.05, 1.5, -1.234, 0.02, 8.03]).reshape(-1,1)\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\nOne class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree \\(K\\) as \\[\n\\sum_{k=0}^K \\theta_k x^k = \\boldsymbol \\phi(x)^T\\boldsymbol\\theta\\,,\\quad\n\\boldsymbol\\phi(x)=\n\\begin{bmatrix}\nx^0\\\\\nx^1\\\\\n\\vdots\\\\\nx^K\n\\end{bmatrix}\\in\\mathbb{R}^{K+1}\\,.\n\\] Here, \\(\\boldsymbol\\phi(x)\\) is a nonlinear feature transformation of the inputs \\(x\\in\\mathbb{R}\\).\nSimilar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: \\[\n\\boldsymbol\\Phi = \\begin{bmatrix}\n\\boldsymbol\\phi(x_1) & \\boldsymbol\\phi(x_2) & \\cdots & \\boldsymbol\\phi(x_n)\n\\end{bmatrix}^T \\in\\mathbb{R}^{N\\times K+1}\n\\]\nLet us start by computing the feature matrix \\(\\boldsymbol \\Phi\\)\n\n## EDIT THIS FUNCTION\ndef poly_features(X, K):\n    \n    # X: inputs of size N x 1\n    # K: degree of the polynomial\n    # computes the feature matrix Phi (N x (K+1))\n    \n    X = X.flatten()\n    N = X.shape[0]\n    \n    #initialize Phi\n    Phi = np.zeros((N, K+1))\n    \n    # Compute the feature matrix in stages\n    for k in range(K+1):\n        Phi[:,k] = X**k ## &lt;-- SOLUTION\n    return Phi\n\nWith this feature matrix we get the maximum likelihood estimator as \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] For reasons of numerical stability, we often add a small diagonal “jitter” \\(\\kappa&gt;0\\) to \\(\\boldsymbol\\Phi^T\\boldsymbol\\Phi\\) so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes \\[\n\\boldsymbol \\theta^\\text{ML} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\kappa\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\]\n\n## EDIT THIS FUNCTION\ndef nonlinear_features_maximum_likelihood(Phi, y):\n    # Phi: features matrix for training inputs. Size of N x D\n    # y: training targets. Size of N by 1\n    # returns: maximum likelihood estimator theta_ml. Size of D x 1\n    \n    kappa = 1e-08 # 'jitter' term; good for numerical stability\n    \n    D = Phi.shape[1]  \n    \n    # maximum likelihood estimate\n    Pt = Phi.T @ y # Phi^T*y\n    PP = Phi.T @ Phi + kappa*np.eye(D) # Phi^T*Phi + kappa*I\n        \n    # maximum likelihood estimate\n    C = scipy.linalg.cho_factor(PP)\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y \n    \n    return theta_ml\n\nNow we have all the ingredients together: The computation of the feature matrix and the computation of the maximum likelihood estimator for polynomial regression. Let’s see how this works.\nTo make predictions at test inputs \\(\\boldsymbol X_{\\text{test}}\\in\\mathbb{R}\\), we need to compute the features (nonlinear transformations) \\(\\boldsymbol\\Phi_{\\text{test}}= \\boldsymbol\\phi(\\boldsymbol X_{\\text{test}})\\) of \\(\\boldsymbol X_{\\text{test}}\\) to give us the predicted mean \\[\n\\mathbb{E}[\\boldsymbol y_{\\text{test}}] = \\boldsymbol \\Phi_{\\text{test}}\\boldsymbol\\theta^{\\text{ML}}\n\\]\n\nK = 5 # Define the degree of the polynomial we wish to fit\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-4,4,100).reshape(-1,1)\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nExperiment with different polynomial degrees in the code above. #### Questions: 1. What do you observe? 2. What is a good fit?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#evaluating-the-quality-of-the-model",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "Let us have a look at a more interesting data set\n\ndef f(x):   \n    return np.cos(x) + 0.2*np.random.normal(size=(x.shape))\n\nX = np.linspace(-4,4,20).reshape(-1,1)\ny = f(X)\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\nNow, let us use the work from above and fit polynomials to this dataset.\n\n## EDIT THIS CELL\nK = 6 # Define the degree of the polynomial we wish to fit\n\nPhi = poly_features(X, K) # N x (K+1) feature matrix\n\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y) # maximum likelihood estimator\n\n# test inputs\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = f(Xtest) # ground-truth y-values\n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, K)\n\ny_pred = Phi_test @ theta_ml # predicted y-values\n\n# plot\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred)\nplt.plot(Xtest, ytest)\nplt.legend([\"data\", \"prediction\", \"ground truth observations\"])\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n\n\nTry out different degrees of polynomials.\nBased on visual inspection, what looks like the best fit?\n\nLet us now look at a more systematic way to assess the quality of the polynomial that we are trying to fit. For this, we compute the root-mean-squared-error (RMSE) between the \\(y\\)-values predicted by our polynomial and the ground-truth \\(y\\)-values. The RMSE is then defined as \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{N}\\sum_{n=1}^N(y_n - y_n^\\text{pred})^2}\n\\] Write a function that computes the RMSE.\n\n## EDIT THIS FUNCTION\ndef RMSE(y, ypred):\n    rmse = np.sqrt(np.mean((y-ypred)**2)) ## SOLUTION\n    return rmse\n\nNow compute the RMSE for different degrees of the polynomial we want to fit.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n     \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)\n    \n\nplt.figure()\nplt.plot(rmse_train)\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\");\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the best polynomial fit according to this plot?\nWrite some code that plots the function that uses the best polynomial degree (use the test set for this plot). What do you observe now?\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\n\n# feature matrix\nPhi = poly_features(X, 5)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, 5)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\nThe RMSE on the training data is somewhat misleading, because we are interested in the generalization performance of the model. Therefore, we are going to compute the RMSE on the test set and use this to choose a good polynomial degree.\n\n## EDIT THIS CELL\nK_max = 20\nrmse_train = np.zeros((K_max+1,))\nrmse_test = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict y-values of training set\n    ypred_train = Phi @ theta_ml\n    \n    # RMSE on training set\n    rmse_train[k] = RMSE(y, ypred_train)    \n    \n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test = Phi_test @ theta_ml\n    \n    # RMSE on test set\n    rmse_test[k] = RMSE(ytest, ypred_test)\n    \n\nplt.figure()\nplt.semilogy(rmse_train) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_test) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"training set\", \"test set\"]);\n\n\n\n\n\n\n\n\nWhat do you observe now?\nWhy does the RMSE for the test set not always go down?\nWhich polynomial degree would you choose now?\nPlot the fit for the “best” polynomial degree.\n\n\n# WRITE THE PLOTTING CODE HERE\nplt.figure()\nplt.plot(X, y, '+')\nk = 5\n# feature matrix\nPhi = poly_features(X, k)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)   \n\n# feature matrix for test inputs\nPhi_test = poly_features(Xtest, k)\n\nypred_test = Phi_test @ theta_ml\n\nplt.plot(Xtest, ypred_test) \nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\")\nplt.legend([\"data\", \"maximum likelihood fit\"]);\n\n\n\n\n\n\n\nIf you did not have a designated test set, what could you do to estimate the generalization error (purely using the training set)?"
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#maximum-a-posteriori-estimation",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "We are still considering the model \\[\ny = \\boldsymbol\\phi(\\boldsymbol x)^T\\boldsymbol\\theta + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\\,.\n\\] We assume that the noise variance \\(\\sigma^2\\) is known.\nInstead of maximizing the likelihood, we can look at the maximum of the posterior distribution on the parameters \\(\\boldsymbol\\theta\\), which is given as \\[\np(\\boldsymbol\\theta|\\mathcal X, \\mathcal Y) = \\frac{\\overbrace{p(\\mathcal Y|\\mathcal X, \\boldsymbol\\theta)}^{\\text{likelihood}}\\overbrace{p(\\boldsymbol\\theta)}^{\\text{prior}}}{\\underbrace{p(\\mathcal Y|\\mathcal X)}_{\\text{evidence}}}\n\\] The purpose of the parameter prior \\(p(\\boldsymbol\\theta)\\) is to discourage the parameters to attain extreme values, a sign that the model overfits. The prior allows us to specify a “reasonable” range of parameter values. Typically, we choose a Gaussian prior \\(\\mathcal N(\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\), centered at \\(\\boldsymbol 0\\) with variance \\(\\alpha^2\\) along each parameter dimension.\nThe MAP estimate of the parameters is \\[\n\\boldsymbol\\theta^{\\text{MAP}} = (\\boldsymbol\\Phi^T\\boldsymbol\\Phi + \\frac{\\sigma^2}{\\alpha^2}\\boldsymbol I)^{-1}\\boldsymbol\\Phi^T\\boldsymbol y\n\\] where \\(\\sigma^2\\) is the variance of the noise.\n\n## EDIT THIS FUNCTION\ndef map_estimate_poly(Phi, y, sigma, alpha):\n    # Phi: training inputs, Size of N x D\n    # y: training targets, Size of D x 1\n    # sigma: standard deviation of the noise \n    # alpha: standard deviation of the prior on the parameters\n    # returns: MAP estimate theta_map, Size of D x 1\n    \n    D = Phi.shape[1] \n    \n    # SOLUTION\n    PP = Phi.T @ Phi + (sigma/alpha)**2 * np.eye(D)\n    theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n    \n    return theta_map\n\n\n# define the function we wish to estimate later\ndef g(x, sigma):\n    p = np.hstack([x**0, x**1, np.sin(x)])\n    w = np.array([-1.0, 0.1, 1.0]).reshape(-1,1)\n    return p @ w + sigma*np.random.normal(size=x.shape) \n\n\n# Generate some data\nsigma = 1.0 # noise standard deviation\nalpha = 1.0 # standard deviation of the parameter prior\nN = 20\n\nnp.random.seed(42)\n\nX = (np.random.rand(N)*10.0 - 5.0).reshape(-1,1)\ny = g(X, sigma) # training targets\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.xlabel(\"$x$\")\nplt.ylabel(\"$y$\");\n\n\n\n\n\n# get the MAP estimate\nK = 8 # polynomial degree   \n\n\n# feature matrix\nPhi = poly_features(X, K)\n\ntheta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n# maximum likelihood estimate\ntheta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n\nXtest = np.linspace(-5,5,100).reshape(-1,1)\nytest = g(Xtest, sigma)\n\nPhi_test = poly_features(Xtest, K)\ny_pred_map = Phi_test @ theta_map\n\ny_pred_mle = Phi_test @ theta_ml\n\nplt.figure()\nplt.plot(X, y, '+')\nplt.plot(Xtest, y_pred_map)\nplt.plot(Xtest, g(Xtest, 0))\nplt.plot(Xtest, y_pred_mle)\n\nplt.legend([\"data\", \"map prediction\", \"ground truth function\", \"maximum likelihood\"]);\n\n\n\n\n\nprint(np.hstack([theta_ml, theta_map]))\n\n[[-1.49712990e+00 -1.08154986e+00]\n [ 8.56868912e-01  6.09177023e-01]\n [-1.28335730e-01 -3.62071208e-01]\n [-7.75319509e-02 -3.70531732e-03]\n [ 3.56425467e-02  7.43090617e-02]\n [-4.11626749e-03 -1.03278646e-02]\n [-2.48817783e-03 -4.89363010e-03]\n [ 2.70146690e-04  4.24148554e-04]\n [ 5.35996050e-05  1.03384719e-04]]\n\n\nNow, let us compute the RMSE for different polynomial degrees and see whether the MAP estimate addresses the overfitting issue we encountered with the maximum likelihood estimate.\n\n## EDIT THIS CELL\n\nK_max = 12 # this is the maximum degree of polynomial we will consider\nassert(K_max &lt; N) # this is the latest point when we'll run into numerical problems\n\nrmse_mle = np.zeros((K_max+1,))\nrmse_map = np.zeros((K_max+1,))\n\nfor k in range(K_max+1):\n   \n    \n    # feature matrix\n    Phi = poly_features(X, k)\n    \n    # maximum likelihood estimate\n    theta_ml = nonlinear_features_maximum_likelihood(Phi, y)\n    \n    # predict the function values at the test input locations (maximum likelihood)\n    y_pred_test = 0*Xtest ## &lt;--- EDIT THIS LINE\n      \n    ####################### SOLUTION\n    # feature matrix for test inputs\n    Phi_test = poly_features(Xtest, k)\n    \n    # prediction\n    ypred_test_mle = Phi_test @ theta_ml\n    #######################\n    \n    # RMSE on test set (maximum likelihood)\n    rmse_mle[k] = RMSE(ytest, ypred_test_mle)\n    \n    # MAP estimate\n    theta_map = map_estimate_poly(Phi, y, sigma, alpha)\n\n    # Feature matrix\n    Phi_test = poly_features(Xtest, k)\n    \n    # predict the function values at the test input locations (MAP)\n    ypred_test_map = Phi_test @ theta_map\n    \n    # RMSE on test set (MAP)\n    rmse_map[k] = RMSE(ytest, ypred_test_map)\n    \n\nplt.figure()\nplt.semilogy(rmse_mle) # this plots the RMSE on a logarithmic scale\nplt.semilogy(rmse_map) # this plots the RMSE on a logarithmic scale\nplt.xlabel(\"degree of polynomial\")\nplt.ylabel(\"RMSE\")\nplt.legend([\"Maximum likelihood\", \"MAP\"])\n\nC:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_30576\\3627804172.py:13: LinAlgWarning: Ill-conditioned matrix (rcond=1.82839e-17): result may not be accurate.\n  theta_map = scipy.linalg.solve(PP, Phi.T @ y)\n\n\n&lt;matplotlib.legend.Legend at 0x14fbafd0f10&gt;\n\n\n\n\n\n\n\n\nWhat do you observe?\nWhat is the influence of the prior variance on the parameters (\\(\\alpha^2\\))? Change the parameter and describe what happens."
  },
  {
    "objectID": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "href": "blogs/blogsData/tutorial_linear_regressionsolution.html#bayesian-linear-regression",
    "title": "Linear Regression Tutorial",
    "section": "",
    "text": "# Test inputs\nNtest = 200\nXtest = np.linspace(-5, 5, Ntest).reshape(-1,1) # test inputs\n\nprior_var = 2.0 # variance of the parameter prior (alpha^2). We assume this is known.\nnoise_var = 1.0 # noise variance (sigma^2). We assume this is known.\n\npol_deg = 3 # degree of the polynomial we consider at the moment\n\nAssume a parameter prior \\(p(\\boldsymbol\\theta) = \\mathcal N (\\boldsymbol 0, \\alpha^2\\boldsymbol I)\\). For every test input \\(\\boldsymbol x_*\\) we obtain the prior mean \\[\nE[f(\\boldsymbol x_*)] = 0\n\\] and the prior (marginal) variance (ignoring the noise contribution) \\[\nV[f(\\boldsymbol x_*)] = \\alpha^2\\boldsymbol\\phi(\\boldsymbol x_*) \\boldsymbol\\phi(\\boldsymbol x_*)^\\top\n\\] where \\(\\boldsymbol\\phi(\\cdot)\\) is the feature map.\n\n## EDIT THIS CELL\n\n# compute the feature matrix for the test inputs\nPhi_test = poly_features(Xtest, pol_deg) # N x (pol_deg+1) feature matrix SOLUTION\n\n# compute the (marginal) prior at the test input locations\n# prior mean\nprior_mean = np.zeros((Ntest,1)) # prior mean &lt;-- SOLUTION\n\n# prior variance\nfull_covariance = Phi_test @ Phi_test.T * prior_var # N x N covariance matrix of all function values\nprior_marginal_var =  np.diag(full_covariance)\n\n# Let us visualize the prior over functions\nplt.figure()\nplt.plot(Xtest, prior_mean, color=\"k\")\n\nconf_bound1 = np.sqrt(prior_marginal_var).flatten()\nconf_bound2 = 2.0*np.sqrt(prior_marginal_var).flatten()\nconf_bound3 = 2.0*np.sqrt(prior_marginal_var + noise_var).flatten()\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound1, \n             prior_mean.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound2, \n                 prior_mean.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), prior_mean.flatten() + conf_bound3, \n                 prior_mean.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\n\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title(\"Prior over functions\");\n\n\n\n\nNow, we will use this prior distribution and sample functions from it.\n\n## EDIT THIS CELL\n\n# samples from the prior\nnum_samples = 10\n\n# We first need to generate random weights theta_i, which we sample from the parameter prior\nrandom_weights = np.random.normal(size=(pol_deg+1,num_samples), scale=np.sqrt(prior_var))\n\n# Now, we compute the induced random functions, evaluated at the test input locations\n# Every function sample is given as f_i = Phi * theta_i, \n# where theta_i is a sample from the parameter prior\n\nsample_function = Phi_test @ random_weights # &lt;-- SOLUTION\n\nplt.figure()\nplt.plot(Xtest, sample_function, color=\"r\")\nplt.title(\"Plausible functions under the prior\")\nprint(\"Every sampled function is a polynomial of degree \"+str(pol_deg));\n\nEvery sampled function is a polynomial of degree 3\n\n\n\n\n\nNow we are given some training inputs \\(\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N\\), which we collect in a matrix \\(\\boldsymbol X = [\\boldsymbol x_1, \\dotsc, \\boldsymbol x_N]^\\top\\in\\mathbb{R}^{N\\times D}\\)\n\nN = 10\nX = np.random.uniform(high=5, low=-5, size=(N,1)) # training inputs, size Nx1\ny = g(X, np.sqrt(noise_var)) # training targets, size Nx1\n\nNow, let us compute the posterior\n\n## EDIT THIS FUNCTION\n\ndef polyfit(X, y, K, prior_var, noise_var):\n    # X: training inputs, size N x D\n    # y: training targets, size N x 1\n    # K: degree of polynomial we consider\n    # prior_var: prior variance of the parameter distribution\n    # sigma: noise variance\n    \n    jitter = 1e-08 # increases numerical stability\n    \n    Phi = poly_features(X, K) # N x (K+1) feature matrix \n    \n    # Compute maximum likelihood estimate\n    Pt = Phi.T @ y # Phi*y, size (K+1,1)\n    PP = Phi.T @ Phi + jitter*np.eye(K+1) # size (K+1, K+1)\n    C = scipy.linalg.cho_factor(PP)\n    # maximum likelihood estimate\n    theta_ml = scipy.linalg.cho_solve(C, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n#     theta_ml = scipy.linalg.solve(PP, Pt) # inv(Phi^T*Phi)*Phi^T*y, size (K+1,1)\n    \n    # MAP estimate\n    theta_map = scipy.linalg.solve(PP + noise_var/prior_var*np.eye(K+1), Pt)\n    \n    # parameter posterior\n    iSN = (np.eye(K+1)/prior_var + PP/noise_var) # posterior precision\n    SN = scipy.linalg.pinv(noise_var*np.eye(K+1)/prior_var + PP)*noise_var  # posterior covariance\n    mN = scipy.linalg.solve(iSN, Pt/noise_var) # posterior mean\n    \n    return (theta_ml, theta_map, mN, SN)\n\n\ntheta_ml, theta_map, theta_mean, theta_var = polyfit(X, y, pol_deg, alpha, sigma)\n\n\nprint(theta_mean, theta_var)\n\n[[-0.59357667]\n [ 0.41955968]\n [ 0.01927393]\n [-0.02591532]] [[ 0.31686871 -0.05423782 -0.03675352  0.0068937 ]\n [-0.05423782  0.05899309  0.00762815 -0.00430896]\n [-0.03675352  0.00762815  0.00680258 -0.00137103]\n [ 0.0068937  -0.00430896 -0.00137103  0.00049154]]\n\n\nNow, let’s make predictions (ignoring the measurement noise). We obtain three predictors: \\[\\begin{align}\n&\\text{Maximum likelihood: }E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{ml}\\\\\n&\\text{Maximum a posteriori: } E[f(\\boldsymbol X_{\\text{test}})] = \\boldsymbol \\phi(X_{\\text{test}})\\boldsymbol \\theta_{map}\\\\\n&\\text{Bayesian: } p(f(\\boldsymbol X_{\\text{test}})) = \\mathcal N(f(\\boldsymbol X_{\\text{test}}) \\,|\\, \\boldsymbol \\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{mean}},\\, \\boldsymbol\\phi(X_{\\text{test}}) \\boldsymbol\\theta_{\\text{var}}  \\boldsymbol\\phi(X_{\\text{test}})^\\top)\n\\end{align}\\] We already computed all quantities. Write some code that implements all three predictors.\n\n## EDIT THIS CELL\n\n# predictions (ignoring the measurement/observations noise)\n\nPhi_test = poly_features(Xtest, pol_deg) # N x (K+1)\n\n# maximum likelihood predictions (just the mean)\nm_mle_test = Phi_test @ theta_ml\n\n# MAP predictions (just the mean)\nm_map_test = Phi_test @ theta_map\n\n# predictive distribution (Bayesian linear regression)\n# mean prediction\nmean_blr = Phi_test @ theta_mean\n# variance prediction\ncov_blr =  Phi_test @ theta_var @ Phi_test.T\n\n\nprint(Xtest.shape, Phi_test.shape)\n\n(200, 1) (200, 4)\n\n\n\nprint(mean_blr.shape, cov_blr.shape)\n\n(200, 1) (200, 200)\n\n\n\n# plot the posterior\nplt.figure()\nplt.plot(X, y, \"+\")\nplt.plot(Xtest, m_mle_test)\nplt.plot(Xtest, m_map_test)\nvar_blr = np.diag(cov_blr)\nconf_bound1 = np.sqrt(var_blr).flatten()\nconf_bound2 = 2.0*np.sqrt(var_blr).flatten()\nconf_bound3 = 2.0*np.sqrt(var_blr + sigma).flatten()\n\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound1, \n                 mean_blr.flatten() - conf_bound1, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound2, \n                 mean_blr.flatten() - conf_bound2, alpha = 0.1, color=\"k\")\nplt.fill_between(Xtest.flatten(), mean_blr.flatten() + conf_bound3, \n                 mean_blr.flatten() - conf_bound3, alpha = 0.1, color=\"k\")\nplt.legend([\"Training data\", \"MLE\", \"MAP\", \"BLR\"])\nplt.xlabel('$x$');\nplt.ylabel('$y$');"
  },
  {
    "objectID": "blogs/blogsData/Untitled-1.html",
    "href": "blogs/blogsData/Untitled-1.html",
    "title": "Demo",
    "section": "",
    "text": "print('hello')\n\nhello"
  },
  {
    "objectID": "blogs/index.html",
    "href": "blogs/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 7, 2023\n\n\nDemo\n\n\nSuraj Jaiswal\n\n\n\n\nJun 6, 2023\n\n\nLinear Regression Tutorial\n\n\nSuraj Jaiswal\n\n\n\n\nJun 5, 2023\n\n\nBaysian Linear Regression\n\n\nSuraj Jaiswal\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Suraj Jaiswal",
    "section": "",
    "text": "Welcome to my blog! I’m Suraj Jaiswal, an M.Tech student at IIT Gandhinagar, pursuing Computer Science and Engineering under the mentorship of Dr. Nipun Batra.  Here, I’ll share my experiences, research projects, and insights as we delve into the exciting world of technology. IIT Gandhinagar, known for its excellence in education and research, provides the ideal environment for me to explore the limitless possibilities of computer science."
  }
]