<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Suraj Jaiswal">
<meta name="dcterms.date" content="2023-06-18">

<title>Suraj Jaiswal - Baysian Linear Regression blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Suraj Jaiswal</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blogs/index.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-uncertainity" id="toc-what-is-uncertainity" class="nav-link active" data-scroll-target="#what-is-uncertainity">What is Uncertainity?</a></li>
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood" id="toc-maximum-likelihood" class="nav-link" data-scroll-target="#maximum-likelihood">Maximum Likelihood</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression">Polynomial Regression</a>
  <ul class="collapse">
  <li><a href="#nonlinear-features" id="toc-nonlinear-features" class="nav-link" data-scroll-target="#nonlinear-features">Nonlinear Features</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#bayesian-linear-regression" id="toc-bayesian-linear-regression" class="nav-link" data-scroll-target="#bayesian-linear-regression">Bayesian Linear Regression</a>
  <ul class="collapse">
  <li><a href="#bayes-theroem" id="toc-bayes-theroem" class="nav-link" data-scroll-target="#bayes-theroem">Bayes theroem</a></li>
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference">Bayesian inference</a></li>
  <li><a href="#bayeisan-approch" id="toc-bayeisan-approch" class="nav-link" data-scroll-target="#bayeisan-approch">Bayeisan approch</a></li>
  <li><a href="#known-entities" id="toc-known-entities" class="nav-link" data-scroll-target="#known-entities">Known entities</a></li>
  <li><a href="#posterior" id="toc-posterior" class="nav-link" data-scroll-target="#posterior">Posterior</a>
  <ul class="collapse">
  <li><a href="#parameter-posteriori-in-closed-form" id="toc-parameter-posteriori-in-closed-form" class="nav-link" data-scroll-target="#parameter-posteriori-in-closed-form">Parameter posteriori in closed form</a></li>
  <li><a href="#posterior-predictive-distribution" id="toc-posterior-predictive-distribution" class="nav-link" data-scroll-target="#posterior-predictive-distribution">Posterior Predictive distribution</a></li>
  <li><a href="#visulizing-the-parameter-posterior" id="toc-visulizing-the-parameter-posterior" class="nav-link" data-scroll-target="#visulizing-the-parameter-posterior">Visulizing the parameter Posterior</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Baysian Linear Regression blog</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Suraj Jaiswal </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 18, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Welcome to my blog on Bayesian linear regression, where we explore the power of this technique. While traditional linear regression provides point estimates, Bayesian linear regression incorporates prior knowledge and quantifies uncertainty. By combining observed data with prior beliefs, we make more informed decisions. Throughout this blog, we’ll delve into key components like probalistic approch to linear regression, basics of types of uncertainity, prior distributions, likelihood functions, and posterior inference. Let’s embark on this enlightening journey together.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="blr_blog_files/figure-html/815967ef-1-stocksimg.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">stocksimg.jpg</figcaption>
</figure>
</div>
<section id="what-is-uncertainity" class="level1">
<h1>What is Uncertainity?</h1>
<p>In nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. It provides us valuable information about trustworthiness and reliability of model predictions. - There are two types of uncertainty :</p>
<ol type="1">
<li>Aleatoric uncertainty, which is the uncertainty that arises from the data. It is inherited in data and can’t be reduced as it is caused by data generation process.</li>
<li>Epistemic uncertainty, which is the uncertainty that arises from the model(here regression model). We usually predict the uncertainty in data (aleatoric) still be uncertain about the uncertainity based on model parameters(epistemic).</li>
</ol>
<ul>
<li>This is exaclty why we need to learn bayesian approch. It provides a way to learn uncertainty. We will see basic bayeian approch in linear regression called bayesian linear regression. -But lets first discuss implementing linear regression before diving into the Bayesian linear regression.</li>
</ul>
</section>
<section id="linear-regression" class="level1">
<h1>Linear Regression</h1>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="blr_blog_files/figure-html/ea813c97-1-LR.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">IMG src medium article</figcaption>
</figure>
</div>
<p>Linear regression is about finding a linear model that best fit a given dataset. Here we will discuss probabilistic interpretation of linear regression</p>
<p>We make two assumptions. 1. The observed value of y for a given data point x is sum of predicted value and error term of the form <span class="math display">\[
y = \boldsymbol x^T\boldsymbol\theta + \epsilon\, ,\quad \boldsymbol\theta = \begin{bmatrix}\theta_0\\
\boldsymbol\theta_1
\end{bmatrix}\,.
\]</span> where <span class="math inline">\(\boldsymbol x\in\mathbb{R}^D\)</span> are inputs and <span class="math inline">\(y\in\mathbb{R}\)</span> are noisy observations. The parameter vector <span class="math inline">\(\boldsymbol\theta\in\mathbb{R}^D\)</span> parametrizes the function.</p>
<ol start="2" type="1">
<li>The error term is independently and identically distributed and is Normally distributed with mean 0 and variance sigma squared. <span class="math display">\[\quad \epsilon \sim \mathcal N(0, \sigma^2)\]</span></li>
</ol>
<p>We assume we have a training set <span class="math inline">\((\boldsymbol x_n, y_n)\)</span>, <span class="math inline">\(n=1,\ldots, N\)</span>. We summarize the sets of training inputs in <span class="math inline">\(\mathcal X = \{\boldsymbol x_1, \ldots, \boldsymbol x_N\}\)</span> and corresponding training targets <span class="math inline">\(\mathcal Y = \{y_1, \ldots, y_N\}\)</span>, respectively.</p>
<p>Here, we are interested in finding good parameters <span class="math inline">\(\boldsymbol\theta\)</span>.</p>
<p><span class="math display">\[
\begin{array}{l}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta \ \ \ \ \ \ \ \ \ \sigma \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \downarrow \ \ \ \swarrow \\
x_{n} \ \ \ \rightarrow \ \ y_{n} \ \ \\
n\ =\ 1,......,N\ \\
\\
\end{array}
\]</span> <span class="math display">\[ Probabilistic \ graphical \ model \ for \ linear \ regression \]</span></p>
<p>Let’s first see the demo dataset</p>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.linalg</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal, norm</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> seed, uniform, randn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> inv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define training set</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># 5x1 vector, N=5, D=1</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">1.2</span>, <span class="op">-</span><span class="fl">0.7</span>, <span class="fl">0.14</span>, <span class="fl">0.67</span>, <span class="fl">1.67</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># 5x1 vector</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y <span class="op">+</span> <span class="fl">2.0</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training set</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="maximum-likelihood" class="level2">
<h2 class="anchored" data-anchor-id="maximum-likelihood">Maximum Likelihood</h2>
<p>We will start with maximum likelihood estimation of the parameters <span class="math inline">\(\boldsymbol\theta\)</span>. In maximum likelihood estimation, we find the parameters <span class="math inline">\(\boldsymbol\theta^{\mathrm{ML}}\)</span> that maximize the likelihood <span class="math display">\[
p(\mathcal Y | \mathcal X, \boldsymbol\theta) = \prod_{n=1}^N p(y_n | \boldsymbol x_n, \boldsymbol\theta)\,.
\]</span> From the lecture we know that the maximum likelihood estimator is given by <span class="math display">\[
\boldsymbol\theta^{\text{ML}} = (\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol y\in\mathbb{R}^D\,,
\]</span> where <span class="math display">\[
\boldsymbol X = [\boldsymbol x_1, \ldots, \boldsymbol x_N]^T\in\mathbb{R}^{N\times D}\,,\quad \boldsymbol y = [y_1, \ldots, y_N]^T \in\mathbb{R}^N\,.
\]</span></p>
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>N, D <span class="op">=</span> X.shape</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>X_aug <span class="op">=</span> np.hstack([np.ones((N,<span class="dv">1</span>)), X])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> max_lik_estimate(X, y):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># X: N x D matrix of training inputs</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y: N x 1 vector of training targets/observations</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># returns: maximum likelihood parameters (D x 1)</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    N, D <span class="op">=</span> X.shape</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    theta_ml <span class="op">=</span> np.linalg.solve(X.T <span class="op">@</span> X, X.T <span class="op">@</span> y) <span class="co">## &lt;-- SOLUTION</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_ml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>theta_ml <span class="op">=</span> max_lik_estimate(X_aug,y)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta_ml)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[2.116]
 [0.499]]</code></pre>
</div>
</div>
<p>Now we will make predictions at test inputs <span class="math inline">\(\boldsymbol X_{\text{test}}\in\mathbb{R}\)</span>, <span class="math display">\[
\ \boldsymbol y_{\text{pred}} = \boldsymbol \Phi_{\text{test}}\boldsymbol\theta^{\text{ML}}
\]</span></p>
<div class="cell" data-execution_count="105">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define a test set</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>Xtest <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># 100 x 1 vector of test inputs</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>Xtest_aug <span class="op">=</span> np.hstack([np.ones((Xtest.shape[<span class="dv">0</span>],<span class="dv">1</span>)), Xtest]) <span class="co"># 100 x (D + 1) vector of test inputs</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>ml_prediction <span class="op">=</span> Xtest_aug <span class="op">@</span> theta_ml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="106">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.plot(Xtest, ml_prediction)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="106">
<pre><code>Text(0, 0.5, '$y$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This gives fairly good results but what if the data is bit complex</p>
<p>Let us have a look at an example where the observations clearly do not lie on a straight line:</p>
<div class="cell" data-execution_count="107">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.2</span><span class="op">**</span><span class="dv">2</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>seed(<span class="dv">10</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>xn <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, N)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>epsilon <span class="op">=</span> np.random.normal(mu, sigma, N)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>yn <span class="op">=</span> <span class="op">-</span>np.sin(xn<span class="op">/</span><span class="dv">5</span>) <span class="op">+</span> np.cos(xn) <span class="op">+</span> epsilon</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> np.column_stack((xn, yn))</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>xn <span class="op">=</span> xn.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>yn <span class="op">=</span> yn.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="108">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training set</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plt.plot(xn, yn, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="108">
<pre><code>(-5.0, 5.0)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Lets first apply linear regressoin without non linear transformation</p>
<div class="cell" data-execution_count="109">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>N, D <span class="op">=</span> xn.shape</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>X_aug <span class="op">=</span> np.hstack([np.ones((N,<span class="dv">1</span>)), xn]) <span class="co"># augmented training inputs of size N x (D+1)</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># theta_aug = np.zeros((D+1, 1)) # new theta vector of size (D+1) x 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="110">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>theta_aug_ml <span class="op">=</span> max_lik_estimate(X_aug, yn)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>theta_aug_ml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="110">
<pre><code>array([[-0.47109666],
       [-0.1808517 ]])</code></pre>
</div>
</div>
<div class="cell" data-execution_count="111">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>ml_predictions <span class="op">=</span> X_aug <span class="op">@</span> theta_aug_ml </span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># X: K x D matrix of test inputs</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># theta: D x 1 vector of parameters</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># returns: prediction of f(X); K x 1 vector</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="112">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training set</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plt.plot(xn, yn, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>plt.plot(xn, ml_predictions)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>(-5.0, 5.0)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-14-output-2.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="polynomial-regression" class="level2">
<h2 class="anchored" data-anchor-id="polynomial-regression">Polynomial Regression</h2>
<section id="nonlinear-features" class="level3">
<h3 class="anchored" data-anchor-id="nonlinear-features">Nonlinear Features</h3>
<p>So far, we have looked at linear regression with linear features. This allowed us to fit straight lines. However, linear regression also allows us to fit functions that are nonlinear in the inputs <span class="math inline">\(\boldsymbol x\)</span>, as long as the parameters <span class="math inline">\(\boldsymbol\theta\)</span> appear linearly. This means, we can learn functions of the form <span class="math display">\[
f(\boldsymbol x, \boldsymbol\theta) = \sum_{k = 1}^K \theta_k \phi_k(\boldsymbol x)\,,
\]</span> where the features <span class="math inline">\(\phi_k(\boldsymbol x)\)</span> are (possibly nonlinear) transformations of the inputs <span class="math inline">\(\boldsymbol x\)</span>.</p>
<p>Polynomial Regression class of functions that is covered by linear regression is the family of polynomials because we can write a polynomial of degree <span class="math inline">\(K\)</span> as <span class="math display">\[
\sum_{k=0}^K \theta_k x^k = \boldsymbol \phi(x)^T\boldsymbol\theta\,,\quad
\boldsymbol\phi(x)=
\begin{bmatrix}
x^0\\
x^1\\
\vdots\\
x^K
\end{bmatrix}\in\mathbb{R}^{K+1}\,.
\]</span> Here, <span class="math inline">\(\boldsymbol\phi(x)\)</span> is a nonlinear feature transformation of the inputs <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Similar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: <span class="math display">\[
\boldsymbol\Phi = \begin{bmatrix}
\boldsymbol\phi(x_1) &amp; \boldsymbol\phi(x_2) &amp; \cdots &amp; \boldsymbol\phi(x_n)
\end{bmatrix}^T \in\mathbb{R}^{N\times K+1}
\]</span></p>
<p>Let us start by computing the feature matrix <span class="math inline">\(\boldsymbol \Phi\)</span></p>
<div class="cell" data-execution_count="113">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> poly_features(X, p):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns a matrix with p columns containing the polynomial features of the input vector X."""</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.flatten()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([<span class="fl">1.0</span><span class="op">*</span>X<span class="op">**</span>i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(p<span class="op">+</span><span class="dv">1</span>)]).T</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this feature matrix we get the maximum likelihood estimator as <span class="math display">\[
\boldsymbol \theta^\text{ML} = (\boldsymbol\Phi^T\boldsymbol\Phi)^{-1}\boldsymbol\Phi^T\boldsymbol y
\]</span> For reasons of numerical stability, we often add a small diagonal “jitter” <span class="math inline">\(\kappa&gt;0\)</span> to <span class="math inline">\(\boldsymbol\Phi^T\boldsymbol\Phi\)</span> so that we can invert the matrix without significant problems so that the maximum likelihood estimate becomes <span class="math display">\[
\boldsymbol \theta^\text{ML} = (\boldsymbol\Phi^T\boldsymbol\Phi + \kappa\boldsymbol I)^{-1}\boldsymbol\Phi^T\boldsymbol y
\]</span></p>
<div class="cell" data-execution_count="114">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nonlinear_features_maximum_likelihood(Phi, y):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Phi: features matrix for training inputs. Size of N x D</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># y: training targets. Size of N by 1</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># returns: maximum likelihood estimator theta_ml. Size of D x 1</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    kappa <span class="op">=</span> <span class="fl">1e-08</span> <span class="co"># 'jitter' term; good for numerical stability</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    D <span class="op">=</span> Phi.shape[<span class="dv">1</span>]  </span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># maximum likelihood estimate</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    Pt <span class="op">=</span> Phi.T <span class="op">@</span> y <span class="co"># Phi^T*y</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    PP <span class="op">=</span> Phi.T <span class="op">@</span> Phi <span class="op">+</span> kappa<span class="op">*</span>np.eye(D) <span class="co"># Phi^T*Phi + kappa*I</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># maximum likelihood estimate</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> scipy.linalg.cho_factor(PP)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    theta_ml <span class="op">=</span> scipy.linalg.cho_solve(C, Pt) <span class="co"># inv(Phi^T*Phi)*Phi^T*y </span></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_ml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To make predictions at test inputs <span class="math inline">\(\boldsymbol X_{\text{test}}\in\mathbb{R}\)</span>, we need to compute the features (nonlinear transformations) <span class="math inline">\(\boldsymbol\Phi_{\text{test}}= \boldsymbol\phi(\boldsymbol X_{\text{test}})\)</span> of <span class="math inline">\(\boldsymbol X_{\text{test}}\)</span> to give us the predicted mean <span class="math display">\[
\mathbb{E}[\boldsymbol y_{\text{test}}] = \boldsymbol \Phi_{\text{test}}\boldsymbol\theta^{\text{ML}}
\]</span></p>
<div class="cell" data-execution_count="115">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> poly_features(xn, p)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>theta_ml <span class="op">=</span> nonlinear_features_maximum_likelihood(Phi, yn)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>Phi_test <span class="op">=</span>  poly_features(X_test, p)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> Phi_test <span class="op">@</span> theta_ml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="116">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training set</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>plt.plot(xn, yn, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>plt.plot(X_test, y_pred)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"$y$"</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="116">
<pre><code>(-5.0, 5.0)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-18-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Now lets try different polynomial fits.</p>
<div class="cell" data-execution_count="117">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Values of p to consider</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>p_values <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">9</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a 2x3 grid of subplots</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, p <span class="kw">in</span> <span class="bu">enumerate</span>(p_values):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> poly_features(xn, p)</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    theta_ml <span class="op">=</span> nonlinear_features_maximum_likelihood(Phi, yn)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    Phi_test <span class="op">=</span> poly_features(X_test, p)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> Phi_test <span class="op">@</span> theta_ml</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axs[i <span class="op">//</span> <span class="dv">3</span>, i <span class="op">%</span> <span class="dv">3</span>]  <span class="co"># Get the correct subplot</span></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    ax.plot(xn, yn, <span class="st">'+'</span>, markersize<span class="op">=</span><span class="dv">10</span>,label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_test, y_pred, label <span class="op">=</span> <span class="st">'MLE'</span>)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"$y$"</span>)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"P = </span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Adjust the spacing between subplots</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the plot</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>You can refer 9.1 and 9.2 section of <a href="https://mml-book.github.io/book/mml-book.pdf">Mathematics for Machine Learning</a> to understand in depth about probalistic approch to linear regression.</p>
</section>
</section>
</section>
<section id="bayesian-linear-regression" class="level1">
<h1>Bayesian Linear Regression</h1>
<p>Bayesian linear regression is the Bayesian interpretation of linear regression.</p>
<p>So far we computed point estimates of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function.</p>
<p>In the Bayesian approach we don’t work with exact values but with probabilities. This allows us to model the uncertainty in our parameter estimates.</p>
<p>In nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty.</p>
<section id="bayes-theroem" class="level2">
<h2 class="anchored" data-anchor-id="bayes-theroem">Bayes theroem</h2>
<p>The basis of bayesian linear regression is bayes theroem. - Bayes’ theorem looks as follows: <span class="math display">\[
\begin{equation}
p(\boldsymbol{\theta} | \mathbf{x}, y) = \frac{p(y | \boldsymbol{x}, \boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{x}, y)}
\end{equation}
\]</span> - <span class="math inline">\(p(y | \boldsymbol{x}, \boldsymbol{\theta})\)</span> is the <em>likelihood</em>. It describes the probability of the target values given the data and parameters. - <span class="math inline">\(p(\boldsymbol{\theta})\)</span> is the <em>prior</em>. It describes our initial knowledge about which parameter values are likely and unlikely. - <span class="math inline">\(p(\boldsymbol{x}, y)\)</span> is the <em>evidence</em>. It describes the joint probability of the data and targets.</p>
</section>
<section id="bayesian-inference" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-inference">Bayesian inference</h2>
<p>In general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis <span class="math inline">\(p(h)\)</span> 2. We observe some data, representating new evidence <span class="math inline">\(e\)</span> 3. We use Bayes’ theorem to update our belief given the new evidence: <span class="math inline">\(p(h|e) = \frac{p(e |h)p(h)}{p(e)}\)</span></p>
<p>Have a look at <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Wiki</a></p>
</section>
<section id="bayeisan-approch" class="level2">
<h2 class="anchored" data-anchor-id="bayeisan-approch">Bayeisan approch</h2>
<p>Unlike linear regression where we computed point estimates of our parameters using maximum likelihood approach and make predictions, here in Bayesian linear regression we estimate</p>
<p>Following are the steps: 1. We assume a that we know standard deviation of the noise, mean and covariance of the prior. 2. We than calculate parameter posteriori 3. Based on that we make posteriori predictions on unseen data ie. test data.</p>
<p>Now lets see along with code</p>
<p>Here we have same assumptions that we took in linear regression <span class="math display">\[
y = \boldsymbol x^T\boldsymbol\theta + \epsilon\,,\quad \epsilon \sim \mathcal N(0, \sigma^2)
\]</span> Where epsilon is the noise from normal distribution with variance <span class="math inline">\(\sigma^2\)</span>. Training inputs in <span class="math inline">\(\mathcal X = \{\boldsymbol x_1, \ldots, \boldsymbol x_N\}\)</span> and corresponding training targets <span class="math inline">\(\mathcal Y = \{y_1, \ldots, y_N\}\)</span>, respectively.</p>
<p>Function</p>
<div class="cell" data-execution_count="118">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> g(x, mu, sigma):   </span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    epsilon <span class="op">=</span> np.random.normal(mu, sigma, size<span class="op">=</span>(x.shape))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.cos(x) <span class="op">+</span> epsilon</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We apply non linear feature transformation on feature matrix with polynomial of degree <span class="math inline">\(K\)</span> <span class="math display">\[
\sum_{k=0}^K \theta_k x^k = \boldsymbol \phi(x)^T\boldsymbol\theta\,,\quad
\boldsymbol\phi(x)=
\begin{bmatrix}
x^0\\
x^1\\
\vdots\\
x^K
\end{bmatrix}\in\mathbb{R}^{K+1}\,.
\]</span> Here, <span class="math inline">\(\boldsymbol\phi(x)\)</span> is a nonlinear feature transformation of the inputs <span class="math inline">\(x\in\mathbb{R}\)</span>.</p>
<p>Similar to the earlier case we can define a matrix that collects all the feature transformations of the training inputs: <span class="math display">\[
\boldsymbol\Phi = \begin{bmatrix}
\boldsymbol\phi(x_1) &amp; \boldsymbol\phi(x_2) &amp; \cdots &amp; \boldsymbol\phi(x_n)
\end{bmatrix}^T \in\mathbb{R}^{N\times K+1}
\]</span></p>
<p>Sample to see nonlinear transformation</p>
<div class="cell" data-execution_count="119">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">3</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># 5x1 vector, N=5, D=1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="120">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>poly_features(X, <span class="dv">3</span>) <span class="co"># defined in linear regression section</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="120">
<pre><code>array([[  1.,  -3.,   9., -27.],
       [  1.,  -1.,   1.,  -1.],
       [  1.,   0.,   0.,   0.],
       [  1.,   1.,   1.,   1.],
       [  1.,   3.,   9.,  27.]])</code></pre>
</div>
</div>
</section>
<section id="known-entities" class="level2">
<h2 class="anchored" data-anchor-id="known-entities">Known entities</h2>
<div class="cell" data-execution_count="121">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># standard deviation of the noise</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>m0 <span class="op">=</span> <span class="fl">0.0</span> <span class="co"># mean of the prior</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>S0 <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># covariance of the prior  </span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">6</span> <span class="co"># order of the polynomial </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><span class="math display">\[
\begin{array}{l}
\ \ \ \ \ \ \ \ \ m_{0} \ \ \ \ \ \ \ \ \ \ \ \ S_{0}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \searrow \ \ \ \swarrow \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \theta \ \ \ \ \ \ \ \ \ \ \ \sigma \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \downarrow \ \ \ \swarrow \\
x_{n} \ \ \rightarrow \ \ \ \ \ \ y_{n} \ \ \\
n\ =\ 1,......,N\ \\
\end{array}
\]</span> <span class="math display">\[ Graphical \ model \ for \ Bayeisan \ linear \ regression \]</span></p>
<div class="cell" data-execution_count="122">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span> <span class="co"># number of data points</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(high<span class="op">=</span><span class="dv">5</span>, low<span class="op">=-</span><span class="dv">5</span>, size<span class="op">=</span>(N,<span class="dv">1</span>)) <span class="co"># training inputs, size Nx1</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> g(X, m0, sigma) <span class="co"># training targets, size Nx1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="posterior" class="level2">
<h2 class="anchored" data-anchor-id="posterior">Posterior</h2>
<section id="parameter-posteriori-in-closed-form" class="level3">
<h3 class="anchored" data-anchor-id="parameter-posteriori-in-closed-form">Parameter posteriori in closed form</h3>
<p>Calculating Parameter posterior: <span class="math display">\[
\begin{aligned}
p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) &amp;=\mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \\
\boldsymbol{S}_{N} &amp;=\left(\boldsymbol{S}_{0}^{-1}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}\right)^{-1} \\
\boldsymbol{m}_{N} &amp;=\boldsymbol{S}_{N}\left(\boldsymbol{S}_{0}^{-1} \boldsymbol{m}_{0}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{y}\right)
\end{aligned}
\]</span></p>
<div class="cell" data-execution_count="123">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior(X, y, p, m0, S0, sigma):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns the posterior mean and covariance matrix of the weights given the training data."""</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    poly_X <span class="op">=</span> poly_features(X, p)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    SN <span class="op">=</span> scipy.linalg.inv(<span class="fl">1.0</span> <span class="op">*</span> np.eye(p<span class="op">+</span><span class="dv">1</span>) <span class="op">/</span> S0  <span class="op">+</span> <span class="fl">1.0</span><span class="op">/</span>sigma<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> poly_X.T <span class="op">@</span> poly_X)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    mN <span class="op">=</span> SN <span class="op">@</span> (m0 <span class="op">/</span> S0 <span class="op">+</span> (<span class="fl">1.0</span><span class="op">/</span>sigma<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> poly_X.T <span class="op">@</span> y)    </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mN, SN</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="124">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>mN , SN <span class="op">=</span> posterior(X, y, p ,m0, S0, sigma)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="125">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>Ntest <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>Xtest <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, Ntest).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="co"># test inputs</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>poly_X_test <span class="op">=</span> poly_features(Xtest, p)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-distribution">Posterior Predictive distribution</h3>
<p>Now, let’s make predictions (ignoring the measurement noise). We obtain three predictors: <span class="math display">\[
\begin{align}
&amp;\text{Bayesian: } p(f(\boldsymbol X_{\text{test}}) \, |\ X, Y, \boldsymbol X_{\text{test}}) = \mathcal N(f(\boldsymbol X_{\text{test}}) \,|\, \boldsymbol \phi(X_{\text{test}}) \boldsymbol M_{\text{n}},\, \boldsymbol\phi(X_{\text{test}}) \boldsymbol S_{\text{N}}  \boldsymbol\phi(X_{\text{test}})^\top + \sigma ^ 2)
\end{align} \]</span> We already computed all quantities. Write some code that implements all three predictors.</p>
<div class="cell" data-execution_count="126">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>posterior_pred_mean <span class="op">=</span> poly_X_test <span class="op">@</span> mN</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>posterior_pred_uncertainty_para <span class="op">=</span> poly_X_test <span class="op">@</span> SN <span class="op">@</span> poly_X_test.T</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>posterior_pred_var <span class="op">=</span> sigma<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> posterior_pred_uncertainty_para</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="129">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the posterior</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>plt.plot(X, y, <span class="st">"+"</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(Xtest, m_mle_test)</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.plot(Xtest, m_map_test)</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>posterior_pred_mean <span class="op">=</span> posterior_pred_mean.flatten()</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>var_blr <span class="op">=</span> np.diag(posterior_pred_uncertainty_para)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># conf_bound1 = np.sqrt(var_blr).flatten()</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># plt.fill_between(Xtest.flatten(), posterior_pred_mean + conf_bound1, posterior_pred_mean - conf_bound1, alpha = 0.1, color="k")</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 95 % parameter uncertainity</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>conf_bound2 <span class="op">=</span> <span class="fl">2.0</span><span class="op">*</span>np.sqrt(var_blr).flatten()</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>plt.fill_between(Xtest.flatten(), posterior_pred_mean <span class="op">+</span> conf_bound2, posterior_pred_mean <span class="op">-</span> conf_bound2, alpha <span class="op">=</span> <span class="fl">0.1</span>, color<span class="op">=</span><span class="st">"r"</span>)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 95 % total uncertainity ie. </span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>conf_bound3 <span class="op">=</span> <span class="fl">2.0</span><span class="op">*</span>np.sqrt(var_blr <span class="op">+</span> sigma<span class="op">**</span><span class="dv">2</span>).flatten()</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>plt.fill_between(Xtest.flatten(), posterior_pred_mean <span class="op">+</span> conf_bound3, posterior_pred_mean <span class="op">-</span> conf_bound3, alpha <span class="op">=</span> <span class="fl">0.1</span>, color<span class="op">=</span><span class="st">"k"</span>)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">"Training data"</span>, <span class="st">'95% para uncertainity'</span>, <span class="st">'95% total uncertainity'</span>])</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$x$'</span>)<span class="op">;</span></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$y$'</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>You can refer 9.3 section of <a href="https://mml-book.github.io/book/mml-book.pdf">Mathematics for Machine Learning</a> to understand in depth about bayesian linear regression.</p>
</section>
<section id="visulizing-the-parameter-posterior" class="level3">
<h3 class="anchored" data-anchor-id="visulizing-the-parameter-posterior">Visulizing the parameter Posterior</h3>
<p>In this section we will visualize the posterior and will see how it changes as it sees more data.</p>
<div class="cell" data-execution_count="136">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, a): <span class="cf">return</span> a[<span class="dv">0</span>] <span class="op">+</span> a[<span class="dv">1</span>] <span class="op">*</span> x</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_prior(m, S, liminf<span class="op">=-</span><span class="dv">1</span>, limsup<span class="op">=</span><span class="dv">1</span>, step<span class="op">=</span><span class="fl">0.05</span>, ax<span class="op">=</span>plt, <span class="op">**</span>kwargs):</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    grid <span class="op">=</span> np.mgrid[liminf:limsup <span class="op">+</span> step:step, liminf:limsup <span class="op">+</span> step:step]</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    nx <span class="op">=</span> grid.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> multivariate_normal.pdf(grid.T.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), mean<span class="op">=</span>m.ravel(), cov<span class="op">=</span>S).reshape(nx, nx).T</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ax.contourf(<span class="op">*</span>grid, z, <span class="op">**</span>kwargs)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_sample_w(mean, cov, size<span class="op">=</span><span class="dv">10</span>, ax<span class="op">=</span>plt):</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    w <span class="op">=</span> np.random.multivariate_normal(mean<span class="op">=</span>mean.ravel(), cov<span class="op">=</span>cov, size<span class="op">=</span>size)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> wi <span class="kw">in</span> w:</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>        ax.plot(x, f(x, wi), c<span class="op">=</span><span class="st">"tab:blue"</span>, alpha<span class="op">=</span><span class="fl">0.4</span>)</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_likelihood_obs(X, T, ix, ax<span class="op">=</span>plt):</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co">    Plot the likelihood function of a single observation</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> np.mgrid[<span class="op">-</span><span class="dv">1</span>:<span class="dv">1</span>:<span class="fl">0.1</span>, <span class="op">-</span><span class="dv">1</span>:<span class="dv">1</span>:<span class="fl">0.1</span>]</span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    x, t <span class="op">=</span> sample_vals(X, T, ix) <span class="co"># ith row</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> W.T.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>) <span class="op">@</span> x.T</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    likelihood <span class="op">=</span> norm.pdf(t, loc<span class="op">=</span>mean, scale<span class="op">=</span> np.sqrt(sigma <span class="op">**</span><span class="dv">2</span>)).reshape(<span class="dv">20</span>, <span class="dv">20</span>).T</span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    ax.contourf(<span class="op">*</span>W, likelihood)</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>    ax.scatter(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, c<span class="op">=</span><span class="st">"white"</span>, marker<span class="op">=</span><span class="st">"+"</span>)</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_vals(X, T, ix):</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Phi: The linear model transormation</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a><span class="co">    t: the target datapoint</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a><span class="co">    return ith data</span></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>    x_in <span class="op">=</span> X[ix]</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a>    Phi <span class="op">=</span> np.c_[np.ones_like(x_in), x_in]</span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> T[[ix]]</span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Phi, t</span>
<span id="cb36-42"><a href="#cb36-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-43"><a href="#cb36-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior_w(phi, t, S0, m0):</span>
<span id="cb36-44"><a href="#cb36-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb36-45"><a href="#cb36-45" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the posterior distribution of </span></span>
<span id="cb36-46"><a href="#cb36-46" aria-hidden="true" tabindex="-1"></a><span class="co">    a Gaussian with known precision and conjugate</span></span>
<span id="cb36-47"><a href="#cb36-47" aria-hidden="true" tabindex="-1"></a><span class="co">    prior a gaussian</span></span>
<span id="cb36-48"><a href="#cb36-48" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb36-49"><a href="#cb36-49" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb36-50"><a href="#cb36-50" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb36-51"><a href="#cb36-51" aria-hidden="true" tabindex="-1"></a><span class="co">    phi: np.array(N, M)</span></span>
<span id="cb36-52"><a href="#cb36-52" aria-hidden="true" tabindex="-1"></a><span class="co">    t: np.array(N, 1)</span></span>
<span id="cb36-53"><a href="#cb36-53" aria-hidden="true" tabindex="-1"></a><span class="co">    S0: np.array(M, M)</span></span>
<span id="cb36-54"><a href="#cb36-54" aria-hidden="true" tabindex="-1"></a><span class="co">        The prior covariance matrix</span></span>
<span id="cb36-55"><a href="#cb36-55" aria-hidden="true" tabindex="-1"></a><span class="co">    m0: np.array(M, 1)</span></span>
<span id="cb36-56"><a href="#cb36-56" aria-hidden="true" tabindex="-1"></a><span class="co">        The prior mean vector</span></span>
<span id="cb36-57"><a href="#cb36-57" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb36-58"><a href="#cb36-58" aria-hidden="true" tabindex="-1"></a>    SN <span class="op">=</span> inv(inv(S0) <span class="op">+</span> ((<span class="dv">1</span> <span class="op">/</span> sigma) <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> phi.T <span class="op">@</span> phi)</span>
<span id="cb36-59"><a href="#cb36-59" aria-hidden="true" tabindex="-1"></a>    mN <span class="op">=</span> SN <span class="op">@</span> (inv(S0) <span class="op">@</span> m0 <span class="op">+</span> ((<span class="dv">1</span> <span class="op">/</span> sigma) <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> phi.T <span class="op">@</span> t)</span>
<span id="cb36-60"><a href="#cb36-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> SN, mN</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="141">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>seed(<span class="dv">314</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> np.array([<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.5</span>]) <span class="co"># true parameter values</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, (N, <span class="dv">1</span>))</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> f(X, a) <span class="op">+</span> randn(N, <span class="dv">1</span>) <span class="op">*</span> sigma</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="142">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># beta = (1 / sigma) ** 2 # precision</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">2.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="143">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>SN <span class="op">=</span> np.eye(<span class="dv">2</span>) <span class="op">/</span> alpha</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>mN <span class="op">=</span> np.zeros((<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>seed(<span class="dv">1643</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="144">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>nobs <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">15</span>, <span class="dv">30</span>]</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>ix_fig <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(<span class="bu">len</span>(nobs) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">12</span>))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>plot_prior(mN, SN, ax<span class="op">=</span>ax[<span class="dv">0</span>,<span class="dv">1</span>])</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">1</span>].scatter(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, c<span class="op">=</span><span class="st">"white"</span>, marker<span class="op">=</span><span class="st">"+"</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>ax[<span class="dv">0</span>, <span class="dv">0</span>].axis(<span class="st">"off"</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>plot_sample_w(mN, SN, ax<span class="op">=</span>ax[<span class="dv">0</span>, <span class="dv">2</span>])</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, N):</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    Phi, t <span class="op">=</span> sample_vals(X, T, i)</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    SN, mN <span class="op">=</span> posterior_w(Phi, t, SN, mN)</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i<span class="op">+</span><span class="dv">1</span> <span class="kw">in</span> nobs:</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        plot_likelihood_obs(X, T, i, ax<span class="op">=</span>ax[ix_fig, <span class="dv">0</span>])</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        plot_prior(mN, SN, ax<span class="op">=</span>ax[ix_fig, <span class="dv">1</span>])</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        ax[ix_fig, <span class="dv">1</span>].scatter(<span class="op">-</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, c<span class="op">=</span><span class="st">"white"</span>, marker<span class="op">=</span><span class="st">"+"</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        ax[ix_fig, <span class="dv">2</span>].scatter(X[:i <span class="op">+</span> <span class="dv">1</span>], T[:i <span class="op">+</span> <span class="dv">1</span>], c<span class="op">=</span><span class="st">"crimson"</span>)</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        ax[ix_fig, <span class="dv">2</span>].set_xlim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        ax[ix_fig, <span class="dv">2</span>].set_ylim(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>            ax[ix_fig, l].set_xlabel(<span class="st">"$w_0$"</span>)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>            ax[ix_fig, l].set_ylabel(<span class="st">"$w_1$"</span>)</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        plot_sample_w(mN, SN, ax<span class="op">=</span>ax[ix_fig, <span class="dv">2</span>])</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        ix_fig <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>titles <span class="op">=</span> [<span class="st">"likelihood"</span>, <span class="st">"prior/posterior"</span>, <span class="st">"data space"</span>]</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> axi, title <span class="kw">in</span> <span class="bu">zip</span>(ax[<span class="dv">0</span>], titles):</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>    axi.set_title(title, size<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="blr_blog_files/figure-html/cell-34-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can see above as the model see more data, the posterior converges close the the true values at end. Refer to <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Bishop - Pattern Recognition and Machine Learning fig 3.7</a> to understand above fig in detail.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>