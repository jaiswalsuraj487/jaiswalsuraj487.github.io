<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Suraj Jaiswal - Bayesian linear regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Suraj Jaiswal</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blogs/index.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#link-to-interactive-demo" id="toc-link-to-interactive-demo" class="nav-link active" data-scroll-target="#link-to-interactive-demo">Link to interactive demo</a></li>
  <li><a href="#what-is-bayesian-linear-regression-blr" id="toc-what-is-bayesian-linear-regression-blr" class="nav-link" data-scroll-target="#what-is-bayesian-linear-regression-blr">1. What is Bayesian linear regression (BLR)? </a><a class="anchor nav-link" id="intro-blr" data-scroll-target="undefined"></a></li>
  <li><a href="#recap-linear-regression" id="toc-recap-linear-regression" class="nav-link" data-scroll-target="#recap-linear-regression">2. Recap linear regression </a><a class="anchor nav-link" id="recap-lr" data-scroll-target="undefined"></a></li>
  <li><a href="#fundamental-concepts" id="toc-fundamental-concepts" class="nav-link" data-scroll-target="#fundamental-concepts">3. Fundamental concepts </a><a class="anchor nav-link" id="fundamental-concepts" data-scroll-target="undefined"></a></li>
  <li><a href="#linear-regression-from-a-probabilistic-perspective" id="toc-linear-regression-from-a-probabilistic-perspective" class="nav-link" data-scroll-target="#linear-regression-from-a-probabilistic-perspective">4. Linear regression from a probabilistic perspective </a><a class="anchor nav-link" id="prob-lr" data-scroll-target="undefined"></a></li>
  <li><a href="#linear-regression-with-basis-functions" id="toc-linear-regression-with-basis-functions" class="nav-link" data-scroll-target="#linear-regression-with-basis-functions">5. Linear regression with basis functions </a><a class="anchor nav-link" id="lr-with-basis-functions" data-scroll-target="undefined"></a>
  <ul class="collapse">
  <li><a href="#example-basis-functions" id="toc-example-basis-functions" class="nav-link" data-scroll-target="#example-basis-functions">5.1 Example basis functions </a><a class="anchor nav-link" id="example-basis-function" data-scroll-target="undefined"></a></li>
  <li><a href="#the-design-matrix" id="toc-the-design-matrix" class="nav-link" data-scroll-target="#the-design-matrix">5.2 The design matrix </a><a class="anchor nav-link" id="design-matrix" data-scroll-target="undefined"></a></li>
  </ul></li>
  <li><a href="#bayesian-linear-regression" id="toc-bayesian-linear-regression" class="nav-link" data-scroll-target="#bayesian-linear-regression">6. Bayesian linear regression </a><a class="anchor nav-link" id="blr" data-scroll-target="undefined"></a>
  <ul class="collapse">
  <li><a href="#step-1-probabilistic-model" id="toc-step-1-probabilistic-model" class="nav-link" data-scroll-target="#step-1-probabilistic-model">6.1 Step 1: Probabilistic model </a><a class="anchor nav-link" id="prob-model" data-scroll-target="undefined"></a></li>
  <li><a href="#generating-a-dataset" id="toc-generating-a-dataset" class="nav-link" data-scroll-target="#generating-a-dataset">6.2 Generating a dataset </a><a class="anchor nav-link" id="dataset" data-scroll-target="undefined"></a></li>
  <li><a href="#step-2-posterior-over-the-parameters" id="toc-step-2-posterior-over-the-parameters" class="nav-link" data-scroll-target="#step-2-posterior-over-the-parameters">6.3 Step 2: Posterior over the parameters </a><a class="anchor nav-link" id="param-posterior" data-scroll-target="undefined"></a></li>
  <li><a href="#visualizing-the-parameter-posterior" id="toc-visualizing-the-parameter-posterior" class="nav-link" data-scroll-target="#visualizing-the-parameter-posterior">6.4 Visualizing the parameter posterior </a><a class="anchor nav-link" id="param-posterior-visualization" data-scroll-target="undefined"></a></li>
  <li><a href="#step-3-posterior-predictive-distribution" id="toc-step-3-posterior-predictive-distribution" class="nav-link" data-scroll-target="#step-3-posterior-predictive-distribution">6.5 Step 3: Posterior predictive distribution </a><a class="anchor nav-link" id="predictive-posterior" data-scroll-target="undefined"></a></li>
  <li><a href="#visualizing-the-predictive-posterior" id="toc-visualizing-the-predictive-posterior" class="nav-link" data-scroll-target="#visualizing-the-predictive-posterior">6.6 Visualizing the predictive posterior </a><a class="anchor nav-link" id="predictive-posterior-visualization" data-scroll-target="undefined"></a></li>
  </ul></li>
  <li><a href="#sources-and-further-reading" id="toc-sources-and-further-reading" class="nav-link" data-scroll-target="#sources-and-further-reading">Sources and further reading </a><a class="anchor nav-link" id="sources" data-scroll-target="undefined"></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bayesian linear regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<ul>
<li><a href="#intro-blr">1. What is Bayesian linear regression?</a></li>
<li><a href="#recap-lr">2. Recap linear regression</a></li>
<li><a href="#fundamental-concepts">3. Fundamental concepts</a></li>
<li><a href="#prob-lr">4. Linear regression from a probabilistic perspective</a></li>
<li><a href="#lr-with-basis-functions">5. Linear regression with basis functions</a>
<ul>
<li><a href="#example-basis-function">5.1 Example basis functions</a></li>
<li><a href="#design-matrix">5.2 The design matrix</a></li>
</ul></li>
<li><a href="#blr">6. Bayesian Linear Regression</a>
<ul>
<li><a href="#prob-model">6.1 Step 1: Probabilistic Model</a></li>
<li><a href="#dataset">6.2 Generating a dataset</a></li>
<li><a href="#param-posterior">6.3 Step 2: Posterior over the parameters</a></li>
<li><a href="#param-posterior-visualization">6.4 Visualizing the parameter posterior</a></li>
<li><a href="#predictive-posterior">6.5 Step 3: Posterior predictive distribution</a></li>
<li><a href="#predictive-posterior-visualization">6.6 Visualizing the predictive posterior</a></li>
</ul></li>
<li><a href="#sources">Sources and further reading</a></li>
</ul>
<section id="link-to-interactive-demo" class="level2">
<h2 class="anchored" data-anchor-id="link-to-interactive-demo">Link to interactive demo</h2>
<p><a href="https://mybinder.org/v2/gh/zotroneneis/machine_learning_basics/HEAD?filepath=bayesian_linear_regression.ipynb">Click here</a> to run the notebook online (using Binder) without installing jupyter or downloading the code.</p>
<p>Sometimes, the GitHub version of the Jupyter notebook does not display the math formulas correctly. Please refer to the Binder version in case you think something might be off or missing.</p>
<p>I also wrote a <a href="https://alpopkes.com/posts/machine_learning/bayesian_linear_regression">blog post containing the contents of the notebook</a>.</p>
</section>
<section id="what-is-bayesian-linear-regression-blr" class="level2">
<h2 class="anchored" data-anchor-id="what-is-bayesian-linear-regression-blr">1. What is Bayesian linear regression (BLR)? <a class="anchor" id="intro-blr"></a></h2>
<p>Bayesian linear regression is the <em>Bayesian</em> interpretation of linear regression. What does that mean? To answer this question we first have to understand the Bayesian approach. In most of the algorithms we have looked at so far we computed <em>point estimates</em> of our parameters. For example, in linear regression we chose values for the weights and bias that minimized our mean squared error cost function. In the Bayesian approach we don’t work with exact values but with <em>probabilities</em>. This allows us to model the <em>uncertainty</em> in our parameter estimates. Why is this important?</p>
<p>In nearly all real-world situations, our data and knowledge about the world is incomplete, indirect and noisy. Hence, uncertainty must be a fundamental part of our decision-making process. This is exactly what the Bayesian approach is about. It provides a formal and consistent way to reason in the presence of uncertainty. Bayesian methods have been around for a long time and are widely-used in many areas of science (e.g.&nbsp;astronomy). Although Bayesian methods have been applied to machine learning problems too, they are usually less well known to beginners. The major reason is that they require a good understanding of probability theory.</p>
<p>In the following notebook we will work our way from linear regression to Bayesian linear regression, including the most important theoretical knowledge and code examples.</p>
</section>
<section id="recap-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="recap-linear-regression">2. Recap linear regression <a class="anchor" id="recap-lr"></a></h2>
<ul>
<li>In linear regression, we want to find a function <span class="math inline">\(f\)</span> that maps inputs <span class="math inline">\(x \in \mathbb{R}^D\)</span> to corresponding function values <span class="math inline">\(f(x) \in \mathbb{R}\)</span>.</li>
<li>We are given an input dataset <span class="math inline">\(D = \big \{ \mathbf{x}_n, y_n \big \}_{n=1}^N\)</span>, where <span class="math inline">\(y_n\)</span> is a noisy observation value: <span class="math inline">\(y_n = f(x_n) + \epsilon\)</span>, with <span class="math inline">\(\epsilon\)</span> being an i.i.d. random variable that describes measurement/observation noise</li>
<li>Our goal is to infer the underlying function <span class="math inline">\(f\)</span> that generated the data such that we can predict function values at new input locations</li>
<li>In linear regression, we model the underlying function <span class="math inline">\(f\)</span> using a linear combination of the input features:</li>
</ul>
<p><span class="math display">\[
\begin{split}
y &amp;= \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_d x_d \\
&amp;= \boldsymbol{x}^T \boldsymbol{\theta}
\end{split}
\]</span></p>
<ul>
<li>For more details take a look at the <a href="https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb">notebook on linear regression</a></li>
</ul>
</section>
<section id="fundamental-concepts" class="level2">
<h2 class="anchored" data-anchor-id="fundamental-concepts">3. Fundamental concepts <a class="anchor" id="fundamental-concepts"></a></h2>
<ul>
<li>One fundamental tool in Bayesian learning is <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a></li>
<li>Bayes’ theorem looks as follows: <span class="math display">\[
\begin{equation}
p(\boldsymbol{\theta} | \mathbf{x}, y) = \frac{p(y | \boldsymbol{x}, \boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\boldsymbol{x}, y)}
\end{equation}
\]</span></li>
<li><span class="math inline">\(p(y | \boldsymbol{x}, \boldsymbol{\theta})\)</span> is the <em>likelihood</em>. It describes the probability of the target values given the data and parameters.</li>
<li><span class="math inline">\(p(\boldsymbol{\theta})\)</span> is the <em>prior</em>. It describes our initial knowledge about which parameter values are likely and unlikely.</li>
<li><span class="math inline">\(p(\boldsymbol{x}, y)\)</span> is the <em>evidence</em>. It describes the joint probability of the data and targets.<br>
</li>
<li><span class="math inline">\(p(\boldsymbol{\theta} | \boldsymbol{x}, y)\)</span> is the <em>posterior</em>. It describes the probability of the parameters given the observed data and targets. <br></li>
<li>Another important tool you need to know about is the <a href="https://en.wikipedia.org/wiki/Normal_distribution">Gaussian distribution</a>. If you are not familiar with it I suggest you pause for a minute and understand its main properties before reading on.</li>
</ul>
<p>In general, Bayesian inference works as follows: 1. We start with some prior belief about a hypothesis <span class="math inline">\(p(h)\)</span> 2. We observe some data, representating new evidence <span class="math inline">\(e\)</span> 3. We use Bayes’ theorem to update our belief given the new evidence: <span class="math inline">\(p(h|e) = \frac{p(e |h)p(h)}{p(e)}\)</span></p>
<p>For more information take a look at the <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Wikipedia article on Bayesian inference</a>.</p>
</section>
<section id="linear-regression-from-a-probabilistic-perspective" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-from-a-probabilistic-perspective">4. Linear regression from a probabilistic perspective <a class="anchor" id="prob-lr"></a></h2>
<p>In order to pave the way for Bayesian linear regression we will take a probabilistic spin on linear regression. Let’s start by explicitly modelling the observation noise <span class="math inline">\(\epsilon\)</span>. For simplicity, we assume that <span class="math inline">\(\epsilon\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and some known variance <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>As mentioned in the beginning, a simple linear regression model assumes that the target function <span class="math inline">\(f(x)\)</span> is given by a linear combination of the input features: <span class="math display">\[
\begin{split}
y = f(\boldsymbol{x}) + \epsilon \\
  = \boldsymbol{x}^T \boldsymbol{\theta} + \epsilon
\end{split}
\]</span></p>
<p>This corresponds to the following likelihood function: <span class="math display">\[p(y | \boldsymbol{x}, \boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{x}^T \boldsymbol{\theta}, \sigma^2)\]</span></p>
<p>Our goal is to find the parameters <span class="math inline">\(\boldsymbol{\theta} = \{\theta_1, ..., \theta_D\}\)</span> that model the given data best. In standard linear regression we can find the best parameters using a least-squares, maximum likelihood (ML) or maximum a posteriori (MAP) approach. If you want to know more about these solutions take a look at the <a href="https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb">notebook on linear regression</a> or at chapter 9.2 of the book <a href="https://mml-book.com">Mathematics for Machine Learning</a>.</p>
</section>
<section id="linear-regression-with-basis-functions" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression-with-basis-functions">5. Linear regression with basis functions <a class="anchor" id="lr-with-basis-functions"></a></h2>
<p>The simple linear regression model above is linear not only with respect to the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span> but also with respect to the inputs <span class="math inline">\(\boldsymbol{x}\)</span>. When <span class="math inline">\(\boldsymbol{x}\)</span> is not a vector but a single value (that is, the dataset is one-dimensional) the model <span class="math inline">\(y_i = x_i \cdot \theta\)</span> describes straight lines with <span class="math inline">\(\theta\)</span> being the slope of the line.</p>
<p>The plot below shows example lines produced with the model <span class="math inline">\(y = x \cdot \theta\)</span>, using different values for the slope <span class="math inline">\(\theta\)</span> and intercept 0.</p>
<p><img src="figures/example_straight_lines.png" alt="Drawing" style="width: 500px;"></p>
<p>Having a model which is linear both with respect to the parameters and inputs limits the functions it can learn significantly. We can make our model more powerful by making it <em>nonlinear</em> with respect to the inputs. After all, <em>linear regression</em> refers to models which are linear in the <em>parameters</em>, not necessarily in the <em>inputs</em> (linear in the parameters means that the model describes a function by a linear combination of input features).</p>
<p>Making the model nonlinear with respect to the inputs is easy. We can adapt it by using a nonlinear transformation of the input features <span class="math inline">\(\phi(\boldsymbol{x})\)</span>. With this adaptation our model looks as follows: <span class="math display">\[
\begin{split}
y &amp;= \boldsymbol{\phi}^T(\boldsymbol{x}) \boldsymbol{\theta} + \epsilon \\
&amp;= \sum_{k=0}^{K-1} \theta_k \phi_k(\boldsymbol{x}) + \epsilon
\end{split}
\]</span></p>
<p>Where <span class="math inline">\(\boldsymbol{\phi}: \mathbf{R}^D \rightarrow \mathbf{R}^K\)</span> is a (non)linear transformation of the inputs <span class="math inline">\(\boldsymbol{x}\)</span> and <span class="math inline">\(\phi_k: \mathbf{R}^D \rightarrow \mathbf{R}\)</span> is the <span class="math inline">\(k-\)</span>th component of the <em>feature vector</em> <span class="math inline">\(\boldsymbol{\phi}\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\phi}(\boldsymbol{x})=\left[\begin{array}{c}
\phi_{0}(\boldsymbol{x}) \\
\phi_{1}(\boldsymbol{x}) \\
\vdots \\
\phi_{K-1}(\boldsymbol{x})
\end{array}
\right]
\in \mathbb{R}^{K}
\]</span></p>
<p>With our new nonlinear transformation the likelihood function is given by</p>
<p><span class="math display">\[
p(y | \boldsymbol{x}, \boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\phi}^T(\boldsymbol{x}) \boldsymbol{\theta},\, \sigma^2)
\]</span></p>
<section id="example-basis-functions" class="level3">
<h3 class="anchored" data-anchor-id="example-basis-functions">5.1 Example basis functions <a class="anchor" id="example-basis-function"></a></h3>
<section id="linear-regression" class="level4">
<h4 class="anchored" data-anchor-id="linear-regression">Linear regression</h4>
<p>The easiest example for a basis function (for one-dimensional data) would be simple linear regression, that is, no non-linear transformation at all. In this case we would choose <span class="math inline">\(\phi_0(x) = 1\)</span> and <span class="math inline">\(\phi_i(x) = x\)</span>. This would result in the following vector <span class="math inline">\(\boldsymbol{\phi}(x)\)</span>:</p>
<p><span class="math display">\[
\boldsymbol{\phi}(x)=
\left[
\begin{array}{c}
\phi_{0}(x) \\
\phi_{1}(x) \\
\vdots \\
\phi_{K-1}(x)
\end{array}
\right] =
\left[
\begin{array}{c}
1 \\
x \\
\vdots \\
x
\end{array}
\right]
\in \mathbb{R}^{K}
\]</span></p>
</section>
<section id="polynomial-regression" class="level4">
<h4 class="anchored" data-anchor-id="polynomial-regression">Polynomial regression</h4>
<p>Another common choice of basis function for the one-dimensional case is polynomial regression. For this we would set <span class="math inline">\(\phi_i(x) = x^i\)</span> for <span class="math inline">\(i=0, ..., K-1\)</span>. The corresponding feature vector <span class="math inline">\(\boldsymbol{\phi}(x)\)</span> would look as follows:</p>
<p><span class="math display">\[
\boldsymbol{\phi}(x)=
\left[
\begin{array}{c}
\phi_{0}(x) \\
\phi_{1}(x) \\
\vdots \\
\phi_{K-1}(x)
\end{array}
\right] =
\left[
\begin{array}{c}
1 \\
x \\
x^2 \\
x^3 \\
\vdots \\
x^{K-1}
\end{array}
\right]
\in \mathbb{R}^{K}
\]</span></p>
<p>With this transformation we can lift our original one-dimensional input into a <span class="math inline">\(K\)</span>-dimensional feature space. Our function <span class="math inline">\(f\)</span> can be any polynomial with degree <span class="math inline">\(\le K-1\)</span>: <span class="math inline">\(f(x) = \sum_{k=0}^{K-1} \theta_k x^k\)</span></p>
</section>
</section>
<section id="the-design-matrix" class="level3">
<h3 class="anchored" data-anchor-id="the-design-matrix">5.2 The design matrix <a class="anchor" id="design-matrix"></a></h3>
<p>To make it easier to work with the transformations <span class="math inline">\(\boldsymbol{\phi}(\boldsymbol{x})\)</span> for the different input vectors <span class="math inline">\(\boldsymbol{x}\)</span> we typically create a so called <em>design matrix</em> (also called <em>feature matrix</em>). Given our dataset <span class="math inline">\(D = \big \{ \mathbf{x}_n, y_n \big \}_{n=1}^N\)</span> we define the design matrix as follows:</p>
<p><span class="math display">\[
\boldsymbol{\Phi}:=\left[\begin{array}{c}
\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{1}\right) \\
\vdots \\
\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{N}\right)
\end{array}\right]=\left[\begin{array}{ccc}
\phi_{0}\left(\boldsymbol{x}_{1}\right) &amp; \cdots &amp; \phi_{K-1}\left(\boldsymbol{x}_{1}\right) \\
\phi_{0}\left(\boldsymbol{x}_{2}\right) &amp; \cdots &amp; \phi_{K-1}\left(\boldsymbol{x}_{2}\right) \\
\vdots &amp; &amp; \vdots \\
\phi_{0}\left(\boldsymbol{x}_{N}\right) &amp; \cdots &amp; \phi_{K-1}\left(\boldsymbol{x}_{N}\right)
\end{array}\right] \in \mathbb{R}^{N \times K}
\]</span></p>
<p>Note that the design matrix is of shape <span class="math inline">\(N \times K\)</span>. <span class="math inline">\(N\)</span> is the number of input examples and <span class="math inline">\(K\)</span> is the output dimension of the non-linear transformation <span class="math inline">\(\boldsymbol{\phi}(\boldsymbol{x})\)</span>.</p>
</section>
</section>
<section id="bayesian-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-linear-regression">6. Bayesian linear regression <a class="anchor" id="blr"></a></h2>
<p>What changes when we consider a Bayesian interpretation of linear regression? Our data stays the same as before: <span class="math inline">\(D = \big \{ \mathbf{x}_n, y_n \big \}_{n=1}^N\)</span>. Given the data <span class="math inline">\(D\)</span> we can define the set of all inputs as <span class="math inline">\(\mathcal{X} := \{\boldsymbol{x}_1, ..., \boldsymbol{x}_n\}\)</span> and the set of all targets as <span class="math inline">\(\mathcal{Y} := \{y_1, ..., y_n \}\)</span>.</p>
<p>In simple linear regression we compute point estimates of our parameters (e.g.&nbsp;using a maximum likelihood approach) and use these estimates to make predictions. Different to this, Bayesian linear regression estimates <em>distributions</em> over the parameters and predictions. This allows us to model the uncertainty in our predictions.</p>
<p>To perform Bayesian linear regression we follow three steps: 1. We set up a probabilistic model that describes our assumptions how the data and parameters are generated 2. We perform inference for the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span>, that is, we compute the posterior probability distribution over the parameters 3. With this posterior we can perform inference for new, unseen inputs <span class="math inline">\(y_*\)</span>. In this step we don’t compute point estimates of the outputs. Instead, we compute the parameters of the posterior distribution over the outputs.</p>
<section id="step-1-probabilistic-model" class="level3">
<h3 class="anchored" data-anchor-id="step-1-probabilistic-model">6.1 Step 1: Probabilistic model <a class="anchor" id="prob-model"></a></h3>
<p>We start by setting up a probabilistic model that describes our assumptions how the data and parameters are generated. For this, we place a prior <span class="math inline">\(p(\boldsymbol{\theta})\)</span> over our parameters which encodes what parameter values are plausible (before we have seen any data). Example: With a single parameter <span class="math inline">\(\theta\)</span>, a Gaussian prior <span class="math inline">\(p(\theta) = \mathcal{N}(0, 1)\)</span> says that parameter values are normally distributed with mean 0 and standard deviation 1. In other words: the parameter values are most likely to fall into the interval [−2,2] which is two standard deviations around the mean value.</p>
<p>To keep things simple we will assume a Gaussian prior over the parameters: <span class="math inline">\(p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{m}_0, \boldsymbol{S}_0)\)</span>. Let’s further assume that the likelihood function is Gaussian, too: <span class="math inline">\(p(y \mid \boldsymbol{x}, \boldsymbol{\theta})=\mathcal{N}\left(y \mid \boldsymbol{\phi}^{\top}(\boldsymbol{x}) \boldsymbol{\theta}, \sigma^{2}\right)\)</span>.</p>
<p>Note: When considering the set of all targets <span class="math inline">\(\mathcal{Y} := \{y_1, ..., y_n \}\)</span>, the likelihood function becomes a multivariate Gaussian distribution: <span class="math inline">\(p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{\Phi} \boldsymbol{\theta}, \sigma^{2} \boldsymbol{I}\right)\)</span></p>
<p>The nice thing about choosing a Gaussian distribution for our prior is that the posterior distributions will be Gaussian, too (keyword <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a>)!</p>
<p>We will start our <code>BayesianLinearRegression</code> class with the knowledge we have so far - our probabilistic model. As mentioned in the beginning we assume that the variance <span class="math inline">\(\sigma^2\)</span> of the noise <span class="math inline">\(\epsilon\)</span> is known. Furthermore, to allow plotting the data later on we will assume that it’s two dimensional (d=2).</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianLinearRegression:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Bayesian linear regression</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_mean: Mean values of the prior distribution (m_0)</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_cov: Covariance matrix of the prior distribution (S_0)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_var: Variance of the noise distribution</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: <span class="bu">float</span>):</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_mean <span class="op">=</span> prior_mean[:, np.newaxis] <span class="co"># column vector of shape (1, d)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_cov <span class="op">=</span> prior_cov <span class="co"># matrix of shape (d, d)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We initalize the prior distribution over the parameters using the given mean and covariance matrix</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior <span class="op">=</span> multivariate_normal(prior_mean, prior_cov)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We also know the variance of the noise</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_var <span class="op">=</span> noise_var <span class="co"># single float value</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_precision <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> noise_var</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Before performing any inference the parameter posterior equals the parameter prior</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param_posterior <span class="op">=</span> <span class="va">self</span>.prior</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accordingly, the posterior mean and covariance equal the prior mean and variance</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_mean <span class="op">=</span> <span class="va">self</span>.prior_mean <span class="co"># corresponds to m_N in formulas</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_cov <span class="op">=</span> <span class="va">self</span>.prior_cov <span class="co"># corresponds to S_N in formulas</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Let's make sure that we can initialize our model</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>prior_mean <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>prior_cov <span class="op">=</span> np.array([[<span class="fl">0.5</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="fl">0.5</span>]])</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>noise_var <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>blr <span class="op">=</span> BayesianLinearRegression(prior_mean, prior_cov, noise_var)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="generating-a-dataset" class="level3">
<h3 class="anchored" data-anchor-id="generating-a-dataset">6.2 Generating a dataset <a class="anchor" id="dataset"></a></h3>
<p>Before going any further we need a dataset to test our implementation. Remember that we assume that our targets were generated by a function of the form <span class="math inline">\(y = \boldsymbol{\phi}^T(\boldsymbol{x}) \boldsymbol{\theta} + \epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is normally distributed with mean <span class="math inline">\(0\)</span> and some known variance <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>.</p>
<p>To keep things simple we will work with one-dimensional data and simple linear regression (that is, no non-linear transformation of the inputs). Consequently, our data generating function will be of the form <span class="math display">\[ y = \theta_0 + \theta_1 \, x + \epsilon \]</span></p>
<p>Note that we added a parameter <span class="math inline">\(\theta_0\)</span> which corresponds to the intercept of the linear function. Until know we assumed <span class="math inline">\(\theta_0 = 0\)</span>. As mentioned earlier, <span class="math inline">\(\theta_1\)</span> represents the slope of the linear function.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compute_function_labels(slope: <span class="bu">float</span>, intercept: <span class="bu">float</span>, noise_std_dev: <span class="bu">float</span>, data: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute target values given function parameters and data.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        slope: slope of the function (theta_1)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">        intercept: intercept of the function (theta_0)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">        data: input feature values (x)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_std_dev: standard deviation of noise distribution (sigma)</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co">        target values, either true or corrupted with noise</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> noise_std_dev <span class="op">==</span> <span class="dv">0</span>: <span class="co"># Real function</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> slope <span class="op">*</span> data <span class="op">+</span> intercept</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>: <span class="co"># Noise corrupted</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> slope <span class="op">*</span> data <span class="op">+</span> intercept <span class="op">+</span> np.random.normal(<span class="dv">0</span>, noise_std_dev, n_samples)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set random seed to ensure reproducibility</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>np.random.seed(seed)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate true values and noise corrupted targets</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>n_datapoints <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> <span class="op">-</span><span class="fl">0.7</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>slope <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>noise_std_dev <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>noise_var <span class="op">=</span> noise_std_dev<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>lower_bound <span class="op">=</span> <span class="op">-</span><span class="fl">1.5</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>upper_bound <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate dataset</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> np.random.uniform(lower_bound, upper_bound, n_datapoints)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> compute_function_labels(slope, intercept, <span class="fl">0.</span>, features)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>noise_corrupted_labels <span class="op">=</span> compute_function_labels(slope, intercept, noise_std_dev, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the dataset</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">7</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.plot(features, labels, color<span class="op">=</span><span class="st">'r'</span>, label<span class="op">=</span><span class="st">"True values"</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(features, noise_corrupted_labels, label<span class="op">=</span><span class="st">"Noise corrupted values"</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Features"</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Labels"</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Real function along with noisy targets"</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="step-2-posterior-over-the-parameters" class="level3">
<h3 class="anchored" data-anchor-id="step-2-posterior-over-the-parameters">6.3 Step 2: Posterior over the parameters <a class="anchor" id="param-posterior"></a></h3>
<p>We finished setting up our probabilistic model. Next, we want to use this model and our dataset <span class="math inline">\(\mathcal{X, Y}\)</span> to estimate the parameter posterior <span class="math inline">\(p(\boldsymbol{\theta} | \mathcal{X, Y})\)</span>. Keep in mind that we don’t compute point estimates of the parameters. Instead, we determine the mean and variance of the (Gaussian) posterior distribution and use this entire distribution when making predictions.</p>
<p>We can estimate the parameter posterior using Bayes theorem: <span class="math display">\[
p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y})=\frac{p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) p(\boldsymbol{\theta})}{p(\mathcal{Y} \mid \mathcal{X})}
\]</span></p>
<ul>
<li><span class="math inline">\(p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})\)</span> is the likelihood function, <span class="math inline">\(p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta})=\mathcal{N}\left(\boldsymbol{y} \mid \boldsymbol{\Phi} \boldsymbol{\theta}, \sigma^{2} \boldsymbol{I}\right)\)</span></li>
<li><span class="math inline">\(p(\boldsymbol{\theta})\)</span> is the prior distribution, <span class="math inline">\(p(\boldsymbol{\theta})=\mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{m}_{0}, \boldsymbol{S}_{0}\right)\)</span></li>
<li><span class="math inline">\(p(\mathcal{Y} \mid \mathcal{X})=\int p(\mathcal{Y} \mid \mathcal{X}, \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}\)</span> is the evidence which ensures that the posterior is normalized (that is, that it integrates to 1).</li>
</ul>
<p>The parameter posterior can be estimated in closed form (for proof see theorem 9.1 in the book <a href="https://mml-book.com">Mathematics for Machine Learning</a>): <span class="math display">\[
\begin{aligned}
p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) &amp;=\mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \\
\boldsymbol{S}_{N} &amp;=\left(\boldsymbol{S}_{0}^{-1}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}\right)^{-1} \\
\boldsymbol{m}_{N} &amp;=\boldsymbol{S}_{N}\left(\boldsymbol{S}_{0}^{-1} \boldsymbol{m}_{0}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{y}\right)
\end{aligned}
\]</span></p>
<p>Coming back to our <code>BayesLinearRegression</code> class we need to add a method which allows us to update the posterior distribution given a dataset.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm <span class="im">as</span> univariate_normal</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BayesianLinearRegression:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Bayesian linear regression</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_mean: Mean values of the prior distribution (m_0)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        prior_cov: Covariance matrix of the prior distribution (S_0)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">        noise_var: Variance of the noise distribution</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, prior_mean: np.ndarray, prior_cov: np.ndarray, noise_var: <span class="bu">float</span>):</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_mean <span class="op">=</span> prior_mean[:, np.newaxis] <span class="co"># column vector of shape (1, d)</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior_cov <span class="op">=</span> prior_cov <span class="co"># matrix of shape (d, d)</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We initalize the prior distribution over the parameters using the given mean and covariance matrix</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># In the formulas above this corresponds to m_0 (prior_mean) and S_0 (prior_cov)</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.prior <span class="op">=</span> multivariate_normal(prior_mean, prior_cov)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># We also know the variance of the noise</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_var <span class="op">=</span> noise_var <span class="co"># single float value</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.noise_precision <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> noise_var</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Before performing any inference the parameter posterior equals the parameter prior</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param_posterior <span class="op">=</span> <span class="va">self</span>.prior</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accordingly, the posterior mean and covariance equal the prior mean and variance</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_mean <span class="op">=</span> <span class="va">self</span>.prior_mean <span class="co"># corresponds to m_N in formulas</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_cov <span class="op">=</span> <span class="va">self</span>.prior_cov <span class="co"># corresponds to S_N in formulas</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_posterior(<span class="va">self</span>, features: np.ndarray, targets: np.ndarray):</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co">        Update the posterior distribution given new features and targets</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="co">            features: numpy array of features</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a><span class="co">            targets: numpy array of targets</span></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape targets to allow correct matrix multiplication</span></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input shape is (N,) but we need (N, 1)</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> targets[:, np.newaxis]</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the design matrix, shape (N, 2)</span></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>        design_matrix <span class="op">=</span> <span class="va">self</span>.compute_design_matrix(features)</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the covariance matrix, shape (2, 2)</span></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        design_matrix_dot_product <span class="op">=</span> design_matrix.T.dot(design_matrix)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        inv_prior_cov <span class="op">=</span> np.linalg.inv(<span class="va">self</span>.prior_cov)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_cov <span class="op">=</span> np.linalg.inv(inv_prior_cov <span class="op">+</span>  <span class="va">self</span>.noise_precision <span class="op">*</span> design_matrix_dot_product)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the mean, shape (2, 1)</span></span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_mean <span class="op">=</span> <span class="va">self</span>.post_cov.dot( </span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>                         inv_prior_cov.dot(<span class="va">self</span>.prior_mean) <span class="op">+</span> </span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>                         <span class="va">self</span>.noise_precision <span class="op">*</span> design_matrix.T.dot(targets))</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the posterior distribution</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.param_posterior <span class="op">=</span> multivariate_normal(<span class="va">self</span>.post_mean.flatten(), <span class="va">self</span>.post_cov)</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> compute_design_matrix(<span class="va">self</span>, features: np.ndarray) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute the design matrix. To keep things simple we use simple linear</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a><span class="co">        regression and add the value phi_0 = 1 to our input data.</span></span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a><span class="co">            features: numpy array of features</span></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a><span class="co">            design_matrix: numpy array of transformed features</span></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="co">            </span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; compute_design_matrix(np.array([2, 3]))</span></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array([[1., 2.], [1., 3.])</span></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>        n_samples <span class="op">=</span> <span class="bu">len</span>(features)</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        phi_0 <span class="op">=</span> np.ones(n_samples)</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>        design_matrix <span class="op">=</span> np.stack((phi_0, features), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> design_matrix</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, features: np.ndarray):</span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a><span class="co">        Compute predictive posterior given new datapoint</span></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="co">        </span></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="co">            features: 1d numpy array of features</span></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a><span class="co">        Returns:</span></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a><span class="co">            pred_posterior: predictive posterior distribution</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        design_matrix <span class="op">=</span> <span class="va">self</span>.compute_design_matrix(features)</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>        pred_mean <span class="op">=</span> design_matrix.dot(<span class="va">self</span>.post_mean)</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        pred_cov <span class="op">=</span> design_matrix.dot(<span class="va">self</span>.post_cov.dot(design_matrix.T)) <span class="op">+</span> <span class="va">self</span>.noise_var</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        pred_posterior <span class="op">=</span> univariate_normal(loc<span class="op">=</span>pred_mean.flatten(), scale<span class="op">=</span>pred_cov<span class="op">**</span><span class="fl">0.5</span>)</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> pred_posterior</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-the-parameter-posterior" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-parameter-posterior">6.4 Visualizing the parameter posterior <a class="anchor" id="param-posterior-visualization"></a></h3>
<p>To ensure that our implementation is correct we can visualize how the posterior over the parameters changes as the model sees more data. We will visualize the distribution using a <a href="https://en.wikipedia.org/wiki/Contour_line">contour plot</a> - a method for visualizing three-dimensional functions. In our case we want to visualize the density of our bi-variate Gaussian for each point (that is, each slope/intercept combination). The plot below shows an example which illustrates how the lines and colours of a contour plot correspond to a Gaussian distribution:</p>
<p><img src="figures/density_plot.png" width="400"></p>
<p>As we can see, the density is highest in the yellow regions decreasing when moving further out into the green and blue parts. This should give you a better understanding of contour plots.</p>
<p>To analyze our Bayesian linear regression class we will start by initializing a new model. We can visualize its prior distribution over the parameters <em>before</em> the model has seen any real data.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize BLR model</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>prior_mean <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>prior_cov <span class="op">=</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span> np.identity(<span class="dv">2</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>blr <span class="op">=</span> BayesianLinearRegression(prior_mean, prior_cov, noise_var)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_param_posterior(lower_bound, upper_bound, blr, title):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure()</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    mesh_features, mesh_labels <span class="op">=</span> np.mgrid[lower_bound:upper_bound:<span class="fl">.01</span>, lower_bound:upper_bound:<span class="fl">.01</span>]</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> np.dstack((mesh_features, mesh_labels))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    plt.contourf(mesh_features, mesh_labels, blr.param_posterior.pdf(pos), levels<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    plt.scatter(intercept, slope, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">"True parameter values"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Intercept"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Slope"</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    plt.legend()<span class="op">;</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize parameter prior distribution</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>plot_param_posterior(lower_bound, upper_bound, blr, title<span class="op">=</span><span class="st">"Prior parameter distribution"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The plot above illustrates both the prior parameter distribution and the true parameter values that we want to find. If our model works correctly, the posterior distribution should become more narrow and move closer to the true parameter values as the model sees more datapoints. This can be visualized with contour plots, too! Below we update the posterior distribution iteratively as the model sees more and more data. The contour plots for each step show how the parameter posterior develops and converges close to the true values in the end.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n_points_lst <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>previous_n_points <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_points <span class="kw">in</span> n_points_lst:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    train_features <span class="op">=</span> features[previous_n_points:n_points]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    train_labels <span class="op">=</span> noise_corrupted_labels[previous_n_points:n_points]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    blr.update_posterior(train_features, train_labels)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize updated parameter posterior distribution</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    plot_param_posterior(lower_bound, </span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                         upper_bound, </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                         blr, </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                         title<span class="op">=</span><span class="ss">f"Updated parameter distribution using </span><span class="sc">{</span>n_points<span class="sc">}</span><span class="ss"> datapoints"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    previous_n_points <span class="op">=</span> n_points</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-3.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-5.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-7.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-8-output-8.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="step-3-posterior-predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="step-3-posterior-predictive-distribution">6.5 Step 3: Posterior predictive distribution <a class="anchor" id="predictive-posterior"></a></h3>
<p>Given the posterior distribution over the parameters we can determine the predictive distribution (= posterior over the outputs) for a new input <span class="math inline">\((\boldsymbol{x}_*, y_*)\)</span>. This is the distribution we are really interested in. A trained model is not particularly useful when we can’t use it to make predictions, right?</p>
<p>The posterior predictive distribution looks as follows:</p>
<p><span class="math display">\[
\begin{aligned}
p\left(y_{*} \mid \mathcal{X}, \mathcal{Y}, \boldsymbol{x}_{*}\right) &amp;=\int p\left(y_{*} \mid \boldsymbol{x}_{*}, \boldsymbol{\theta}\right) p(\boldsymbol{\theta} \mid \mathcal{X}, \mathcal{Y}) \mathrm{d} \boldsymbol{\theta} \\
&amp;=\int \mathcal{N}\left(y_{*} \mid \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{\theta}, \sigma^{2}\right) \mathcal{N}\left(\boldsymbol{\theta} \mid \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \mathrm{d} \boldsymbol{\theta} \\
&amp;=\mathcal{N}\left(y_{*} \mid \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{m}_{N}, \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{S}_{N} \boldsymbol{\phi}\left(\boldsymbol{x}_{*}\right)+\sigma^{2}\right)
\end{aligned}
\]</span></p>
<p>First of all: note that the predictive posterior for a new input <span class="math inline">\(\boldsymbol{x}_{*}\)</span> is a <em>univariate</em> Gaussian distribution. We can see that the mean of the distribution is given by the product of the design matrix for the new example (<span class="math inline">\(\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right)\)</span>) and the mean of the parameter posterior (<span class="math inline">\(\boldsymbol{m}_{N}\)</span>). The variance <span class="math inline">\((\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{S}_{N} \boldsymbol{\phi}\left(\boldsymbol{x}_{*}\right)+\sigma^{2}\)</span>) of the predictive posterior has two parts: 1. <span class="math inline">\(\sigma^{2}\)</span>: The variance of the noise 2. <span class="math inline">\(\boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{S}_{N} \boldsymbol{\phi}\left(\boldsymbol{x}_{*}\right)\)</span>: The posterior uncertainty associated with the parameters <span class="math inline">\(\boldsymbol{\theta}\)</span></p>
<p>Let’s add a <code>predict</code> method to our <code>BayesianLinearRegression</code> class which computes the predictive posterior for a new input (you will find the method in the class definition above):</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(<span class="va">self</span>, features: np.ndarray):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute predictive posterior given new datapoint</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">        features: 1d numpy array of features</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        pred_posterior: predictive posterior distribution</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    design_matrix <span class="op">=</span> <span class="va">self</span>.compute_design_matrix(features)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    pred_mean <span class="op">=</span> design_matrix.dot(<span class="va">self</span>.post_mean)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    pred_cov <span class="op">=</span> design_matrix.dot(<span class="va">self</span>.post_cov.dot(design_matrix.T)) <span class="op">+</span> <span class="va">self</span>.noise_var</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    pred_posterior <span class="op">=</span> univariate_normal(pred_mean.flatten(), pred_cov)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pred_posterior</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="visualizing-the-predictive-posterior" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-the-predictive-posterior">6.6 Visualizing the predictive posterior <a class="anchor" id="predictive-posterior-visualization"></a></h3>
<p>Our original dataset follows a simple linear function. After training the model it should be able to predict labels for new datapoints, even if they lie beyond the range from [-1.5, 1.5]. But how can we get from the predictive distribution that our model computes to actual labels? That’s easy: we <em>sample</em> from the predictive posterior.</p>
<p>To make sure that we are all on the same page: given a new input example our Bayesian linear regression model predicts not a single label but a <em>distribution</em> over possible labels. This distribution is Gaussian. We can get actual labels by sampling from this distribution.</p>
<p>The code below implements and visualizes this: - We create some test features for which we want predictions - Each feature is given to the trained BLR model which returns a univariate Gaussian distribution over possible labels (<code>pred_posterior = blr.predict(np.array([feat]))</code>) - We sample from this distribution (<code>sample_predicted_labels = pred_posterior.rvs(size=sample_size)</code>) - The predicted labels are saved in a format that makes it easy to plot them - Finally, we plot each input feature, its true label and the sampled predictions. Remember: the samples are generated from the predictive posterior returned by the <code>predict</code> method. Think of a Gaussian distribution plotted along the y-axis for each feature. We visualize this with a histogram: more likely values close to the mean will be sampled more often than less likely values.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>all_rows <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>test_features <span class="op">=</span> [<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>all_labels <span class="op">=</span> []</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> feat <span class="kw">in</span> test_features:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    true_label <span class="op">=</span> compute_function_labels(slope, intercept, <span class="dv">0</span>, np.array([feat]))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    all_labels.append(true_label)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    pred_posterior <span class="op">=</span> blr.predict(np.array([feat]))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    sample_predicted_labels <span class="op">=</span> pred_posterior.rvs(size<span class="op">=</span>sample_size)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> sample_predicted_labels:</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        all_rows.append([feat, label])</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>all_data <span class="op">=</span> pd.DataFrame(all_rows, columns<span class="op">=</span>[<span class="st">"feature"</span>, <span class="st">"label"</span>]) </span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>all_data, x<span class="op">=</span><span class="st">"feature"</span>, y<span class="op">=</span><span class="st">"label"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>plt.scatter(x<span class="op">=</span>test_features, y<span class="op">=</span>all_labels, color<span class="op">=</span><span class="st">"red"</span>, label<span class="op">=</span><span class="st">"True values"</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Predictive posterior distributions"</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="bayesian_linear_regression_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="sources-and-further-reading" class="level2">
<h2 class="anchored" data-anchor-id="sources-and-further-reading">Sources and further reading <a class="anchor" id="sources"></a></h2>
<p>The basis for this notebook is chapter 9.2 of the book <a href="https://mml-book.com">Mathematics for Machine Learning</a>. I can highly recommend to read through chapter 9 to get a deeper understanding of (Bayesian) linear regression.</p>
<p>You will find explanations and an implementation of simple linear regression in the <a href="https://github.com/zotroneneis/machine_learning_basics/blob/master/linear_regression.ipynb">notebook on linear regression</a></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>